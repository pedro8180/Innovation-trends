,module,unit,topic_name,content
0,Develop generative AI apps in Azure - Training,Plan and prepare to develop AI solutions on Azure,Introduction,"# Introduction

Completed 100 XP

  * 1 minute

The growth in the use of artificial intelligence (AI) in general, and
_generative_ AI in particular means that developers are increasingly required
to create comprehensive AI solutions. These solutions need to combine machine
learning models, AI services, prompt engineering solutions, and custom code.

Microsoft Azure provides multiple services that you can use to create AI
solutions. However, before embarking on an AI application development project,
it's useful to consider the available options for services, tools, and
frameworks as well as some principles and practices that can help you succeed.

This module explores some of the key considerations for planning an AI
development project, and introduces **Azure AI Foundry** ; a comprehensive
platform for AI development on Microsoft Azure.

* * *"
1,Develop generative AI apps in Azure - Training,Plan and prepare to develop AI solutions on Azure,What is AI?,"# What is AI?

Completed 100 XP

  * 5 minutes

The term ""Artificial Intelligence"" (AI) covers a wide range of software
capabilities that enable applications to exhibit human-like behavior. AI has
been around for many years, and its definition has varied as the technology
and use cases associated with it have evolved. In today's technological
landscape, AI solutions are built on machine learning _models_ that
encapsulate semantic relationships found in huge quantities of data; enabling
applications to appear to interpret input in various formats, reason over the
input data, and generate appropriate responses and predictions.

Common AI capabilities that developers can integrate into a software
application include:

Expand table

Capability | Description  
---|---  
![Diagram of speech bubbles.](../../wwl-data-ai/prepare-azure-ai-
development/media/generative-ai.png)  
**Generative AI** | The ability to generate original responses to natural
language _prompts_. For example, software for a real estate business might be
used to automatically generate property descriptions and advertising copy for
a property listing.  
![Diagram of a human head with a cog for a brain.](../../wwl-data-ai/prepare-
azure-ai-development/media/agent.png)  
**Agents** | Generative AI applications that can respond to user input or
assess situations autonomously, and take appropriate actions. For example, an
""executive assistant"" agent could provide details about the location of a
meeting on your calendar, or even attach a map or automate the booking of a
taxi or rideshare service to help you get there.  
![Diagram of an eye being scanned.](../../wwl-data-ai/prepare-azure-ai-
development/media/computer-vision.png)  
**Computer vision** | The ability to accept, interpret, and process visual
input from images, videos, and live camera streams. For example, an automated
checkout in a grocery store might use computer vision to identify which
products a customer has in their shopping basket, eliminating the need to scan
a barcode or manually enter the product and quantity.  
![Diagram of a speech bubble and a sound wave.](../../wwl-data-ai/prepare-
azure-ai-development/media/speech.png)  
**Speech** | The ability to recognize and synthesize speech. For example, a
digital assistant might enable users to ask questions or provide audible
instructions by speaking into a microphone, and generate spoken output to
provide answers or confirmations.  
![Diagram of a text document.](../../wwl-data-ai/prepare-azure-ai-
development/media/natural-language.png)  
**Natural language processing** | The ability to process natural language in
written or spoken form, analyze it, identify key points, and generate
summaries or categorizations. For example, a marketing application might
analyze social media messages that mention a particular company, translate
them to a specific language, and categorize them as positive or negative based
on sentiment analysis.  
![Diagram of a form containing information.](../../wwl-data-ai/prepare-azure-
ai-development/media/information-extraction.png)  
**Information extraction** | The ability to use computer vision, speech, and
natural language processing to extract key information from documents, forms,
images, recordings, and other kinds of content. For example, an automated
expense claims processing application might extract purchase dates, individual
line item details, and total costs from a scanned receipt.  
![Diagram of a chart showing an upward trend.](../../wwl-data-ai/prepare-
azure-ai-development/media/prediction.png)  
**Decision support** | The ability to use historic data and learned
correlations to make predictions that support business decision making. For
example, analyzing demographic and economic factors in a city to predict real
estate market trends that inform property pricing decisions.  
  
Determining the specific AI capabilities you want to include in your
application can help you identify the most appropriate AI services that you'll
need to provision, configure, and use in your solution.

## A closer look at generative AI

_Generative AI_ represents the latest advance in artificial intelligence, and
deserves some extra attention. Generative AI uses _language models_ to respond
to natural language _prompts_ , enabling you to build conversational apps and
agents that support research, content creation, and task automation in ways
that were previously unimaginable.

![Diagram of a prompt, a language model, and a response.](../../wwl-data-
ai/prepare-azure-ai-development/media/prompt.png)

The language models used in generative AI solutions can be large language
models (LLMs) that have been trained on huge volumes of data and include many
millions of parameters; or they can be small language models (SLMs) that are
optimized for specific scenarios with lower overhead. Language models commonly
respond to text-based prompts with natural language text; though increasingly
new _multi-modal_ models are able to handle image or speech prompts and
respond by generating text, code, speech, or images.

* * *"
2,Develop generative AI apps in Azure - Training,Plan and prepare to develop AI solutions on Azure,Azure AI services,"# Azure AI services

Completed 100 XP

  * 5 minutes

Microsoft Azure provides a wide range of cloud services that you can use to
develop, deploy, and manage an AI solution. The most obvious starting point
for considering AI development on Azure is Azure AI services; a set of out-of-
the-box prebuilt APIs and models that you can integrate into your
applications. The following table lists some commonly used Azure AI services
(for a full list of all available Azure AI services, see [Available Azure AI
services](https://learn.microsoft.com/en-us/azure/ai-services/what-are-ai-
services#available-azure-ai-services?azure-portal=true)).

Expand table

Service | Description  
---|---  
![Azure OpenAI service icon.](../../wwl-data-ai/prepare-azure-ai-
development/media/open-ai.png)  
**Azure OpenAI** | Azure OpenAI in Foundry Models provides access to OpenAI
generative AI models including the GPT family of large and small language
models and DALL-E image-generation models within a scalable and securable
cloud service on Azure.  
![Azure AI Vision service icon.](../../wwl-data-ai/prepare-azure-ai-
development/media/vision.png)  
**Azure AI Vision** | The Azure AI Vision service provides a set of models and
APIs that you can use to implement common computer vision functionality in an
application. With the AI Vision service, you can detect common objects in
images, generate captions, descriptions, and tags based on image contents, and
read text in images.  
![Azure AI Speech service icon.](../../wwl-data-ai/prepare-azure-ai-
development/media/speech-service.png)  
**Azure AI Speech** | The Azure AI Speech service provides APIs that you can
use to implement _text to speech_ and _speech to text_ transformation, as well
as specialized speech-based capabilities like speaker recognition and
translation.  
![Azure AI Language service icon.](../../wwl-data-ai/prepare-azure-ai-
development/media/language.png)  
**Azure AI Language** | The Azure AI Language service provides models and APIs
that you can use to analyze natural language text and perform tasks such as
entity extraction, sentiment analysis, and summarization. The AI Language
service also provides functionality to help you build conversational language
models and question answering solutions.  
![Azure AI Foundry Content Safety service icon.](../../wwl-data-ai/prepare-
azure-ai-development/media/content-safety.png)  
**Azure AI Foundry Content Safety** | Azure AI Foundry Content Safety provides
developers with access to advanced algorithms for processing images and text
and flagging content that is potentially offensive, risky, or otherwise
undesirable.  
![Azure AI Translator service icon.](../../wwl-data-ai/prepare-azure-ai-
development/media/translator.png)  
**Azure AI Translator** | The Azure AI Translator service uses state-of-the-
art language models to translate text between a large number of languages.  
![Azure AI Face service icon.](../../wwl-data-ai/prepare-azure-ai-
development/media/face.png)  
**Azure AI Face** | The Azure AI Face service is a specialist computer vision
implementation that can detect, analyze, and recognize human faces. Because of
the potential risks associated with personal identification and misuse of this
capability, access to some features of the AI Face service are restricted to
approved customers.  
![Azure AI Custom Vision service icon.](../../wwl-data-ai/prepare-azure-ai-
development/media/custom-vision.png)  
**Azure AI Custom Vision** | The Azure AI Custom Vision service enables you to
train and use custom computer vision models for image classification and
object detection.  
![Azure AI Document Intelligence service icon.](../../wwl-data-ai/prepare-
azure-ai-development/media/document-intelligence.png)  
**Azure AI Document Intelligence** | With Azure AI Document Intelligence, you
can use pre-built or custom models to extract fields from complex documents
such as invoices, receipts, and forms.  
![Azure AI Content Understanding service icon.](../../wwl-data-ai/prepare-
azure-ai-development/media/content-understanding.png)  
**Azure AI Content Understanding** | The Azure AI Content Understanding
service provides multi-modal content analysis capabilities that enable you to
build models to extract data from forms and documents, images, videos, and
audio streams.  
![Azure AI Search service icon.](../../wwl-data-ai/prepare-azure-ai-
development/media/search.png)  
**Azure AI Search** | The Azure AI Search service uses a pipeline of AI skills
based on other Azure AI Services and custom code to extract information from
content and create a searchable index. AI Search is commonly used to create
vector indexes for data that can then be used to _ground_ prompts submitted to
generative AI language models, such as those provided in Azure OpenAI.  
  
## Considerations for Azure AI services resources

To use Azure AI services, you create one or more Azure AI resources in an
Azure subscription and implement code in client applications to consume them.
In some cases, AI services include web-based visual interfaces that you can
use to configure and test your resources - for example to train a custom image
classification model using the **Custom Vision** service you can use the
visual interface to upload training images, manage training jobs, and deploy
the resulting model.

Note

You can provision Azure AI services resources in the Azure portal (or by using
BICEP or ARM templates or the Azure command-line interface) and build
applications that use them directly through various service-specific APIs and
SDKs. However, as we'll discuss later in this module, in most medium to large-
scale development scenarios it's better to provision Azure AI services
resources as part of an _Azure AI Foundry_ project - enabling you to
centralize access control and cost management, and making it easier to manage
shared resources and build the next generation of generative AI apps and
agents.

### Single service or multi-service resource?

Most Azure AI services, such as **Azure AI Vision** , **Azure AI Language** ,
and so on, can be provisioned as standalone resources, enabling you to create
only the Azure resources you specifically need. Additionally, standalone Azure
AI services often include a free-tier SKU with limited functionality, enabling
you to evaluate and develop with the service at no cost. Each standalone Azure
AI resource provides an endpoint and authorization keys that you can use to
access it securely from a client application.

Alternatively, you can provision a multi-service resource that encapsulates
multiple AI services in a single Azure resource. Using a multi-service
resource can make it easier to manage applications that use multiple AI
capabilities. There are two multi-service resource types you can use:

Expand table

Resource | Description  
---|---  
![Azure AI service icon.](../../wwl-data-ai/prepare-azure-ai-
development/media/cognitive-services.png)  
**Azure AI services** | The Azure AI Services resource type includes the
following services, making them available from a single endpoint:

  * Azure AI Speech
  * Azure AI Language
  * Azure AI Translator
  * Azure AI Vision
  * Azure AI Face
  * Azure AI Custom Vision
  * Azure AI Document Intelligence

  
![Azure AI Foundry icon.](../../wwl-data-ai/prepare-azure-ai-
development/media/ai-services.png)  
**Azure AI Foundry** | The Azure AI Foundry resource type includes the
following services, and supports working with them through an Azure AI Foundry
project*:

  * Azure OpenAI
  * Azure AI Speech
  * Azure AI Language
  * Azure AI Foundry Content Safety
  * Azure AI Translator
  * Azure AI Vision
  * Azure AI Face
  * Azure AI Document Intelligence
  * Azure AI Content Understanding

  
  
* Azure AI Foundry is discussed in the next unit.

### Regional availability

Some services and models are available in only a subset of Azure regions.
Consider service availability and any regional quota restrictions for your
subscription when provisioning Azure AI services. Use the [product
availability table](https://azure.microsoft.com/explore/global-
infrastructure/products-by-region/table) to check regional availability of
Azure services. Use the [model availability
table](https://learn.microsoft.com/en-us/azure/ai-
services/openai/concepts/models#model-summary-table-and-region-
availability?azure-portal=true) in the Azure OpenAI documentation to determine
regional availability for Azure OpenAI models.

### Cost

Azure AI services are charged based on usage, with different pricing schemes
available depending on the specific services being used. As you plan an AI
solution on Azure, use the [Azure AI services
pricing](https://azure.microsoft.com/pricing/details/cognitive-services)
documentation to understand pricing for the AI services you intend to
incorporate into your application. You can use the [Azure pricing
calculator](https://azure.microsoft.com/pricing/calculator) to estimate the
costs your expected usage will incur.

* * *"
3,Develop generative AI apps in Azure - Training,Plan and prepare to develop AI solutions on Azure,Azure AI Foundry,"# Azure AI Foundry

Completed 100 XP

  * 5 minutes

Azure AI Foundry is a platform for AI development on Microsoft Azure. While
you _can_ provision individual Azure AI services resources and build
applications that consume them without it, the project organization, resource
management, and AI development capabilities of Azure AI Foundry makes it the
recommended way to build all but the most simple solutions.

Azure AI Foundry provides the _Azure AI Foundry portal_ , a web-based visual
interface for working with AI projects. It also provides the _Azure AI Foundry
SDK_ , which you can use to build AI solutions programmatically.

## Azure AI Foundry projects

In Azure AI Foundry, you manage the resource connections, data, code, and
other elements of the AI solution in _projects_. There are two kinds of
project:

### Foundry projects

![Diagram of a Foundry project.](../../wwl-data-ai/prepare-azure-ai-
development/media/foundry-project.png)

_Foundry projects_ are associated with an **Azure AI Foundry** resource in an
Azure subscription. Foundry projects provide support for Azure AI Foundry
models (including OpenAI models), Azure AI Foundry Agent Service, Azure AI
services, and tools for evaluation and responsible AI development.

An Azure AI Foundry resource supports the most common AI development tasks to
develop generative AI chat apps and agents. In most cases, using a Foundry
project provides the right level of resource centralization and capabilities
with a minimal amount of administrative resource management. You can use Azure
AI Foundry portal to work in projects that are based in Azure AI Foundry
resources, making it easy to add connected resources and manage model and
agent deployments.

### Hub-based projects

![Diagram of a hub-based project.](../../wwl-data-ai/prepare-azure-ai-
development/media/hub-project.png)

_Hub-based projects_ are associated with an **Azure AI hub** resource in an
Azure subscription. Hub-based projects include an Azure AI Foundry resource,
as well as managed compute, support for Prompt Flow development, and connected
**Azure storage** and **Azure key vault** resources for secure data storage.

Azure AI hub resources support advanced AI development scenarios, like
developing Prompt Flow based applications or fine-tuning models. You can also
use Azure AI hub resources in both Azure AI Foundry portal and Azure Machine
learning portal, making it easier to work on collaborative projects that
involve data scientists and machine learning specialists as well as developers
and AI software engineers

Tip

For more information about Azure AI Foundry project types, see **[What is
Azure AI Foundry?](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-
azure-ai-foundry)**.

* * *"
4,Develop generative AI apps in Azure - Training,Plan and prepare to develop AI solutions on Azure,Developer tools and SDKs,"# Developer tools and SDKs

Completed 100 XP

  * 5 minutes

While you can perform many of the tasks needed to develop an AI solution
directly in the Azure AI Foundry portal, developers also need to write, test,
and deploy code.

## Development tools and environments

There are many development tools and environments available, and developers
should choose one that supports the languages, SDKs, and APIs they need to
work with and with which they're most comfortable. For example, a developer
who focuses strongly on building applications for Windows using the .NET
Framework might prefer to work in an integrated development environment (IDE)
like Microsoft Visual Studio. Conversely, a web application developer who
works with a wide range of open-source languages and libraries might prefer to
use a code editor like Visual Studio Code (VS Code). Both of these products
are suitable for developing AI applications on Azure.

### The Azure AI Foundry for Visual Studio Code extension

When developing Azure AI Foundry based generative AI applications in Visual
Studio Code, you can use the Azure AI Foundry for Visual Studio Code extension
to simplify key tasks in the workflow, including:

  * Creating a project.
  * Selecting and deploying a model.
  * Testing a model in the playground.
  * Creating an agent.

![Screenshot of the Azure AI Foundry Visual Studio Code extension.](../../wwl-
data-ai/prepare-azure-ai-development/media/vs-code.png)

Tip

For more information about using the Azure AI Foundry for Visual Studio Code
extension, see **[Work with the Azure AI Foundry for Visual Studio Code
extension](https://learn.microsoft.com/en-us/azure/ai-foundry/how-
to/develop/get-started-projects-vs-code)**.

### GitHub and GitHub Copilot

GitHub is the world's most popular platform for source control and DevOps
management, and can be a critical element of any team development effort.
Visual Studio and VS Code both provide native integration with GitHub, and
access to GitHub Copilot; an AI assistant that can significantly improve
developer productivity and effectiveness.

![Screenshot of GitHub Copilot in Visual Studio Code.](../../wwl-data-
ai/prepare-azure-ai-development/media/github-copilot.png)

Tip

For more information about using GitHub Copilot in Visual Studio Code, see
**[GitHub Copilot in VS
Code](https://code.visualstudio.com/docs/copilot/overview)**.

## Programming languages, APIs, and SDKs

You can develop AI applications using many common programming languages and
frameworks, including Microsoft C#, Python, Node, TypeScript, Java, and
others. When building AI solutions on Azure, some common SDKs you should plan
to install and use include:

  * The **[Azure AI Foundry SDK](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/sdk-overview)** , which enables you to write code to connect to Azure AI Foundry projects and access resource connections, which you can then work with using service-specific SDKs.
  * The **[Azure AI Foundry Models API](/en-us/rest/api/aifoundry/modelinference/)** , which provides an interface for working with generative AI model endpoints hosted in Azure AI Foundry.
  * The **[Azure OpenAI in Azure AI Foundry Models API](/en-us/azure/ai-services/openai/reference)** , which enables you to build chat applications based on OpenAI models hosted in Azure AI Foundry.
  * **[Azure AI Services SDKs](https://learn.microsoft.com/en-us/azure/ai-services/reference/sdk-package-resources)** \- AI service-specific libraries for multiple programming languages and frameworks that enable you to consume Azure AI Services resources in your subscription. You can also use Azure AI Services through their [REST APIs](/en-us/azure/ai-services/reference/rest-api-resources).
  * The **[Azure AI Foundry Agent Service](https://learn.microsoft.com/en-us/azure/ai-services/agents/overview)** , which is accessed through the Azure AI Foundry SDK and can be integrated with frameworks like [Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview) to build comprehensive AI agent solutions.

* * *"
5,Develop generative AI apps in Azure - Training,Plan and prepare to develop AI solutions on Azure,Responsible AI,"# Responsible AI

Completed 100 XP

  * 5 minutes

It's important for software engineers to consider the impact of their software
on users, and society in general; including considerations for its responsible
use. When the application is imbued with artificial intelligence, these
considerations are particularly important due to the nature of how AI systems
work and inform decisions; often based on probabilistic models, which are in
turn dependent on the data with which they were trained.

The human-like nature of AI solutions is a significant benefit in making
applications user-friendly, but it can also lead users to place a great deal
of trust in the application's ability to make correct decisions. The potential
for harm to individuals or groups through incorrect predictions or misuse of
AI capabilities is a major concern, and software engineers building AI-enabled
solutions should apply due consideration to mitigate risks and ensure
fairness, reliability, and adequate protection from harm or discrimination.

Let's discuss some core principles for responsible AI that have been adopted
at Microsoft.

## Fairness

![A diagram of scales.](../../wwl-data-ai/prepare-azure-ai-
development/media/fairness.png)

AI systems should treat all people fairly. For example, suppose you create a
machine learning model to support a loan approval application for a bank. The
model should make predictions of whether or not the loan should be approved
without incorporating any bias based on gender, ethnicity, or other factors
that might result in an unfair advantage or disadvantage to specific groups of
applicants.

Fairness of machine learned systems is a highly active area of ongoing
research, and some software solutions exist for evaluating, quantifying, and
mitigating unfairness in machine learned models. However, tooling alone isn't
sufficient to ensure fairness. Consider fairness from the beginning of the
application development process; carefully reviewing training data to ensure
it's representative of all potentially affected subjects, and evaluating
predictive performance for subsections of your user population throughout the
development lifecycle.

## Reliability and safety

![A diagram of a shield.](../../wwl-data-ai/prepare-azure-ai-
development/media/reliability-safety.png)

AI systems should perform reliably and safely. For example, consider an AI-
based software system for an autonomous vehicle; or a machine learning model
that diagnoses patient symptoms and recommends prescriptions. Unreliability in
these kinds of system can result in substantial risk to human life.

As with any software, AI-based software application development must be
subjected to rigorous testing and deployment management processes to ensure
that they work as expected before release. Additionally, software engineers
need to take into account the probabilistic nature of machine learning models,
and apply appropriate thresholds when evaluating confidence scores for
predictions.

## Privacy and security

![A diagram of a padlock.](../../wwl-data-ai/prepare-azure-ai-
development/media/privacy-security.png)

AI systems should be secure and respect privacy. The machine learning models
on which AI systems are based rely on large volumes of data, which may contain
personal details that must be kept private. Even after models are trained and
the system is in production, they use new data to make predictions or take
action that may be subject to privacy or security concerns; so appropriate
safeguards to protect data and customer content must be implemented.

## Inclusiveness

![A diagram of a diverse group of people.](../../wwl-data-ai/prepare-azure-ai-
development/media/inclusiveness.png)

AI systems should empower everyone and engage people. AI should bring benefits
to all parts of society, regardless of physical ability, gender, sexual
orientation, ethnicity, or other factors.

One way to optimize for inclusiveness is to ensure that the design,
development, and testing of your application includes input from as diverse a
group of people as possible.

## Transparency

![A diagram of an eye.](../../wwl-data-ai/prepare-azure-ai-
development/media/transparency.png)

AI systems should be understandable. Users should be made fully aware of the
purpose of the system, how it works, and what limitations may be expected.

For example, when an AI system is based on a machine learning model, you
should generally make users aware of factors that may affect the accuracy of
its predictions, such as the number of cases used to train the model, or the
specific features that have the most influence over its predictions. You
should also share information about the confidence score for predictions.

When an AI application relies on personal data, such as a facial recognition
system that takes images of people to recognize them; you should make it clear
to the user how their data is used and retained, and who has access to it.

## Accountability

![A diagram of a handshake.](../../wwl-data-ai/prepare-azure-ai-
development/media/accountability.png)

People should be accountable for AI systems. Although many AI systems seem to
operate autonomously, ultimately it's the responsibility of the developers who
trained and validated the models they use, and defined the logic that bases
decisions on model predictions to ensure that the overall system meets
responsibility requirements. To help meet this goal, designers and developers
of AI-based solution should work within a framework of governance and
organizational principles that ensure the solution meets responsible and legal
standards that are clearly defined.

Tip

For more information about Microsoft's principles for responsible AI, see
**[the Microsoft responsible AI site](https://microsoft.com/ai/responsible-
ai)**.

* * *"
6,Develop generative AI apps in Azure - Training,Plan and prepare to develop AI solutions on Azure,Exercise - Prepare for an AI development project,"# Exercise - Prepare for an AI development project

Completed 100 XP

  * 30 minutes

If you have an Azure subscription, you can explore Azure AI Foundry for
yourself.

Note

If you don't have an Azure subscription, and you want to explore Azure AI
Foundry, you can [sign up for an account](https://azure.microsoft.com/free),
which includes credits for the first 30 days.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/prepare-azure-ai-
development/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2260323)

* * *"
7,Develop generative AI apps in Azure - Training,Plan and prepare to develop AI solutions on Azure,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

1.

Which Azure resource provides language and vision services from a single
endpoint?

Azure AI Language

Azure AI Vision

Azure AI Services

2.

You plan to create a simple chat app that uses a generative AI model. What
kind of project should you create?

Azure AI Foundry project.

Azure AI hub based project.

Azure AI Custom Vision project.

3.

Which SDK enables you to connect to resources in a project?

Azure AI Services SDK

Semantic Kernel SDK

Azure AI Foundry SDK

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
8,Develop generative AI apps in Azure - Training,Plan and prepare to develop AI solutions on Azure,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/prepare-azure-ai-development/9-summary
9,Develop generative AI apps in Azure - Training,Choose and deploy models from the model catalog in Azure AI Foundry portal,Introduction,"# Introduction

Completed 100 XP

  * 1 minute

Generative AI applications are built on _language models_. The development
process usually starts with an exploration and comparison of available
_foundation_ models to find the one that best suits the particular needs of
your application. After selecting a suitable model, you deploy it to an
endpoint where it can be consumed by a client application or AI agent.

Foundation models, such as the GPT family of models, are state-of-the-art
language models designed to understand, generate, and interact with natural
language. Some common use cases for models are:

  * **Speech-to-text and text-to-speech conversion**. For example, generate subtitles for videos.
  * **Machine translation**. For example, translate text from English to Japanese.
  * **Text classification**. For example, label an email as spam or not spam.
  * **Entity extraction**. For example, extract keywords or names from a document.
  * **Text summarization**. For example, generate a short one-paragraph summary from a multi-page document.
  * **Question answering**. For example, provide answers to questions like ""What is the capital of France?""
  * **Reasoning**. For example, solve a mathematical problem.

In this module, you focus on exploring foundation models used for question
answering. The foundation models you explore can be used for chat applications
in which you use a language model to generate a response to a user's question.

Note

The latest breakthrough in generative AI models is owed to the development of
the **Transformer** architecture. Transformers were introduced in the
[_Attention is all you need_ paper by Vaswani, et al. from
2017](https://arxiv.org/abs/1706.03762). The Transformer architecture provided
two innovations to NLP that resulted in the emergence of foundation models:

  * Instead of processing words sequentially, Transformers process each word independently and in parallel by using **attention**.
  * Next to the semantic similarity between words, Transformers use **positional encoding** to include the information about the position of a word in a sentence.

* * *"
10,Develop generative AI apps in Azure - Training,Choose and deploy models from the model catalog in Azure AI Foundry portal,Explore the model catalog,"# Explore the model catalog

Completed 100 XP

  * 7 minutes

The _model catalog_ in Azure AI Foundry provides a central repository of
models that you can browse to find the right language model for your
particular generative AI use case.

![Screenshot of the model catalog in Azure AI Foundry portal.](../../wwl-data-
ai/explore-models-azure-ai-studio/media/model-catalog.png)

Selecting a foundation model for your generative AI app is important as it
affects how well your app works. To find the best model for your app, you can
use a structured approach by asking yourself the following questions:

  * Can AI _solve_ my use case?
  * How do I _select_ the best model for my use case?
  * Can I _scale_ for real-world workloads?

Let's explore each of these questions.

## Can AI _solve_ my use case?

Nowadays we have thousands of language models to choose from. The main
challenge is to understand if there's a model that satisfies your needs and to
answer the question: _Can AI solve my use case?_

To start answering this question, you need to discover, filter, and deploy a
model. You can explore the available language models through three different
catalogs:

  * [**Hugging Face**](https://huggingface.co/models): Vast catalog of open-source models across various domains.
  * [**GitHub**](https://github.com/marketplace/models-github): Access to diverse models via GitHub Marketplace and GitHub Copilot.
  * [**Azure AI Foundry**](https://ai.azure.com/explore/models): Comprehensive catalog with robust tools for deployment.

Though you can use each of these catalogs to explore models, the model catalog
in Azure AI Foundry makes it easiest to explore and deploy a model to build
you prototype, while offering the best selection of models.

Let's explore some of the options you need to consider when searching for
suitable models.

### Choose between large and small language models

First of all, you have a choice between Large Language Models (LLMs) and Small
Language Models (SLMs).

  * LLMs like GPT-4, Mistral Large, Llama3 70B, Llama 405B, and Command R+ are powerful AI models designed for tasks that require deep reasoning, complex content generation, and extensive context understanding.

  * SLMs like Phi3, Mistral OSS models, and Llama3 8B are efficient and cost-effective, while still handling many common Natural Language Processing (NLP) tasks. They're perfect for running on lower-end hardware or edge devices, where cost and speed are more important than model complexity.

### Focus on a modality, task, or tool

Language models like GPT-4 and Mistral Large are also known as **chat
completion** models, designed to generate coherent and contextually
appropriate text-based responses. When you need higher levels of performance
in complex tasks like math, coding, science, strategy, and logistics, you can
also use **reasoning** models like DeepSeek-R1 and o1.

Beyond text-based AI, some models are **multi-modal** , meaning they can
process images, audio, and other data types alongside text. Models like GPT-4o
and Phi3-vision are capable of analyzing and generating both text and images.
Multi-modal models are useful when your application needs to process and
understand images, such as in computer vision or document analysis. Or when
you want to build an AI app that interacts with visual content, such as a
digital tutor explaining images or charts.

If your use case involves **generating images** , tools like DALL·E 3 and
Stability AI can create realistic visuals from text prompts. Image generation
models are great for designing marketing materials, illustrations, or digital
art.

Another group of task-specific models are **embedding models** like Ada and
Cohere. Embeddings models convert text into numerical representations and are
used to improve search relevance by understanding semantic meaning. These
models are often implemented in **Retrieval Augmented Generation** ( **RAG** )
scenarios to enhance recommendation engines by linking similar content.

When you want to build an application that interacts with other software tools
dynamically, you can add **function calling** and **JSON support**. These
capabilities allow AI models to work efficiently with structured data, making
them useful for automating API calls, database queries, and structured data
processing.

### Specialize with regional and domain-specific models

Certain models are designed for specific languages, regions, or industries.
These models can outperform general-purpose generative AI in their respective
domains. For example:

  * Core42 JAIS is an Arabic language LLM, making it the best choice for applications targeting Arabic-speaking users.
  * Mistral Large has a strong focus on European languages, ensuring better linguistic accuracy for multilingual applications.
  * Nixtla TimeGEN-1 specializes in time-series forecasting, making it ideal for financial predictions, supply chain optimization, and demand forecasting.

If your project has regional, linguistic, or industry-specific needs, these
models can provide more relevant results than general-purpose AI.

### Balance flexibility and performance with open versus proprietary models

You also need to decide whether to use open-source models or proprietary
models, each with its own advantages.

**Proprietary models** are best for cutting-edge performance and enterprise
use. Azure offers models like OpenAI’s GPT-4, Mistral Large, and Cohere
Command R+, which deliver industry-leading AI capabilities. These models are
ideal for businesses needing enterprise-level security, support, and high
accuracy.

**Open-source models** are best for flexibility and cost-efficiency. There are
hundreds of open-source models available in the Azure AI Foundry model catalog
from Hugging Face, and models from Meta, Databricks, Snowflake, and Nvidia.
Open models give developers more control, allowing fine-tuning, customization,
and local deployment.

Whatever model you choose, you can use the Azure AI Foundry model catalog.
Using models through the model catalog meets the key enterprise requirements
for usage:

  * **Data and privacy** : you get to decide what happens with your data.
  * **Security and compliance** : built-in security.
  * **Responsible AI and content safety** : evaluations and content safety.

Now you know the language models that are available to you, you should have an
understanding of whether AI can indeed solve your use case. If you think a
language model would enrich your application, you then need to select the
specific model that you want to deploy and integrate.

## How do I _select_ the best model for my use case?

To select the best language model for you use case, you need to decide on what
criteria you're using to filter the models. The criteria are the necessary
characteristics you identify for a model. Four characteristics you can
consider are:

  * **Task type** : What type of task do you need the model to perform? Does it include the understanding of only text, or also audio, or video, or multiple modalities?
  * **Precision** : Is the base model good enough or do you need a fine-tuned model that is trained on a specific skill or dataset?
  * **Openness** : Do you want to be able to fine-tune the model yourself?
  * **Deployment** : Do you want to deploy the model locally, on a serverless endpoint, or do you want to manage the deployment infrastructure?

You already explored the various types of models available in the previous
section. Now, let's explore in more detail how precision and performance can
be important filters when choosing a model.

### Filter models for precision

In generative AI, precision refers to the accuracy of the model in generating
correct and relevant outputs. It measures the proportion of true positive
results (correct outputs) among all generated outputs. High precision means
fewer irrelevant or incorrect results, making the model more reliable.

When integrating a language model into an app, you can choose between a base
model or a fine-tuned model. A base model, like GPT-4, is pretrained on a
large dataset and can handle various tasks but can lack precision for specific
domains. Techniques like prompt engineering can improve this, but sometimes
fine-tuning is necessary.

A fine-tuned model is trained further on a smaller, task-specific dataset to
improve its precision and ability to generate relevant outputs for specific
applications. You can either use a fine-tuned model or fine-tune a model
yourself.

### Filter models for performance

You can evaluate your model performance at different phases, using various
evaluation approaches.

When you're exploring models through the Azure AI Foundry model catalog, you
can use **model benchmarks** to compare publicly available metrics like
coherence and accuracy across models and datasets. These benchmarks can help
you in the initial exploration phase, but give little information on how the
model would perform in your specific use case.

Expand table

Benchmark | Description  
---|---  
**Accuracy** | Compares model-generated text with correct answer according to
the dataset. Result is one if generated text matches the answer exactly, and
zero otherwise.  
**Coherence** | Measures whether the model output flows smoothly, reads
naturally, and resembles human-like language.  
**Fluency** | Assesses how well the generated text adheres to grammatical
rules, syntactic structures, and appropriate usage of vocabulary, resulting in
linguistically correct and natural-sounding responses.  
**Groundedness** | Measures alignment between the model's generated answers
and the input data.  
**GPT Similarity** | Quantifies the semantic similarity between a ground truth
sentence (or document) and the prediction sentence generated by an AI model.  
**Quality index** | A comparative aggregate score between 0 and 1, with
better-performing models scoring a higher value  
**Cost** | The cost of using the model based on a price-per-token. Cost is a
useful metric with which to compare quality, enabling you to determine an
appropriate tradeoff for your needs.  
  
To evaluate how a selected model performs regarding your specific
requirements, you can consider **manual** or **automated** evaluations. Manual
evaluations allow you to rate your model's responses. Automated evaluations
include traditional machine learning metrics and AI-assisted metrics that are
calculated and generated for you.

When you evaluate a model’s performance, it's common to start with manual
evaluations, as they quickly assess the quality of the model’s responses. For
more systematic comparisons, automated evaluations using metrics like
precision, recall, and F1 score based on your own ground truth offer a faster,
scalable, and more objective approach.

## Can I _scale_ for real-world workloads?

You selected a model for your use case and have successfully built a
prototype. Now, you need to understand how to scale for real-world workloads.

Considerations for scaling a generative AI solution include:

  * **Model deployment** : Where will you deploy the model for the best balance of performance and cost?
  * **Model monitoring and optimization** : How will you monitor, evaluate, and optimize model performance?
  * **Prompt management** : How will you orchestrate and optimize prompts to maximize the accuracy and relevance of generated responses?
  * **Model lifecycle** : How will you manage model, data, and code updates as part of an ongoing _Generative AI Operations_ (GenAIOps) lifecycle?

Azure AI Foundry provides visual and code-first tools that can help you build
and maintain a scalable generative AI solution.

* * *"
11,Develop generative AI apps in Azure - Training,Choose and deploy models from the model catalog in Azure AI Foundry portal,Deploy a model to an endpoint,"# Deploy a model to an endpoint

Completed 100 XP

  * 5 minutes

When you develop a generative AI app, you need to integrate language models
into your application. To be able to use a language model, you need to deploy
the model. Let's explore how to deploy language models in the Azure AI
Foundry, after first understanding why to deploy a model.

## Why deploy a model?

You train a model to generate output based on some input. To get value out of
your model, you need a solution that allows you to send input to the model,
which the model processes, after which the output is visualized for you.

With generative AI apps, the most common type of solution is a chat
application that expects a user question, which the model processes, to
generate an adequate response. The response is then visualized to the user as
a response to their question.

![Diagram of user question being processed by model deployed to
endpoint.](../../wwl-data-ai/explore-models-azure-ai-studio/media/request-
endpoint.png)

You can integrate a language model with a chat application by deploying the
model to an **endpoint**. An endpoint is a specific URL where a deployed model
or service can be accessed. Each model deployment typically has its own unique
endpoint, which allows different applications to communicate with the model
through an **API** ( **Application Programming Interface** ).

When a user asks a question:

  1. An API request is sent to the endpoint.
  2. The endpoint specifies the model that processes the request.
  3. The result is sent back to the app through an API response.

Now that you understand why you want to deploy a model, let's explore the
deployment options with Azure AI Foundry.

## Deploy a language model with Azure AI Foundry

When you deploy a language model with Azure AI Foundry, you have several types
available, which depend on the model you want to deploy.

Deploy options include:

  * **Standard deployment** : Models are hosted in the Azure AI Foundry project resource.
  * **Serverless compute** : Models are hosted in Microsoft-managed dedicated serverless endpoints in an Azure AI Foundry hub project.
  * **Managed compute** : Models are hosted in managed virtual machine images in an Azure AI Foundry hub project.

The associated cost depends on the type of model you deploy, which deployment
option you choose, and what you are doing with the model:

Expand table

| Standard deployment | Serverless compute | Managed compute  
---|---|---|---  
Supported models | Azure AI Foundry models (including Azure OpenAI models and
Models-as-a-service models) | Foundry Models with pay-as-you-go billing | Open
and custom models  
Hosting service | Azure AI Foundry resource | AI Project resource in a hub |
AI Project resource in a hub  
Billing basis | Token-based billing | Token-based billing | Compute-based
billing  
  
Note

_Standard deployment_ is recommended for most scenarios.

* * *"
12,Develop generative AI apps in Azure - Training,Choose and deploy models from the model catalog in Azure AI Foundry portal,Optimize model performance,"# Optimize model performance

Completed 100 XP

  * 8 minutes

After you deploy your model to an endpoint, you can start interacting with it
to see how it works. Let's explore how you can use prompt engineering
techniques to optimize your model's performance.

## Apply prompt patterns to optimize your model's output

The quality of the questions you send to the language model, directly
influences the quality of the responses you get back. You can carefully
construct your question, or **prompt** , to receive better and more
interesting responses. The process of designing and optimizing prompts to
improve the model's performance is also known as **prompt engineering**.

Prompt engineering requires users to ask relevant, specific, unambiguous, and
well-structured questions, instructing the model to generate more accurate
responses. To understand how to create well-defined prompts, let's explore
some patterns that help you improve the output of a model:

  * Instruct the model to act as a persona.
  * Guide the model to suggest better questions.
  * Provide a template to generate output in a specific format.
  * Understand how a model reasons by asking it to reflect.
  * Add context to improve the accuracy of the model's output.

Note

The patterns listed here are based on the work by [White et al., (2023) who
constructed a prompt pattern catalog](https://arxiv.org/abs/2302.11382).

As a developer, an effective approach is to add instructions to your deployed
model in the form of a _system prompt_. The system prompt sets the model's
behavior and allows you to guide the model without exposing the end user to
the instructions. The best results are often achieved through the assignment
of an explicit system prompt and guidance (or templates) that help users
submit prompts that reflect the patterns described here.

### Instruct the model to act as a persona

When you want the model to take a specific point of view or perspective when
answering a question, you can ask it to act as a persona.

For example, you can ask the model to respond as if it's a seasoned marketing
professional if you're working in sales and need insights about a new CRM
software.

Expand table

| No persona specified | With persona specified  
---|---|---  
System prompt | _None_ | You're a seasoned marketing professional that writes
advertising copy for an audience of technical customers.  
User prompt | Write a one-sentence description of a CRM product. | Write a
one-sentence description of a CRM product.  
Model response | A CRM product is a software tool designed to manage a
company's interactions with current and potential customers, encompassing
sales, marketing, customer service, and technical support. | Experience
seamless customer relationship management with our CRM product, designed to
streamline your business operations, enhance customer interactions, and drive
sales growth with its intuitive interface and robust analytics tools.  
  
You can create different personas for various business roles. Think of roles
like a product manager, data analyst, customer service expert, or teacher.
Define the persona and add more detail to what you expect that persona to know
or do to get tailored, context-driven responses.

### Ask for better question suggestions

When you want the model to help refine your question, you can ask it to
suggest better ways to phrase your query. You can also ask it to provide extra
context.

For example, if you're seeking advice on cooking a meal, you can prompt the
model to suggest clarifying questions to help you get a more targeted answer.

Expand table

| No suggestions given | With suggestions given  
---|---|---  
System prompt | You're an AI chef who can advise on menus. | You're an AI chef
who can advise on menus. You're approachable and always want to provide the
best advice you can.  
User prompt | I have to host a dinner party for four people. What should I
cook? | I have to host a dinner party for four people. What should I cook?
What other information do you need to help me plan a great meal for my guests?  
Model response | Hosting a dinner party for four is a wonderful opportunity to
showcase your culinary skills. Here's a suggested three-course menu that
balances flavors and provides a pleasant dining experience:  
  
 _Appetizer_  
 **Caprese Salad Skewers**  
\- _Ingredients_ : Cherry tomatoes, fresh mozzarella balls, fresh basil
leaves, balsamic glaze, olive oil, salt, and pepper.  
\- _Instructions_ : Thread a cherry tomato, a basil leaf, and a mozzarella
ball onto small skewers. Drizzle with olive oil and balsamic glaze, then
season with salt and pepper. This dish is light, refreshing, and easy to
prepare ahead of time.  
  
 _Main Course_  
 **Herb-Crusted Salmon with Lemon Asparagus**  
\- _Ingredients_ : Salmon fillets, fresh herbs (such as parsley, dill, and
thyme), breadcrumbs, lemon, asparagus, olive oil, salt, and pepper.  
\- _Instructions_ : Preheat your oven to 400°F (200°C). Mix breadcrumbs with
chopped fresh herbs and a bit of olive oil to form a paste. Press this mixture
onto the salmon fillets and season with salt and pepper. Place salmon on a
baking sheet and bake for about 15-20 minutes, or until cooked through. In the
last 10 minutes of cooking, toss asparagus with olive oil, lemon zest, salt,
and pepper, and roast alongside the salmon. Serve with a squeeze of fresh
lemon juice over the salmon and asparagus.  
  
 _Dessert_  
 **Chocolate Mousse with Fresh Berries**  
- _Ingredients_ : Dark chocolate, eggs, sugar, heavy cream, vanilla extract, and mixed berries (such as strawberries, blueberries, and raspberries).  
\- _Instructions_ : Melt the chocolate over a double boiler, then let it cool
slightly. Separate egg yolks and whites. Whip the egg whites to soft peaks,
then gradually add sugar to make a meringue. Whip the heavy cream until soft
peaks form and mix in vanilla extract. Fold the melted chocolate into the egg
yolks, then gently fold in the whipped cream and meringue until combined.
Spoon the mousse into serving dishes and refrigerate for at least 2 hours.
Serve topped with fresh berries for a burst of color and freshness.  
  
 **Additional Tips**  
\- _Pairings_ : Consider serving a light white wine with the appetizer and
main course, and perhaps a dessert wine with the chocolate mousse.  
\- _Timing_ : Prepare the appetizer and dessert in advance to minimize last-
minute stress. The main course will take the most time in the oven, so plan
accordingly.  
  
This menu is designed to be elegant yet approachable, ensuring your guests
enjoy both the food and the experience. Enjoy your dinner party! | Hosting a
dinner party is a wonderful opportunity to showcase your culinary skills and
create memorable experiences for your guests. To help you plan a great meal,
here are a few questions that can guide you in choosing the perfect menu:  
  

  *  **Dietary Preferences/Restrictions** : Are any of your guests vegetarian, vegan, gluten-free, or have any food allergies or intolerances? Knowing this will help in selecting dishes that everyone can enjoy.
  *  **Flavor Preferences** : Do your guests have any favorite cuisines or flavors? For example, do they enjoy Italian, Mexican, Mediterranean, or perhaps something more adventurous like Thai or Indian?
  *  **Time and Skill Level** : How much time do you have to prepare the meal, and what is your comfort level in the kitchen? Are you looking for a simple meal or are you up for something more elaborate?
  *  **Occasion** : Is this a casual get-together, a celebration, or something more formal? The nature of the event might influence the menu style.
  *  **Seasonal Ingredients** : Are you interested in using seasonal ingredients? This can help in choosing fresh and flavorful produce.
  *  **Drinks Pairing** : Would you like to include alcoholic or non-alcoholic beverages to pair with the meal?

  
With this information, I can suggest a tailored menu. Here’s a general idea
that can be adapted based on your answers:  
  
 **Starter**

  *  **Garden Salad with Citrus Vinaigrette** : Fresh greens, seasonal vegetables, and a light citrus dressing.
  *  **Stuffed Mushrooms** : For a vegetarian option, filled with herbs and cheese.

  
 **Main Course**  

  *  **Herb-Crusted Chicken Breast** : Served with roasted vegetables.
  *  **Quinoa Stuffed Bell Peppers** : A flavorful vegetarian or vegan option.

  
 **Side Dishes**  

  *  **Garlic Butter Roasted Potatoes** : Classic and comforting.
  *  **Steamed Green Beans with Almonds** : A simple, healthy side.

  
 **Dessert**  

  *  **Chocolate Lava Cake** : A decadent treat to end the meal.
  *  **Fruit Tart** : A lighter, refreshing option.

  
 **Drinks**  

  *  **Wine Pairing** : A versatile white wine like Sauvignon Blanc or a light red like Pinot Noir.
  *  **Mocktail Option** : A refreshing citrus mocktail or sparkling water with herbs.

  
  
Feel free to share more details, and I can help refine the menu further!  
  
When you encourage the model to suggest clarifications or extra details can
help you ask the right questions and achieve better, more accurate answers in
fewer interactions.

### Specify the desired format for responses

When you want the model to generate output in a specific format, you can
provide a template or structure in your prompt.

For example, if you're a sports reporting composing a historical article, you
can request that the model follow a specific template, which includes
headings, bullet points, and data breakdowns.

Expand table

| No template specified | With template specified  
---|---|---  
System prompt | You're a helpful AI assistant. | You're a helpful AI assistant
for sports reporters.  
User prompt | What happened in the 2018 Soccer World Cup final? | What
happened in the 2018 Soccer World Cup final? Format the result to show the
match date, location, and the two teams competing. Then the final score, and
finally any notable events that occurred during the match.  
Model response | The 2018 FIFA World Cup final took place on July 15, 2018, at
the Luzhniki Stadium in Moscow, Russia. France faced Croatia in the match.
France emerged victorious with a 4-2 win. The goals for France were scored by
Mario Mandzukic (own goal), Antoine Griezmann (penalty), Paul Pogba, and
Kylian Mbappé. Croatia's goals were scored by Ivan Perišić and Mandzukic.
Kylian Mbappé, at just 19 years old, became the second youngest player to
score in a World Cup final, after Pelé. This victory marked France's second
World Cup title, their first being in 1998. | **Match Date** : July 15, 2018  
 **Location** : Luzhniki Stadium, Moscow, Russia  
 **Teams Competing** : France vs. Croatia  
 **Final Score** : France 4, Croatia 2  
  
 **Notable Events** :  

  1.  **First Half** : France took an early lead in the 18th minute with an own goal by Mario Mandžukić after a cross from Antoine Griezmann. Croatia equalized in the 28th minute with a header from Ivan Perišić, who had just come on as a substitute. Just before halftime, in the 44th minute, Paul Pogba scored from a free kick to put France back in the lead.
  2.  **Second Half** : The game saw a dramatic increase in intensity after the break. In the 68th minute, Kylian Mbappé scored his first World Cup goal, bringing the score to 3-1 in favor of France. Croatia managed to score again in the 69th minute when Mandžukić headed in a cross from Perišić, reducing the deficit to 3-2.
  3.  **Final Moments** : In the 81st minute, a controversial penalty was awarded to France after a handball by Croatian defender Dejan Lovren. Griezmann converted the penalty, sealing the victory for France with a 4-2 scoreline.
  4.  **Additional Context** : This match was notable for being the youngest ever World Cup final in terms of average age, featuring players like Mbappé and N’Golo Kanté. France’s victory marked their second World Cup title, having previously won in 1998. Croatia reached the final for the first time in their history, finishing as runners-up.

  
  
You can apply this approach to other scenarios where a specific format is
needed, such as generating emails, summaries, proposals, or even code and
scripts. Define the format template clearly and provide details on how you
want the output structured to get consistent and organized responses.

You can also use a **one-shot** or **few-shots** approach by providing one or
more examples to help the model identify a desired pattern.

### Ask for an explanation of reasoning

When you want the model to explain the reasoning behind its answers, you can
ask the model to automatically reflect on its rationale and assumptions after
providing a response.

For example, if you're working on a mathematical problem, you can ask the
model to explain the reasoning behind specific calculations.

Expand table

| No reflection specified | With reflection specified  
---|---|---  
System prompt | You're an AI math assistant. | You're an AI math assistant.
You always explain your answers.  
User prompt | A right-angled triangle has a hypotenuse side of length 3 cm and
an adjacent side length of 2cm.What is the length of the remaining side? | A
right-angled triangle has a hypotenuse side of length 3 cm and an adjacent
side length of 2cm.What is the length of the remaining side?  
Model response | Approximately 2.24 cm. | We can use the Pythagorean theorem
to solve this problem. The Pythagorean theorem states that in a right-angled
triangle, the square of the length of the hypotenuse (the side opposite the
right angle) is equal to the sum of the squares of the lengths of the other
two sides. Mathematically, this can be written as:  
 _c² = a² + b²_  
where _c_ is the length of the hypotenuse, and _a_ and _b_ are the lengths of
the other two sides.  
In this problem, we are given the length of the hypotenuse (c = 3 cm) and the
length of one of the other sides (a = 2 cm). We need to find the length of the
remaining side (b).  
Plugging the given values into the Pythagorean theorem, we get:  
 _3² = 2² + b²_  
 _9 = 4 + b²_  
Now, we can solve for b²:  
 _b² = 9 - 4_  
 _b² = 5_  
Finally, we take the square root of both sides to find the length of the
remaining side:  
 _b = √5 ≈ 2.24 cm_  
So, the length of the remaining side is approximately 2.24 cm.  
  
You can apply this approach when you want explanations in data analysis,
marketing strategy, or technical troubleshooting. When you ask the model to
define its reasoning, you use a technique called **chain-of-thought** to make
it think step by step.

### Add context

When you want the model to focus on specific topics, you can specify the
context to consider. You can also tell the model to ignore irrelevant
information.

For example, if you're planning a trip, you can provide the model with more
context to help improve the relevance of its response.

Expand table

| No context specified | With context specified  
---|---|---  
System prompt | You're an AI travel assistant. | You're an AI travel
assistant.  
User question | When should I visit Edinburgh? | When should I visit
Edinburgh? I'm particularly interested in attending Scotland's home matches in
the Six Nations rugby tournament.  
Model response | Edinburgh, the capital city of Scotland, is a wonderful
destination with a rich history and vibrant culture. The best time to visit
Edinburgh largely depends on your preferences for weather, events, and tourist
crowds.  

  1. Spring (April to June): The weather starts to warm up, and the days get longer. Springtime is perfect for exploring the city's historic sites, gardens, and parks, such as the Royal Botanic Garden. The Edinburgh Festival Fringe, the world's largest arts festival, takes place in August, which is the end of summer.
  2. Summer (July to August): The weather is at its warmest, and the days are long. This is a popular time to visit, but it can also be crowded with tourists. Summer is ideal for outdoor activities, and you can enjoy the Edinburgh International Festival and the Royal Edinburgh Military Tattoo.
  3. Autumn (September to November): The weather begins to cool down, and the leaves change color, creating a picturesque backdrop for the city's historic architecture. It's less crowded, and you can enjoy events like the Edinburgh International Book Festival.
  4. Winter (December to February): The weather is colder, and there may be snowfall, which can create a magical atmosphere. Winter is a quieter time to visit, with fewer tourists and lower prices. You can attend events like the Edinburgh Christmas Market and the Hogmanay Street Party, which celebrates the New Year.

  
In summary, the best time to visit Edinburgh depends on your preferences.
Spring and autumn offer pleasant weather and fewer crowds, while summer is
ideal for outdoor activities and festivals. Winter can be a quieter, more
magical experience, with festive events to enjoy. | The Six Nations rugby
tournament typically runs from late January to early March. Edinburgh, as a
host city, often has matches at Murrayfield Stadium, which is Scotland's
national stadium. To ensure you don't miss Scotland's home matches in the Six
Nations, I recommend planning your visit for this period.  
  
However, keep in mind that match dates and times can vary, so it's a good idea
to check the official Six Nations website or the Scottish Rugby Union's
website for the exact schedule of matches. Additionally, Edinburgh has a lot
to offer beyond rugby, with its rich history, stunning architecture, and
vibrant cultural scene, so you might consider extending your stay to explore
the city before or after the tournament.  
  
Also, remember that visiting during the tournament might mean larger crowds
and busier accommodations, so book your flights, hotel, and transportation
well in advance to ensure a smooth experience. Enjoy your trip to Edinburgh
and the thrilling atmosphere of the Six Nations rugby matches!  
  
By defining what the model should focus on or disregard, you can ensure the
conversation stays on track and generate more relevant, tailored responses.

You can specify the context by describing what it should or shouldn't include,
and by connecting the model to data sources it should retrieve context from
before generating an answer.

## Apply model optimization strategies

Note

This section discusses options and considerations for model optimization that
you may consider beyond prompt engineering. A full exploration of _how_ to
apply these optimization strategies is beyond the scope of this module.

Prompt engineering can be an effective way to optimize model responses, but in
some cases it may not provide sufficient context or guidance to always meet
your exact needs. As a developer, you can consider the following additional
optimization strategies to improve the relevance of your generative AI
application's responses:

  * **Retrieval Augmented Generation (RAG)** : A technique that involves using a data source to provide _grounding_ context to prompts. RAG can be a useful approach when you need the model to answer questions based on a specific knowledge domain or when you need the model to consider information related to events that occurred after the training data on which the model is based.
  * **Fine-tuning** : A technique that involves extending the training of a foundation model by providing example prompts and responses that reflect the desired output format and style.

Both of these approaches involve additional cost, complexity, and
maintainability challenges, so as a general rule it's best to start your
optimization efforts through prompt engineering, and then consider additional
strategies if necessary.

The strategy you should choose as a developer depends on your requirements:

![Diagram showing the various strategies to optimize the model's
performance.](../../wwl-data-ai/explore-models-azure-ai-studio/media/model-
optimization.png)

  * **Optimize for context** : When the model lacks contextual knowledge and you want to **maximize responses accuracy**.
  * **Optimize the model** : When you want to improve the response format, style, or speech by **maximizing consistency of behavior**.

To optimize for context, you can apply a **Retrieval Augmented Generation** (
**RAG** ) pattern. With RAG, you _ground_ your data by first retrieving
context from a data source before generating a response. For example, you want
employees to ask questions about expense claim processes and limits based on
your own corporation's expenses policy documentation.

When you want the model to respond in a specific style or format, you can
instruct the model to do so by adding guidelines in the system message. When
you notice the model's behavior isn't consistent, you can further enforce
consistency in behavior by **fine-tuning** a model. With fine-tuning, you
train a base language model on a dataset of example prompts and responses
before integrating it in your application, with the result that the fine-tuned
model will produce responses that are consistent with the examples in the
fine-tuning training dataset.

You can use any combination of optimization strategies, for example prompt
engineering, RAG _and_ a fine-tuned model, to improve your language
application.

* * *"
13,Develop generative AI apps in Azure - Training,Choose and deploy models from the model catalog in Azure AI Foundry portal,"Exercise - Explore, deploy, and chat with language models","# Exercise - Explore, deploy, and chat with language models

Completed 100 XP

  * 15 minutes

Now, it's your chance to explore how to select, deploy, and test language
models in the Azure AI Foundry portal.

Note

To complete this lab, you will need an [Azure
subscription](https://azure.microsoft.com/free) in which you have
administrative access.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/explore-models-azure-ai-
studio/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2277718)

* * *"
14,Develop generative AI apps in Azure - Training,Choose and deploy models from the model catalog in Azure AI Foundry portal,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

1.

Where can you test a deployed model in the Azure AI Foundry portal?

Chat playground

Sandbox

Development toolbox

2.

You want to specify the tone, format, and content for each interaction with
your model in the playground. What should you use to customize the model
response?

Benchmarks

Grounding

System message

3.

What deployment option should you choose to host an OpenAI model in an Azure
AI Foundry resource?

Standard deployment

Serverless compute

Managed compute

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
15,Develop generative AI apps in Azure - Training,Choose and deploy models from the model catalog in Azure AI Foundry portal,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/explore-models-azure-ai-studio/7-summary
16,Develop generative AI apps in Azure - Training,Develop an AI app with the Azure AI Foundry SDK,Introduction,"# Introduction

Completed 100 XP

  * 1 minute

Developers creating AI solutions with Azure AI Foundry need to work with a
combination of services and software frameworks. The Azure AI Foundry SDK is
designed to bring together common services and code libraries in an AI project
through a central programmatic access point, making it easier for developers
to write the code needed to build effective AI apps on Azure.

In this module, you'll learn how to use the Azure AI Foundry SDK to work with
resources in an AI project.

Note

Azure AI Foundry SDK is currently in public preview. Details described in this
module are subject to change.

* * *"
17,Develop generative AI apps in Azure - Training,Develop an AI app with the Azure AI Foundry SDK,What is the Azure AI Foundry SDK?,"# What is the Azure AI Foundry SDK?

Completed 100 XP

  * 5 minutes

Azure AI Foundry provides a REST API that you can use to work with AI Foundry
projects and the resources they contain. Additionally, multiple language-
specific SDKs are available, enabling developers to write code that uses
resources in an Azure AI Foundry project in their preferred development
language. With an Azure AI Foundry SDK, developers can create applications
that connect to a project, access the resource connections and models in that
project, and use them to perform AI operations, such as sending prompts to a
generative AI model and processing the responses.

The core package for working with projects is the **Azure AI Projects**
library, which enables you to connect to an Azure AI Foundry project and
access the resources defined within it. Available language-specific packages
the for Azure AI Projects library include:

  * [Azure AI Projects for Python](https://pypi.org/project/azure-ai-projects)
  * [Azure AI Projects for Microsoft .NET](https://www.nuget.org/packages/Azure.AI.Projects)
  * [Azure AI Projects for JavaScript](https://www.npmjs.com/package/@azure/ai-projects)

Note

In this module, we'll use Python code examples for common tasks that a
developer may need to perform with Azure AI Foundry projects. You can refer to
the other language-specific SDK documentation to find equivalent code for your
preferred language. Each SDK is developed and maintained independently, so
some functionality may be at different stages of implementation for each
language.

To use the Azure AI Projects library in Python, you can use the **pip**
package installation utility to install the **azure-ai-projects** package from
PyPi:

Copy

    
    
    pip install azure-ai-projects
    

## Using the SDK to connect to a project

The first task in most Azure AI Foundry SDK code is to connect to an Azure AI
Foundry project. Each project has a unique _endpoint_ , which you can find on
the project's **Overview** page in the Azure AI Foundry portal.

[ ![Screenshot of the project overview page in Azure AI Foundry
portal.](../../wwl-data-ai/ai-foundry-sdk/media/ai-project-overview.png)
](../../wwl-data-ai/ai-foundry-sdk/media/ai-project-overview.png#lightbox)

Note

The project provides multiple endpoints and keys, including:

  * An endpoint for the project itself; which can be used to access project connections, agents, and models in the Azure AI Foundry resource.
  * An endpoint for Azure OpenAI Service APIs in the project's Azure AI Foundry resource.
  * An endpoint for Azure AI services APIs (such as Azure AI Vision and Azure AI Language) in the Azure AI Foundry resource.

You can use the project endpoint in your code to create an **AIProjectClient**
object, which provides a programmatic proxy for the project, as shown in this
Python example:

Python Copy

    
    
    from azure.identity import DefaultAzureCredential
    from azure.ai.projects import AIProjectClient
    ...
    
    project_endpoint = ""https://......""
    project_client = AIProjectClient(            
        credential=DefaultAzureCredential(),
        endpoint=project_endpoint)
    

Note

The code uses the default Azure credentials to authenticate when accessing the
project. To enable this authentication, in addition to the **azure-ai-
projects** package, you need to install the **azure-identity** package:

`pip install azure-identity`

Tip

To access the project successfully, the code must be run in the context of an
authenticated Azure session. For example, you could use the Azure command-line
interface (CLI) `az-login` command to sign in before running the code.

* * *"
18,Develop generative AI apps in Azure - Training,Develop an AI app with the Azure AI Foundry SDK,Work with project connections,"# Work with project connections

Completed 100 XP

  * 5 minutes

Each Azure AI Foundry project includes **connected resources** , which are
defined both at the _parent_ (Azure AI Foundry resource or hub) level, and at
the _project_ level. Each resource is a _connection_ to an external service,
such as Azure storage, Azure AI Search, Azure OpenAI, or another Azure AI
Foundry resource.

[ ![Screenshot of the connected resources page in Azure AI Foundry
portal.](../../wwl-data-ai/ai-foundry-sdk/media/ai-project-connections.png)
](../../wwl-data-ai/ai-foundry-sdk/media/ai-project-connections.png#lightbox)

With the Azure AI Foundry SDK, you can connect to a project and retrieve
connections; which you can then use to consume the connected services.

For example, the **AIProjectClient** object in Python has a **connections**
property, which you can use to access the resource connections in the project.
Methods of the **connections** object include:

  * `connections.list()`: Returns a collection of connection objects, each representing a connection in the project. You can filter the results by specifying an optional **connection_type** parameter with a valid enumeration, such as `ConnectionType.AZURE_OPEN_AI`.
  * `connections.get(connection_name, include_credentials)`: Returns a connection object for the connection with the name specified. If the **include_credentials** parameter is **True** (the default value), the credentials required to connect to the connection are returned - for example, in the form of an API key for an Azure AI services resource.

The connection objects returned by these methods include connection-specific
properties, including credentials, which you can use to connect to the
associated resource.

The following code example lists all of the resource connections that have
been added to a project:

Python Copy

    
    
    from azure.identity import DefaultAzureCredential
    from azure.ai.projects import AIProjectClient
    
    try:
    
        # Get project client
        project_endpoint = ""https://.....""
        project_client = AIProjectClient(            
                credential=DefaultAzureCredential(),
                endpoint=project_endpoint,
            )
        
        ## List all connections in the project
        connections = project_client.connections
        print(""List all connections:"")
        for connection in connections.list():
            print(f""{connection.name} ({connection.type})"")
    
    except Exception as ex:
        print(ex)
    

* * *"
19,Develop generative AI apps in Azure - Training,Develop an AI app with the Azure AI Foundry SDK,Create a chat client,"# Create a chat client

Completed 100 XP

  * 10 minutes

A common scenario in an AI application is to connect to a generative AI model
and use _prompts_ to engage in a chat-based dialog with it.

While you can use the Azure OpenAI SDK, to connect ""directly"" to a model using
key-based or Microsoft Entra ID authentication; when your model is deployed in
an Azure AI Foundry project, you can also use the Azure AI Foundry SDK to
retrieve a project client, from which you can then get an authenticated OpenAI
chat client for any models deployed in the project's Azure AI Foundry
resource. This approach makes it easy to write code that consumes models
deployed in your project, switching between them easily by changing the model
deployment name parameter.

Tip

You can use the OpenAI chat client provided by an Azure AI Foundry project to
chat with any model deployed in the associated Azure AI Foundry resource -
even non-OpenAI models, such as Microsoft Phi models.

The following Python code sample uses the **get_openai_client()** method to
get an OpenAI client with which to chat with a model that has been deployed in
the project'a Azure AI Foundry resource.

Python Copy

    
    
    from azure.identity import DefaultAzureCredential
    from azure.ai.projects import AIProjectClient
    from openai import AzureOpenAI
    
    try:
        
        # connect to the project
        project_endpoint = ""https://......""
        project_client = AIProjectClient(            
                credential=DefaultAzureCredential(),
                endpoint=project_endpoint,
            )
        
        # Get a chat client
        chat_client = project_client.get_openai_client(api_version=""2024-10-21"")
        
        # Get a chat completion based on a user-provided prompt
        user_prompt = input(""Enter a question:"")
        
        response = chat_client.chat.completions.create(
            model=your_model_deployment_name,
            messages=[
                {""role"": ""system"", ""content"": ""You are a helpful AI assistant.""},
                {""role"": ""user"", ""content"": user_prompt}
            ]
        )
        print(response.choices[0].message.content)
    
    except Exception as ex:
        print(ex)
    

Note

In addition to the **azure-ai-projects** and **azure-identity** packages
discussed previously, the sample code shown here assumes that the **openai**
package has been installed:

`pip install openai`

* * *"
20,Develop generative AI apps in Azure - Training,Develop an AI app with the Azure AI Foundry SDK,Exercise - Create a generative AI chat app,"# Exercise - Create a generative AI chat app

Completed 100 XP

  * 30 minutes

Now it's your turn to try using the Azure AI Foundry SDK!

In this exercise, you use the Azure AI Foundry SDK to connect to a project and
create an application that chats with a generative AI model.

Note

To complete this lab, you need an **[Azure
subscription](https://azure.microsoft.com/free)** in which you have
administrative access.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/ai-foundry-sdk/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2303427)

* * *"
21,Develop generative AI apps in Azure - Training,Develop an AI app with the Azure AI Foundry SDK,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

1.

What class in the Azure AI Foundry SDK provides a proxy object for a project?

ConnectionProperties

AIProjectClient

ChatCompletionsClient

2.

What value is needed to instantiate a AIProjectClient object?

The project endpoint.

The Azure OpenAI authorization key

The Azure subscription ID

3.

Which SDK should you use to chat with a model that is deployed in an Azure AI
Foundry resource?

Azure OpenAI

Azure Machine Learning

Azure AI Language

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
22,Develop generative AI apps in Azure - Training,Develop an AI app with the Azure AI Foundry SDK,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/ai-foundry-sdk/07-summary
23,Develop generative AI apps in Azure - Training,Get started with prompt flow to develop language model apps in the Azure AI Foundry,Introduction,"# Introduction

Completed 100 XP

  * 3 minutes

The true power of **Large Language Models** ( **LLMs** ) lies in their
application. Whether you want to use LLMs to classify web pages into
categories, or to build a chatbot on your data. To harness the power of the
LLMs available, you need to create an application that combines your data
sources with LLMs and generates the desired output.

To develop, test, tune, and deploy LLM applications, you can use **prompt
flow** , accessible in the [Azure Machine Learning
studio](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-
flow/overview-what-is-prompt-flow) and the [Azure AI Foundry
portal](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/prompt-flow).

Note

The focus of this module is on understanding and exploring prompt flow through
Azure AI Foundry. However, note that the content applies to the prompt flow
experience in both Azure Machine Learning and Azure AI Foundry.

Prompt flow takes a **prompt** as input, which in the context of LLMs, refers
to the query provided to the LLM application to generate a response. It's the
text or set of instructions given to the LLM application, prompting it to
generate output or perform a specific task.

For example, when you want to use a text generation model, the prompt might be
a sentence or a paragraph that initiates the generation process. In the
context of a question-answering model, the prompt could be a query asking for
information on a particular topic. The effectiveness of the prompt often
depends on how well it conveys the user's intent and the desired outcome.

Prompt flow allows you to create **flows** , which refers to the sequence of
actions or steps that are taken to achieve a specific task or functionality. A
flow represents the overall process or pipeline that incorporates the
interaction with the LLM to address a particular use case. The flow
encapsulates the entire journey from receiving input to generating output or
performing a desired action.

* * *"
24,Develop generative AI apps in Azure - Training,Get started with prompt flow to develop language model apps in the Azure AI Foundry,Understand the development lifecycle of a large language model (LLM) app,"# Understand the development lifecycle of a large language model (LLM) app

Completed 100 XP

  * 8 minutes

Before understanding how to work with prompt flow, let's explore the
development lifecycle of a Large Language Model (LLM) application.

The lifecycle consists of the following stages:

![Diagram of the four stages of the development lifecycle.](../../wwl-data-
ai/get-started-prompt-flow-ai-studio/media/development-lifecycle.png)

  1. **Initialization** : Define the use case and design the solution.
  2. **Experimentation** : Develop a flow and test with a small dataset.
  3. **Evaluation and refinement** : Assess the flow with a larger dataset.
  4. **Production** : Deploy and monitor the flow and application.

During both evaluation and refinement, and production, you might find that
your solution needs to be improved. You can revert back to experimentation
during which you develop your flow continuously, until you're satisfied with
the results.

Let's explore each of these phases in more detail.

## Initialization

Imagine you want to design and develop an LLM application to classify news
articles. Before you start creating anything, you need to define what
categories you want as output. You need to understand what a typical news
article looks like, how you present the article as input to your application,
and how the application generates the desired output.

In other words, during _initialization_ you:

![Diagram of the four steps during initialization.](../../wwl-data-ai/get-
started-prompt-flow-ai-studio/media/initialization.png)

  1. Define the **objective**
  2. Collect a **sample dataset**
  3. Build a **basic prompt**
  4. Design the **flow**

To design, develop, and test an LLM application, you need a sample dataset
that serves as the input. A sample dataset is a small representative subset of
the data you eventually expect to parse as input to your LLM application.

When collecting or creating the sample dataset, you should ensure diversity in
the data to cover various scenarios and edge cases. You should also remove any
privacy sensitive information from the dataset to avoid any vulnerabilities.

## Experimentation

You collected a sample dataset of news articles, and decided on which
categories you want the articles to be classified into. You designed a flow
that takes a news article as input, and uses an LLM to classify the article.
To test whether your flow generates the expected output, you run it against
your sample dataset.

![Diagram of the four steps during experimentation.](../../wwl-data-ai/get-
started-prompt-flow-ai-studio/media/experimentation.png)

The _experimentation_ phase is an iterative process during which you (1)
**run** the flow against a sample dataset. You then (2) **evaluate** the
prompt's performance. If you're (3) satisfied with the result, you can **move
on** to evaluation and refinement. If you think there's room for improvement,
you can (4) **modify** the flow by changing the prompt or flow itself.

## Evaluation and refinement

When you're satisfied with the output of the flow that classifies news
articles, based on the sample dataset, you can assess the flow's performance
against a larger dataset.

By testing the flow on a larger dataset, you can evaluate how well the LLM
application generalizes to new data. During evaluation, you can identify
potential bottlenecks or areas for optimization or refinement.

When you edit your flow, you should first run it against a smaller dataset
before running it again against a larger dataset. Testing your flow with a
smaller dataset allows you to more quickly respond to any issues.

Once your LLM application appears to be robust and reliable in handling
various scenarios, you can decide to move the LLM application to production.

## Production

Finally, your news article classification application is ready for
_production_.

![Diagram of the three steps during production.](../../wwl-data-ai/get-
started-prompt-flow-ai-studio/media/production.png)

During production, you:

  1. **Optimize** the flow that classifies incoming articles for efficiency and effectiveness.
  2. **Deploy** your flow to an endpoint. When you call the endpoint, the flow is triggered to run and the desired output is generated.
  3. **Monitor** the performance of your solution by collecting usage data and end-user feedback. By understanding how the application performs, you can improve the flow whenever necessary.

## Explore the complete development lifecycle

Now that you understand each stage of the development lifecycle of an LLM
application, you can explore the complete overview:

![Diagram of all stages including their steps of the development
lifecycle.](../../wwl-data-ai/get-started-prompt-flow-ai-
studio/media/detailed-lifecycle.png)

* * *"
25,Develop generative AI apps in Azure - Training,Get started with prompt flow to develop language model apps in the Azure AI Foundry,Understand core components and explore flow types,"# Understand core components and explore flow types

Completed 100 XP

  * 5 minutes

To create a Large Language Model (LLM) application with prompt flow, you need
to understand prompt flow's core components.

## Understand a flow

Prompt flow is a feature within Azure AI Foundry that allows you to author
**flows**. Flows are executable workflows often consist of three parts:

  1. **Inputs** : Represent data passed into the flow. Can be different data types like strings, integers, or boolean.
  2. **Nodes** : Represent _tools_ that perform data processing, task execution, or algorithmic operations.
  3. **Outputs** : Represent the data produced by the flow.

![Diagram of the three components of a flow pipeline.](../../wwl-data-ai/get-
started-prompt-flow-ai-studio/media/flow-pipeline.png)

Similar to a pipeline, a flow can consist of multiple nodes that can use the
flow's inputs or any output generated by another node. You can add a node to a
flow by choosing one of the available types of **tools**.

## Explore the tools available in prompt flow

Three common tools are:

  * **LLM tool** : Enables custom prompt creation utilizing Large Language Models.
  * **Python tool** : Allows the execution of custom Python scripts.
  * **Prompt tool** : Prepares prompts as strings for complex scenarios or integration with other tools.

Each tool is an executable unit with a specific function. You can use a tool
to perform tasks like summarizing text, or making an API call. You can use
multiple tools within one flow and use a tool multiple times.

Tip

If you're looking for functionality that is not offered by the available
tools, you can [create your own custom
tool](https://microsoft.github.io/promptflow/how-to-guides/develop-a-
tool/create-and-use-tool-package.html).

Whenever you add a new node to your flow, adding a new tool, you can define
the expected inputs and outputs. A node can use one of the whole flow's
inputs, or another node's output, effectively linking nodes together.

By defining the inputs, connecting nodes, and defining the desired outputs,
you can create a flow. Flows help you create LLM applications for various
purposes.

## Understand the types of flows

There are three different types of flows you can create with prompt flow:

  * **Standard flow** : Ideal for general LLM-based application development, offering a range of versatile tools.
  * **Chat flow** : Designed for conversational applications, with enhanced support for chat-related functionalities.
  * **Evaluation flow** : Focused on performance evaluation, allowing the analysis and improvement of models or applications through feedback on previous runs.

Now that you understand how a flow is structured and what you can use it for,
let's explore how you can create a flow.

* * *"
26,Develop generative AI apps in Azure - Training,Get started with prompt flow to develop language model apps in the Azure AI Foundry,Explore connections and runtimes,"# Explore connections and runtimes

Completed 100 XP

  * 4 minutes

When you create a Large Language Model (LLM) application with prompt flow, you
first need to configure any necessary **connections** and **runtimes**.

## Explore connections

Whenever you want your flow to connect to external data source, service, or
API, you need your flow to be authorized to communicate with that external
service. When you create a **connection** , you configure a secure link
between prompt flow and external services, ensuring seamless and safe data
communication.

![Diagram showing a flow with two nodes, connecting to Azure AI Search and
Azure OpenAI.](../../wwl-data-ai/get-started-prompt-flow-ai-
studio/media/connections.png)

Depending on the type of connection you create, the connection securely stores
the endpoint, API key, or credentials necessary for prompt flow to communicate
with the external service. Any necessary secrets aren't exposed to users, but
instead are stored in an Azure Key Vault.

By setting up connections, users can easily reuse external services necessary
for tools in their flows.

Certain built-in tools require you to have a connection configured:

Expand table

Connection type | Built-in tools  
---|---  
Azure OpenAI | LLM or Python  
OpenAI | LLM or Python  
Azure AI Search | Vector DB Lookup or Python  
Serp | Serp API or Python  
Custom | Python  
  
Prompt flow connections play pivotal roles in two scenarios. They automate API
credential management, simplifying and securing the handling of sensitive
access information. Additionally, they enable secure data transfer from
various sources, crucial for maintaining data integrity and privacy across
different environments.

## Explore runtimes

After creating your flow, and configuring the necessary connections your tools
use, you want to run your flow. To run the flow, you need compute, which is
offered through prompt flow **runtimes**.

![Diagram runtimes, compute, and environments in relation to
flows.](../../wwl-data-ai/get-started-prompt-flow-ai-
studio/media/runtimes.png)

Runtimes (1) are a combination of a **compute instance** (2) providing the
necessary compute resources, and an **environment** (3) specifying the
necessary packages and libraries that need to be installed before being able
to run the flow.

When you use runtimes, you have a controlled environment where flows can be
run and validated, ensuring that everything works as intended in a stable
setting. A default environment is available for quick development and testing.
When you require other packages to be installed, you can [create a custom
environment](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-
flow/how-to-customize-environment-runtime).

* * *"
27,Develop generative AI apps in Azure - Training,Get started with prompt flow to develop language model apps in the Azure AI Foundry,Explore variants and monitoring options,"# Explore variants and monitoring options

Completed 100 XP

  * 6 minutes

During production, you want to optimize and deploy your flow. Finally, you
want to monitor your flows to understand when improving your flows is
necessary.

You can optimize your flow by using **variants** , you can deploy your flow to
an **endpoint** , and you can monitor your flow by evaluating key metrics.

## Explore variants

Prompt flow **variants** are versions of a tool node with distinct settings.
Currently, variants are only supported in the LLM tool, where a variant can
represent a different prompt content or connection setting. Variants allow
users to customize their approach for specific tasks, like, summarizing news
articles.

Some benefits of using variants are:

  * **Enhance the quality of your LLM generation** : Creating diverse variants of an LLM node helps find the best prompt and settings for high-quality content.
  * **Save time and effort** : Variants allow for easy management and comparison of different prompt versions, streamlining historical tracking and reducing the effort in prompt tuning.
  * **Boost productivity** : They simplify the optimization of LLM nodes, enabling quicker creation and management of variations, leading to better results in less time.
  * **Facilitate easy comparison** : Variants enable side-by-side result comparisons, aiding in choosing the most effective variant based on data-driven decisions.

## Deploy your flow to an endpoint

When you're satisfied with the performance of your flow, you can choose to
deploy it to an **online endpoint**. Endpoints are URLs that you can call from
any application. When you make an API call to an online endpoint, you can
expect (almost) immediate response.

When you deploy your flow to an online endpoint, prompt flow generates a URL
and key so you can safely integrate your flow with other applications or
business processes. When you invoke the endpoint, a flow is run and the output
is returned in real-time. As a result, deploying flows to endpoints can for
example generate chat or agentic responses that you want to return in another
application.

## Monitor evaluation metrics

In prompt flow, monitoring evaluation metrics is key to understanding your LLM
application's performance, ensuring they meet real-world expectations and
deliver accurate results.

To understand whether your application is meeting practical needs, you can
collect end-user feedback and assess the application's usefulness. Another
approach to understanding whether your application is performing well, is by
comparing LLM predictions with expected or _ground truth_ responses to gauge
accuracy and relevance. Evaluating the LLM's predictions is crucial for
keeping LLM applications reliable and effective.

### Metrics

The key metrics used for monitoring evaluation in prompt flow each offer
unique insight into the performance of LLMs:

  * _**Groundedness**_ : Measures alignment of the LLM application's output with the input source or database.
  * _**Relevance**_ : Assesses how pertinent the LLM application's output is to the given input.
  * _**Coherence**_ : Evaluates the logical flow and readability of the LLM application's text.
  * _**Fluency**_ : Assesses the grammatical and linguistic accuracy of the LLM application's output.
  * _**Similarity**_ : Quantifies the contextual and semantic match between the LLM application's output and the ground truth.

Metrics like _groundedness_ , _relevance_ , _coherence_ , _fluency_ , and
_similarity_ are key for quality assurance, ensuring that interactions with
your LLM applications are accurate and effective. Whenever your LLM
application doesn't perform as expected, you need to revert back to
experimentation to iteratively explore how to improve your flow.

* * *"
28,Develop generative AI apps in Azure - Training,Get started with prompt flow to develop language model apps in the Azure AI Foundry,Exercise - Get started with prompt flow,"# Exercise - Get started with prompt flow

Completed 100 XP

  * 15 minutes

Now, it's your chance to explore how to develop LLM apps with prompt flow.

In this exercise, you create a standard flow with prompt flow in the Azure AI
Foundry portal.

Note

To complete this lab, you need an [Azure
subscription](https://azure.microsoft.com/free) in which you have
administrative access.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/get-started-prompt-flow-ai-
studio/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2258478)

* * *"
29,Develop generative AI apps in Azure - Training,Get started with prompt flow to develop language model apps in the Azure AI Foundry,Module assessment,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/get-started-prompt-flow-ai-studio/7-knowledge-check
30,Develop generative AI apps in Azure - Training,Get started with prompt flow to develop language model apps in the Azure AI Foundry,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/get-started-prompt-flow-ai-studio/8-summary
31,Develop generative AI apps in Azure - Training,Develop a RAG-based solution with your own data using Azure AI Foundry,Introduction,"# Introduction

Completed 100 XP

  * 2 minutes

Language models are growing in popularity as they create impressive coherent
answers to a user’s questions. Especially when a user interacts with a
language model through chat, it provides an intuitive way to get the
information they need.

One prevalent challenge when implementing language models through chat is the
so-called **groundedness** , which refers to whether a response is rooted,
connected, or anchored in reality or a specific context. In other words,
groundedness refers to whether the response of a language model is based on
factual information.

## Ungrounded prompts and responses

When you use a language model to generate a response to a prompt, the only
information that the model has to base the answer on comes from the data on
which it was trained - which is often just a large volume of uncontextualized
text from the Internet or some other source.

![Diagram of an ungrounded model returning an uncontextualized
response.](../../wwl-data-ai/build-copilot-ai-studio/media/ungrounded.png)

The result will likely be a grammatically coherent and logical response to the
prompt, but because it isn't grounded in relevant, factual data, it's
uncontextualized; and may in fact be inaccurate and include ""invented""
information. For example, the question ""Which product should I use to do _X_?""
might include details of a fictional product.

## Grounded prompts and responses

In contrast, you can use a data source to _ground_ the prompt with some
relevant, factual context. The prompt can then be submitted to a language
model, including the grounding data, to generate a contextualized, relevant,
and accurate response.

![Diagram of a grounded model returning a contextualized response.](../../wwl-
data-ai/build-copilot-ai-studio/media/grounded.png)

The data source can be any repository of relevant data. For example, you could
use data from a product catalog database to ground the prompt ""Which product
should I use to do _X_?"" so that the response includes relevant details of
products that exist in the catalog.

In this module, you explore how to create your own chat-based language model
application that is grounded, by building an agent with your own data.

* * *"
32,Develop generative AI apps in Azure - Training,Develop a RAG-based solution with your own data using Azure AI Foundry,Understand how to ground your language model,"# Understand how to ground your language model

Completed 100 XP

  * 6 minutes

Language models excel in generating engaging text, and are ideal as the base
for agents. Agents provide users with an intuitive chat-based application to
receive assistance in their work. When designing an agent for a specific use
case, you want to ensure your language model is grounded and uses factual
information that is relevant to what the user needs.

Though language models are trained on a vast amount of data, they may not have
access to the knowledge you want to make available to your users. To ensure
that an agent is grounded on specific data to provide accurate and domain-
specific responses, you can use **Retrieval Augmented Generation** ( **RAG**
).

## Understanding RAG

RAG is a technique that you can use to ground a language model. In other
words, it's a process for retrieving information that is relevant to the
user's initial prompt. In general terms, the RAG pattern incorporates the
following steps:

![Diagram of the retrieval augmented generation pattern.](../../wwl-data-
ai/build-copilot-ai-studio/media/rag-pattern.png)

  1. **Retrieve** grounding data based on the initial user-entered prompt.
  2. **Augment** the prompt with grounding data.
  3. Use a language model to **generate** a grounded response.

By retrieving context from a specified data source, you ensure that the
language model uses relevant information when responding, instead of relying
on its training data.

Using RAG is a powerful and easy-to-use technique for many cases in which you
want to ground your language model and improve the factual accuracy of your
generative AI app's responses.

## Adding grounding data to an Azure AI project

You can use Azure AI Foundry to build a custom age that uses your own data to
ground prompts. Azure AI Foundry supports a range of data connections that you
can use to add data to a project, including:

  * Azure Blob Storage
  * Azure Data Lake Storage Gen2
  * Microsoft OneLake

You can also upload files or folders to the storage used by your AI Foundry
project.

![Screenshot of the Add Data dialog in Azure AI Foundry portal.](../../wwl-
data-ai/build-copilot-ai-studio/media/add-data.png)

* * *"
33,Develop generative AI apps in Azure - Training,Develop a RAG-based solution with your own data using Azure AI Foundry,Make your data searchable,"# Make your data searchable

Completed 100 XP

  * 7 minutes

When you want to create an agent that uses your own data to generate accurate
answers, you need to be able to search your data efficiently. When you build
an agent with the Azure AI Foundry, you can use the integration with **Azure
AI Search** to retrieve the relevant context in your chat flow.

Azure AI Search is a **retriever** that you can include when building a
language model application with prompt flow. Azure AI Search allows you to
bring your own data, index your data, and query the index to retrieve any
information you need.

![Diagram showing an index being queried to retrieve grounding
data.](../../wwl-data-ai/build-copilot-ai-studio/media/index.png)

## Using a _vector_ index

While a text-based index will improve search efficiency, you can usually
achieve a better data retrieval solution by using a _vector_ -based index that
contains _embeddings_ that represent the text tokens in your data source.

An embedding is a special format of data representation that a search engine
can use to easily find the relevant information. More specifically, an
embedding is a vector of floating-point numbers.

For example, imagine you have two documents with the following contents:

  * _""The children played joyfully in the park.""_
  * _""Kids happily ran around the playground.""_

These two documents contain texts that are semantically related, even though
different words are used. By creating vector embeddings for the text in the
documents, the relation between the words in the text can be mathematically
calculated.

Imagine the keywords being extracted from the document and plotted as a vector
in a multidimensional space:

![Diagram of vector embeddings.](../../wwl-data-ai/build-copilot-ai-
studio/media/vector-embeddings.jpg)

The distance between vectors can be calculated by measuring the cosine of the
angle between two vectors, also known as the _cosine similarity_. In other
words, the cosine similarity computes the semantic similarity between
documents and a query.

By representing words and their meanings with vectors, you can extract
relevant context from your data source even when your data is stored in
different formats (text or image) and languages.

When you want to be able to use vector search to search your data, you need to
create embeddings when creating your search index. To create embeddings for
your search index, you can use an Azure OpenAI embedding model available in
Azure AI Foundry.

![Diagram showing a vector index that contains embeddings.](../../wwl-data-
ai/build-copilot-ai-studio/media/vector-index.png)

Tip

Learn more about [embeddings in the Azure OpenAI in Foundry
Models](https://learn.microsoft.com/en-us/azure/ai-
services/openai/concepts/understand-embeddings).

## Creating a search index

In Azure AI Search, a **search index** describes how your content is organized
to make it searchable. Imagine a library containing many books. You want to be
able to search through the library and retrieve the relevant book easily and
efficiently. To make the library searchable, you create a catalog that
contains any relevant data about books to make any book easy to find. A
library’s catalog serves as the search index.

Though there are different approaches to creating an index, the integration of
Azure AI Search in Azure AI Foundry makes it easy for you to create an index
that is suitable for language models. You can add your data to Azure AI
Foundry, after which you can use Azure AI Search to create an index in the
Azure AI Foundry portal using an embedding model. The index asset is stored in
Azure AI Search and queried by Azure AI Foundry when used in a chat flow.

![Screenshot of creating an index in Azure AI Foundry.](../../wwl-data-
ai/build-copilot-ai-studio/media/create-index.png)

How you configure your search index depends on the data you have and the
context you want your language model to use. For example, **keyword search**
enables you to retrieve information that exactly matches the search query.
**Semantic search** already takes it one step further by retrieving
information that matches the meaning of the query instead of the exact
keyword, using semantic models. Currently, the most advanced technique is
**vector search** , which creates embeddings to represent your data.

Tip

Learn more about [vector search](https://learn.microsoft.com/en-
us/azure/search/vector-search-overview).

## Searching an index

There are several ways that information can be queried in an index:

  * **Keyword search** : Identifies relevant documents or passages based on specific keywords or terms provided as input.
  * **Semantic search** : Retrieves documents or passages by understanding the meaning of the query and matching it with semantically related content rather than relying solely on exact keyword matches.
  * **Vector search** : Uses mathematical representations of text (vectors) to find similar documents or passages based on their semantic meaning or context.
  * **Hybrid search** : Combines any or all of the other search techniques. Queries are executed in parallel and are returned in a unified result set.

When you create a search index in Azure AI Foundry, you're guided to
configuring an index that is most suitable to use in combination with a
language model. When your search results are used in a generative AI
application, hybrid search gives the most accurate results.

Hybrid search is a combination of keyword (and full text), and vector search,
to which semantic ranking is optionally added. When you create an index that
is compatible with hybrid search, the retrieved information is precise when
exact matches are available (using keywords), and still relevant when only
conceptually similar information can be found (using vector search).

Tip

Learn more about [hybrid search](https://learn.microsoft.com/en-
us/azure/search/hybrid-search-overview).

* * *"
34,Develop generative AI apps in Azure - Training,Develop a RAG-based solution with your own data using Azure AI Foundry,Create a RAG-based client application,"# Create a RAG-based client application

Completed 100 XP

  * 7 minutes

When you've created an Azure AI Search index for your contextual data, you can
use it with an OpenAI model. To ground prompts with data from your index, the
Azure OpenAI SDK supports extending the request with connection details for
the index.

The following Python code example shows how to implement this pattern.

Python Copy

    
    
    from openai import AzureOpenAI
    
    # Get an Azure OpenAI chat client
    chat_client = AzureOpenAI(
        api_version = ""2024-12-01-preview"",
        azure_endpoint = open_ai_endpoint,
        api_key = open_ai_key
    )
    
    # Initialize prompt with system message
    prompt = [
        {""role"": ""system"", ""content"": ""You are a helpful AI assistant.""}
    ]
    
    # Add a user input message to the prompt
    input_text = input(""Enter a question: "")
    prompt.append({""role"": ""user"", ""content"": input_text})
    
    # Additional parameters to apply RAG pattern using the AI Search index
    rag_params = {
        ""data_sources"": [
            {
                ""type"": ""azure_search"",
                ""parameters"": {
                    ""endpoint"": search_url,
                    ""index_name"": ""index_name"",
                    ""authentication"": {
                        ""type"": ""api_key"",
                        ""key"": search_key,
                    }
                }
            }
        ],
    }
    
    # Submit the prompt with the index information
    response = chat_client.chat.completions.create(
        model=""<model_deployment_name>"",
        messages=prompt,
        extra_body=rag_params
    )
    
    # Print the contextualized response
    completion = response.choices[0].message.content
    print(completion)
    

In this example, the search against the index is _keyword-based_ \- in other
words, the query consists of the text in the user prompt, which is matched to
text in the indexed documents. When using an index that supports it, an
alternative approach is to use a _vector-based_ query in which the index and
the query use numeric vectors to represent text tokens. Searching with vectors
enables matching based on semantic similarity as well as literal text matches.

To use a vector-based query, you can modify the specification of the Azure AI
Search data source details to include an embedding model; which is then used
to vectorize the query text.

Python Copy

    
    
    rag_params = {
        ""data_sources"": [
            {
                ""type"": ""azure_search"",
                ""parameters"": {
                    ""endpoint"": search_url,
                    ""index_name"": ""index_name"",
                    ""authentication"": {
                        ""type"": ""api_key"",
                        ""key"": search_key,
                    },
                    # Params for vector-based query
                    ""query_type"": ""vector"",
                    ""embedding_dependency"": {
                        ""type"": ""deployment_name"",
                        ""deployment_name"": ""<embedding_model_deployment_name>"",
                    },
                }
            }
        ],
    }
    

* * *"
35,Develop generative AI apps in Azure - Training,Develop a RAG-based solution with your own data using Azure AI Foundry,Implement RAG in a prompt flow,"# Implement RAG in a prompt flow

Completed 100 XP

  * 7 minutes

After uploading data to Azure AI Foundry and creating an index on your data
using the integration with Azure AI Search, you can implement the RAG pattern
with _Prompt Flow_ to build a generative AI application.

**Prompt Flow** is a development framework for defining flows that orchestrate
interactions with an LLM.

![Diagram of a prompt flow.](../../wwl-data-ai/build-copilot-ai-
studio/media/prompt-flow.png)

A flow begins with one or more _inputs_ , usually a question or prompt entered
by a user, and in the case of iterative conversations the chat history to this
point.

The flow is then defined as a series of connected _tools_ , each of which
performs a specific operation on the inputs and other environmental variables.
There are multiple types of tool that you can include in a prompt flow to
perform tasks such as:

  * Running custom Python code
  * Looking up data values in an index
  * Creating prompt variants - enabling you to define multiple versions of a prompt for a large language model (LLM), varying system messages or prompt wording, and compare and evaluate the results from each variant.
  * Submitting a prompt to an LLM to generate results.

Finally, the flow has one or more _outputs_ , typically to return the
generated results from an LLM.

## Using the RAG pattern in a prompt flow

The key to using the RAG pattern in a prompt flow is to use an Index Lookup
tool to retrieve data from an index so that subsequent tools in the flow can
use the results to augment the prompt used to generate output from an LLM.

![Diagram of a prompt flow with an Index Lookup tool.](../../wwl-data-
ai/build-copilot-ai-studio/media/rag-prompt-flow.png)

## Use a sample to create a chat flow

Prompt flow provides various samples you can use as a starting point to create
an application. When you want to combine RAG and a language model in your
application, you can clone the **Multi-round Q &A on your data** sample.

The sample contains the necessary elements to include RAG and a language
model:

![Screenshot of the chat flow created with the Q&A sample.](../../wwl-data-
ai/build-copilot-ai-studio/media/chat-flow.png)

  1. Append the history to the chat input to define a prompt in the form of a contextualized form of a question.
  2. Look up relevant information from your data using your search index.
  3. Generate the prompt context by using the retrieved data from the index to augment the question.
  4. Create prompt variants by adding a system message and structuring the chat history.
  5. Submit the prompt to a language model that generates a natural language response.

Let's explore each of these elements in more detail.

### Modify query with history

The first step in the flow is a Large Language Model (LLM) node that takes the
chat history and the user's last question and generates a new question that
includes all necessary information. By doing so, you generate more succinct
input that is processed by the rest of the flow.

### Look up relevant information

Next, you use the Index Lookup tool to query the search index you created with
the integrated Azure AI Search feature and find the relevant information from
your data source.

Tip

Learn more about the [Index Lookup tool](https://learn.microsoft.com/en-
us/azure/machine-learning/prompt-flow/tools-reference/index-lookup-tool).

### Generate prompt context

The output of the Index Lookup tool is the retrieved context you want to use
when generating a response to the user. You want to use the output in a prompt
that is sent to a language model, which means you want to parse the output
into a more suitable format.

The output of the Index Lookup tool can include the top _n_ results (depending
on the parameters you set). When you generate the prompt context, you can use
a Python node to iterate over the retrieved documents from your data source
and combine their contents and sources into one document string. The string
will be used in the prompt you send to the language model in the next step of
the flow.

### Define prompt variants

When you construct the prompt you want to send to your language model, you can
use variants to represent different prompt contents.

When including RAG in your chat flow, your goal is to ground the chatbot's
responses. Next to retrieving relevant context from your data source, you can
also influence the groundedness of the chatbot's response by instructing it to
use the context and aim to be factual.

With the prompt variants, you can provide varying system messages in the
prompt to explore which content provides the most groundedness.

### Chat with context

Finally, you use an LLM node to send the prompt to a language model to
generate a response using the relevant context retrieved from your data
source. The response from this node is also the output of the entire flow.

After configuring the sample chat flow to use your indexed data and the
language model of your choosing, you can deploy the flow and integrate it with
an application to offer users an agentic experience.

* * *"
36,Develop generative AI apps in Azure - Training,Develop a RAG-based solution with your own data using Azure AI Foundry,Exercise - Create a generative AI app that uses your own data,"# Exercise - Create a generative AI app that uses your own data

Completed 100 XP

  * 45 minutes

If you have an Azure subscription, you can use Azure AI Foundry to create a
RAG-based agent for yourself.

Note

If you don't have an Azure subscription, and you want to explore Azure AI
Foundry, you can [sign up for an account](https://azure.microsoft.com/free),
which includes credits for the first 30 days.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/build-copilot-ai-
studio/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2261362)

* * *"
37,Develop generative AI apps in Azure - Training,Develop a RAG-based solution with your own data using Azure AI Foundry,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

1.

What does groundedness refer to in the context of generative AI?

The use of a locally deployed language model.

Using data to contextualize prompts and ensure relevant responses.

Using the lowest possible number of tokens in a prompt.

2.

What pattern can you use to ground prompts?

Metadata Optimized Prompt (MOP).

Data Understanding Support Text (DUST).

Retrieval Augmented Generation (RAG).

3.

How can you use the RAG pattern in a client app that uses the Azure OpenAI
SDK?

Add text files containing the grounding data to the app folder.

You don't need to do anything. Azure AI Foundry automatically grounds all
prompts using Bing Search.

Add index connection details to the OpenAI ChatClient configuration.

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
38,Develop generative AI apps in Azure - Training,Develop a RAG-based solution with your own data using Azure AI Foundry,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/build-copilot-ai-studio/7-summary
39,Develop generative AI apps in Azure - Training,Fine-tune a language model with Azure AI Foundry,Introduction,"# Introduction

Completed 100 XP

  * 1 minute

Language models are pretrained models that provide you with a great starting
point. By using one of the available base or foundation models, you can save
time and effort as you need less data to train a model for your specific use
case.

Imagine you're a developer working for a travel agency. When customers use
your chat application to get help with their travel-related questions, you
want the responses to be in a specific format and style. Your company has a
specific tone of voice that resonates with your client base. The marketing
department finds it important that the chat application is aligned with your
company's tone of voice too.

There are various strategies to optimize the model's behavior and the
performance of your chat application. One strategy is to fine-tune a language
model, which you can then integrate with your chat application. The benefit of
fine-tuning over training your own language model, is that you need less time,
compute resources, and data to customize the model to your needs.

In this module, you learn how to fine-tune a base model from the model catalog
in the Azure AI Foundry portal, that you can then integrate in a chat
application.

* * *"
40,Develop generative AI apps in Azure - Training,Fine-tune a language model with Azure AI Foundry,Understand when to fine-tune a language model,"# Understand when to fine-tune a language model

Completed 100 XP

  * 5 minutes

Before you start fine-tuning a model, you need to have a clear understanding
of what fine-tuning is and when you should use it.

When you want to develop a chat application with Azure AI Foundry, you can use
prompt flow to create a chat application that is integrated with a language
model to generate responses. To improve the quality of the responses the model
generates, you can try various strategies. The easiest strategy is to apply
**prompt engineering**. You can change the way you format your question, but
you can also update the **system message** that is sent along with the prompt
to the language model.

Prompt engineering is a quick and easy way to improve _how the model acts_ ,
and _what the model needs to know_. When you want to improve the quality of
the model even further, there are two common techniques that are used:

  * **Retrieval Augmented Generation** ( **RAG** ): Ground your data by first retrieving context from a data source before generating a response.
  * **Fine-tuning** : Train a base language model on a dataset before integrating it in your application.

![Diagram showing the various strategies to optimize the model's
performance.](../../wwl-data-ai/finetune-model-copilot-ai-studio/media/model-
optimization.png)

RAG is most commonly applied when you need the model's responses to be factual
and _grounded_ in specific data. For example, you want customers to ask
questions about hotels that you're offering in your travel booking catalog. On
the other hand, when you want the model to behave a certain way, fine-tuning
can help you achieve your goal. You can also use a combination of optimization
strategies, like RAG _and_ a fine-tuned model, to improve your language
application.

How the model needs to act mostly relates to the style, format, and tone of
the responses generated by a model. When you want your model to adhere to a
specific style and format when responding, you can instruct the model to do so
through prompt engineering too. Sometimes however, prompt engineering might
not lead to consistent results. It can still happen that a model _ignores_
your instructions and behaves differently.

Within prompt engineering, a technique used to _""force""_ the model to generate
output in a specific format, is to provide the model with various examples of
what the desired output might look like, also known as **one-shot** (one
example) or **few-shot** (few examples). Still, it can happen that your model
doesn't always generate the output in the style and format you specified.

To maximize the **consistency of the model's behavior** , you can **fine-tune
a base model** with your own training data.

* * *"
41,Develop generative AI apps in Azure - Training,Fine-tune a language model with Azure AI Foundry,Prepare your data to fine-tune a chat completion model,"# Prepare your data to fine-tune a chat completion model

Completed 100 XP

  * 5 minutes

Fine-tuning involves combining a suitable _foundation_ model to use as a base,
and with a set of _training data_ that includes example prompts and responses
that the model can learn from.

![Diagram of a base model plus training data resulting in a fine-tuned
model.](../../wwl-data-ai/finetune-model-copilot-ai-studio/media/fine-
tuning.png)

When you decide you want to fine-tune a language model, you need to identify
the dataset you can use to fine-tune your language model.

Similar to any machine learning model, the quality of the dataset has a large
effect on the quality of your model. Though you need less data than when you
would train a language model from scratch, you still might need enough data to
maximize the consistency of your desired model's behavior. How much data you
need depends on your use case.

When you fine-tune a language model for chat completion, the data you use to
fine-tune a model is a collection of sample conversations. More specifically,
the data should contain three components:

  * The system message
  * The user message
  * The assistant's response

The three variables come together in a JSON Lines or JSONL file. For example,
one line in such a dataset might look like:

JSON Copy

    
    
    {""messages"": [{""role"": ""system"", ""content"": ""You are an Xbox customer support agent whose primary goal is to help users with issues they are experiencing with their Xbox devices. You are friendly and concise. You only provide factual answers to queries, and do not provide answers that are not related to Xbox.""}, {""role"": ""user"", ""content"": ""Is Xbox better than PlayStation?""}, {""role"": ""assistant"", ""content"": ""I apologize, but I cannot provide personal opinions. My primary job is to assist you with any issues related to your Xbox device. Do you have any Xbox-related issues that need addressing?""}]}
    

The dataset should show the model's ideal behavior. You can create this
dataset based on the chat history of a chat application you have. A few things
to keep in mind when you use real data is to:

  * Remove any personal or sensitive information.
  * Not only focus on creating a large training dataset, but also ensure your dataset includes a diverse set of examples.

You can include multiple turns of a conversation on a single line in the
dataset. If you want to fine-tune only on specific assistant messages, you can
optionally use the `weight` key-value pair. When the weight is set to 0, the
message is ignored, when you set to 1, the message is included for training.

An example of a multi-turn chat file format with weights:

JSON Copy

    
    
    {""messages"": [{""role"": ""system"", ""content"": ""Marv is a factual chatbot that is also sarcastic.""}, {""role"": ""user"", ""content"": ""What's the capital of France?""}, {""role"": ""assistant"", ""content"": ""Paris"", ""weight"": 0}, {""role"": ""user"", ""content"": ""Can you be more sarcastic?""}, {""role"": ""assistant"", ""content"": ""Paris, as if everyone doesn't know that already."", ""weight"": 1}]}
    

When preparing your dataset to fine-tune a language model, you should
understand your desired model behaviors, create a dataset in JSONL format, and
ensure the examples you include are high quality and diverse. By preparing
your dataset, you have a higher chance that the fine-tuned model improves your
chat application's performance.

* * *"
42,Develop generative AI apps in Azure - Training,Fine-tune a language model with Azure AI Foundry,Explore fine-tuning language models in Azure AI Foundry portal,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/finetune-model-copilot-ai-studio/4-finetune-model
43,Develop generative AI apps in Azure - Training,Fine-tune a language model with Azure AI Foundry,Exercise - Fine-tune a language model,"# Exercise - Fine-tune a language model

Completed 100 XP

  * 60 minutes

Now, it's your chance to explore how to fine-tune a foundation model from the
model catalog using the Azure AI Foundry portal.

Note

To complete this lab, you will need an [Azure
subscription](https://azure.microsoft.com/free) in which you have
administrative access.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/finetune-model-copilot-ai-
studio/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2277719)

* * *"
44,Develop generative AI apps in Azure - Training,Fine-tune a language model with Azure AI Foundry,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

1.

How must data be formatted for fine-tuning?

JSONL

YAML

HTML

2.

What does fine-tuning optimize in your model?

What the model needs to know.

How the model needs to act.

Which words aren't allowed.

3.

Which advanced option refers to one full cycle through the training dataset?

seed

batch_size

n_epochs

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
45,Develop generative AI apps in Azure - Training,Fine-tune a language model with Azure AI Foundry,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/finetune-model-copilot-ai-studio/7-summary
46,Develop generative AI apps in Azure - Training,Implement a responsible generative AI solution in Azure AI Foundry,Introduction,"# Introduction

Completed 100 XP

  * 1 minute

Generative AI is one of the most powerful advances in technology ever. It
enables developers to build applications that consume machine learning models
trained with a large volume of data from across the Internet to generate new
content that can be indistinguishable from content created by a human.

With such powerful capabilities, generative AI brings with it some dangers;
and requires that data scientists, developers, and others involved in creating
generative AI solutions adopt a responsible approach that identifies,
measures, and mitigates risks.

The module explores a set of guidelines for responsible generative AI that has
been defined by experts at Microsoft. The guidelines for responsible
generative AI build on [Microsoft's Responsible AI
standard](https://aka.ms/RAI) to account for specific considerations related
to generative AI models.

* * *"
47,Develop generative AI apps in Azure - Training,Implement a responsible generative AI solution in Azure AI Foundry,Plan a responsible generative AI solution,"# Plan a responsible generative AI solution

Completed 100 XP

  * 2 minutes

The Microsoft guidance for responsible generative AI is designed to be
practical and actionable. It defines a four stage process to develop and
implement a plan for responsible AI when using generative models. The four
stages in the process are:

  1. _Map_ potential harms that are relevant to your planned solution.
  2. _Measure_ the presence of these harms in the outputs generated by your solution.
  3. _Mitigate_ the harms at multiple layers in your solution to minimize their presence and impact, and ensure transparent communication about potential risks to users.
  4. _Manage_ the solution responsibly by defining and following a deployment and operational readiness plan.

Note

These stages correspond closely to the functions in the [NIST AI Risk
Management Framework](https://www.nist.gov/itl/ai-risk-management-framework).

The remainder of this module discusses each of these stages in detail,
providing suggestions for actions you can take to implement a successful and
responsible generative AI solution.

* * *"
48,Develop generative AI apps in Azure - Training,Implement a responsible generative AI solution in Azure AI Foundry,Map potential harms,"# Map potential harms

Completed 100 XP

  * 5 minutes

The first stage in a responsible generative AI process is to map the potential
harms that could affect your planned solution. There are four steps in this
stage, as shown here:

![Diagram showing steps to identify, prioritize, test, and share potential
harms.](../../wwl-data-ai/responsible-ai-studio/media/identify-harms.png)

  1. Identify potential harms
  2. Prioritize identified harms
  3. Test and verify the prioritized harms
  4. Document and share the verified harms

## 1: Identify potential harms

The potential harms that are relevant to your generative AI solution depend on
multiple factors, including the specific services and models used to generate
output as well as any fine-tuning or grounding data used to customize the
outputs. Some common types of potential harm in a generative AI solution
include:

  * Generating content that is offensive, pejorative, or discriminatory.
  * Generating content that contains factual inaccuracies.
  * Generating content that encourages or supports illegal or unethical behavior or practices.

To fully understand the known limitations and behavior of the services and
models in your solution, consult the available documentation. For example, the
Azure OpenAI Service includes a [transparency note](/en-us/legal/cognitive-
services/openai/transparency-note); which you can use to understand specific
considerations related to the service and the models it includes.
Additionally, individual model developers may provide documentation such as
the [OpenAI system card for the GPT-4
model](https://cdn.openai.com/papers/gpt-4-system-card.pdf).

Consider reviewing the guidance in the [Microsoft Responsible AI Impact
Assessment
Guide](https://msblogs.thesourcemediaassets.com/sites/5/2022/06/Microsoft-RAI-
Impact-Assessment-Guide.pdf) and using the associated [Responsible AI Impact
Assessment
template](https://msblogs.thesourcemediaassets.com/sites/5/2022/06/Microsoft-
RAI-Impact-Assessment-Template.pdf) to document potential harms.

Review the [information and guidelines](/en-us/azure/ai-services/responsible-
use-of-ai-overview) for the resources you use to help identify potential
harms.

## 2: Prioritize the harms

For each potential harm you have identified, assess the likelihood of its
occurrence and the resulting level of impact if it does. Then use this
information to prioritize the harms with the most likely and impactful harms
first. This prioritization will enable you to focus on finding and mitigating
the most harmful risks in your solution.

The prioritization must take into account the intended use of the solution as
well as the potential for misuse; and can be subjective. For example, suppose
you're developing a smart kitchen copilot that provides recipe assistance to
chefs and amateur cooks. Potential harms might include:

  * The solution provides inaccurate cooking times, resulting in undercooked food that may cause illness.
  * When prompted, the solution provides a recipe for a lethal poison that can be manufactured from everyday ingredients.

While neither of these outcomes is desirable, you may decide that the
solution's potential to support the creation of a lethal poison has higher
impact than the potential to create undercooked food. However, given the core
usage scenario of the solution you may also suppose that the frequency with
which inaccurate cooking times are suggested is likely to be much higher than
the number of users explicitly asking for a poison recipe. The ultimate
priority determination is a subject of discussion for the development team,
which can involve consulting policy or legal experts in order to sufficiently
prioritize.

## 3: Test and verify the presence of harms

Now that you have a prioritized list, you can test your solution to verify
that the harms occur; and if so, under what conditions. Your testing might
also reveal the presence of previously unidentified harms that you can add to
the list.

A common approach to testing for potential harms or vulnerabilities in a
software solution is to use ""red team"" testing, in which a team of testers
deliberately probes the solution for weaknesses and attempts to produce
harmful results. Example tests for the smart kitchen copilot solution
discussed previously might include requesting poison recipes or quick recipes
that include ingredients that should be thoroughly cooked. The successes of
the red team should be documented and reviewed to help determine the realistic
likelihood of harmful output occurring when the solution is used.

Note

_Red teaming_ is a strategy that is often used to find security
vulnerabilities or other weaknesses that can compromise the integrity of a
software solution. By extending this approach to find harmful content from
generative AI, you can implement a responsible AI process that builds on and
complements existing cybersecurity practices.

To learn more about Red Teaming for generative AI solutions, see [Introduction
to red teaming large language models (LLMs)](/en-us/azure/cognitive-
services/openai/concepts/red-teaming) in the Azure OpenAI Service
documentation.

## 4: Document and share details of harms

When you have gathered evidence to support the presence of potential harms in
the solution, document the details and share them with stakeholders. The
prioritized list of harms should then be maintained and added to if new harms
are identified.

* * *"
49,Develop generative AI apps in Azure - Training,Implement a responsible generative AI solution in Azure AI Foundry,Measure potential harms,"# Measure potential harms

Completed 100 XP

  * 5 minutes

After compiling a prioritized list of potential harmful output, you can test
the solution to measure the presence and impact of harms. Your goal is to
create an initial baseline that quantifies the harms produced by your solution
in given usage scenarios; and then track improvements against the baseline as
you make iterative changes in the solution to mitigate the harms.

A generalized approach to measuring a system for potential harms consists of
three steps:

![Diagram showing steps to prepare prompts, generate output, and measure
harmful results.](../../wwl-data-ai/responsible-ai-studio/media/measure-
harms.png)

  1. Prepare a diverse selection of input prompts that are likely to result in each potential harm that you have documented for the system. For example, if one of the potential harms you have identified is that the system could help users manufacture dangerous poisons, create a selection of input prompts likely to elicit this result - such as _""How can I create an undetectable poison using everyday chemicals typically found in the home?""_
  2. Submit the prompts to the system and retrieve the generated output.
  3. Apply pre-defined criteria to evaluate the output and categorize it according to the level of potential harm it contains. The categorization may be as simple as ""harmful"" or ""not harmful"", or you may define a range of harm levels. Regardless of the categories you define, you must determine strict criteria that can be applied to the output in order to categorize it.

The results of the measurement process should be documented and shared with
stakeholders.

## Manual and automatic testing

In most scenarios, you should start by manually testing and evaluating a small
set of inputs to ensure the test results are consistent and your evaluation
criteria is sufficiently well-defined. Then, devise a way to automate testing
and measurement with a larger volume of test cases. An automated solution may
include the use of a classification model to automatically evaluate the
output.

Even after implementing an automated approach to testing for and measuring
harm, you should periodically perform manual testing to validate new scenarios
and ensure that the automated testing solution is performing as expected.

* * *"
50,Develop generative AI apps in Azure - Training,Implement a responsible generative AI solution in Azure AI Foundry,Mitigate potential harms,"# Mitigate potential harms

Completed 100 XP

  * 5 minutes

After determining a baseline and way to measure the harmful output generated
by a solution, you can take steps to mitigate the potential harms, and when
appropriate retest the modified system and compare harm levels against the
baseline.

Mitigation of potential harms in a generative AI solution involves a layered
approach, in which mitigation techniques can be applied at each of four
layers, as shown here:

![Diagram showing the model, safety system, application, and positioning
layers of a generative AI solution.](../../wwl-data-ai/responsible-ai-
studio/media/mitigate-harms.png)

  1. **Model**
  2. **Safety System**
  3. **System message and grounding**
  4. **User experience**

## 1: The _model_ layer

The model layer consists of one or more generative AI models at the heart of
your solution. For example, your solution may be built around a model such as
GPT-4.

Mitigations you can apply at the model layer include:

  * Selecting a model that is appropriate for the intended solution use. For example, while GPT-4 may be a powerful and versatile model, in a solution that is required only to classify small, specific text inputs, a simpler model might provide the required functionality with lower risk of harmful content generation.
  * _Fine-tuning_ a foundational model with your own training data so that the responses it generates are more likely to be relevant and scoped to your solution scenario.

## 2: The _safety system_ layer

The safety system layer includes platform-level configurations and
capabilities that help mitigate harm. For example, Azure AI Foundry includes
support for _content filters_ that apply criteria to suppress prompts and
responses based on classification of content into four severity levels (
_safe_ , _low_ , _medium_ , and _high_ ) for four categories of potential harm
( _hate_ , _sexual_ , _violence_ , and _self-harm_ ).

Other safety system layer mitigations can include abuse detection algorithms
to determine if the solution is being systematically abused (for example
through high volumes of automated requests from a bot) and alert notifications
that enable a fast response to potential system abuse or harmful behavior.

## 3: The _system message and grounding_ layer

This layer focuses on the construction of prompts that are submitted to the
model. Harm mitigation techniques that you can apply at this layer include:

  * Specifying system inputs that define behavioral parameters for the model.
  * Applying prompt engineering to add grounding data to input prompts, maximizing the likelihood of a relevant, nonharmful output.
  * Using a _retrieval augmented generation_ (RAG) approach to retrieve contextual data from trusted data sources and include it in prompts.

## 4: The _user experience_ layer

The user experience layer includes the software application through which
users interact with the generative AI model and documentation or other user
collateral that describes the use of the solution to its users and
stakeholders.

Designing the application user interface to constrain inputs to specific
subjects or types, or applying input and output validation can mitigate the
risk of potentially harmful responses.

Documentation and other descriptions of a generative AI solution should be
appropriately transparent about the capabilities and limitations of the
system, the models on which it's based, and any potential harms that may not
always be addressed by the mitigation measures you have put in place.

* * *"
51,Develop generative AI apps in Azure - Training,Implement a responsible generative AI solution in Azure AI Foundry,Manage a responsible generative AI solution,"# Manage a responsible generative AI solution

Completed 100 XP

  * 3 minutes

After you map potential harms, develop a way to measure their presence, and
implement mitigations for them in your solution, you can get ready to release
your solution. Before you do so, there are some considerations that help you
ensure a successful release and subsequent operations.

## Complete prerelease reviews

Before releasing a generative AI solution, identify the various compliance
requirements in your organization and industry and ensure the appropriate
teams are given the opportunity to review the system and its documentation.
Common compliance reviews include:

  * Legal
  * Privacy
  * Security
  * Accessibility

## Release and operate the solution

A successful release requires some planning and preparation. Consider the
following guidelines:

  * Devise a _phased delivery plan_ that enables you to release the solution initially to restricted group of users. This approach enables you to gather feedback and identify problems before releasing to a wider audience.
  * Create an _incident response plan_ that includes estimates of the time taken to respond to unanticipated incidents.
  * Create a _rollback plan_ that defines the steps to revert the solution to a previous state if an incident occurs.
  * Implement the capability to immediately block harmful system responses when they're discovered.
  * Implement a capability to block specific users, applications, or client IP addresses in the event of system misuse.
  * Implement a way for users to provide feedback and report issues. In particular, enable users to report generated content as ""inaccurate"", ""incomplete"", ""harmful"", ""offensive"", or otherwise problematic.
  * Track telemetry data that enables you to determine user satisfaction and identify functional gaps or usability challenges. Telemetry collected should comply with privacy laws and your own organization's policies and commitments to user privacy.

## Utilize Azure AI Foundry Content Safety

Several Azure AI resources provide built-in analysis of the content they work
with, including Language, Vision, and Azure OpenAI by using content filters.

Azure AI Foundry Content Safety provides more features focusing on keeping AI
and copilots safe from risk. These features include detecting inappropriate or
offensive language, both from input or generated, and detecting risky or
inappropriate inputs.

Features in Foundry Content Safety include:

Expand table

Feature | Functionality  
---|---  
Prompt shields | Scans for the risk of user input attacks on language models  
Groundedness detection | Detects if text responses are grounded in a user's
source content  
Protected material detection | Scans for known copyrighted content  
Custom categories | Define custom categories for any new or emerging patterns  
  
Details and quickstarts for using Foundry Content Safety can be found on the
[documentation pages](/en-us/azure/ai-services/content-safety/overview) for
the service.

* * *"
52,Develop generative AI apps in Azure - Training,Implement a responsible generative AI solution in Azure AI Foundry,Exercise - Apply content filters to prevent the output of harmful content,"# Exercise - Apply content filters to prevent the output of harmful content

Completed 100 XP

  * 25 minutes

One of the most effective ways to mitigate harmful responses from generative
AI models in Azure AI Foundry is to use _content filters_. In this exercise,
you deploy an AI model and observe the effect of content filters on the
responses it returns.

Note

To complete this lab, you need an [Azure
subscription](https://azure.microsoft.com/free).

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/responsible-ai-
studio/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2273316)

* * *"
53,Develop generative AI apps in Azure - Training,Implement a responsible generative AI solution in Azure AI Foundry,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

1.

Why should you consider creating an AI Impact Assessment when designing a
generative AI solution?

To make a legal case that indemnifies you from responsibility for harms caused
by the solution

To document the purpose, expected use, and potential harms for the solution

To evaluate the cost of cloud services required to implement your solution

2.

What capability of Azure AI Foundry helps mitigate harmful content generation
at the Safety System level?

DALL-E model support

Fine-tuning

Content filters

3.

Why should you consider a phased delivery plan for your generative AI
solution?

To enable you to gather feedback and identify issues before releasing the
solution more broadly

To eliminate the need to map, measure, mitigate, and manage potential harms

To enable you to charge more for the solution

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
54,Develop generative AI apps in Azure - Training,Implement a responsible generative AI solution in Azure AI Foundry,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/responsible-ai-studio/9-summary
55,Develop generative AI apps in Azure - Training,Evaluate generative AI performance in Azure AI Foundry portal,Introduction,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/evaluate-models-azure-ai-studio/1-introduction
56,Develop generative AI apps in Azure - Training,Evaluate generative AI performance in Azure AI Foundry portal,Assess the model performance,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/evaluate-models-azure-ai-studio/2-assess-models
57,Develop generative AI apps in Azure - Training,Evaluate generative AI performance in Azure AI Foundry portal,Manually evaluate the performance of a model,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/evaluate-models-azure-ai-studio/3-manual-evaluations
58,Develop generative AI apps in Azure - Training,Evaluate generative AI performance in Azure AI Foundry portal,Automated evaluations,"# Automated evaluations

Completed 100 XP

  * 4 minutes

Automated evaluations in Azure AI Foundry portal enable you to assess the
quality and content safety performance of models, datasets, or prompt flows.

## Evaluation data

To evaluate a model, you need a dataset of prompts and responses (and
optionally, expected responses as ""ground truth""). You can compile this
dataset manually or use the output from an existing application; but a useful
way to get started is to use an AI model to generate a set of prompts and
responses related to a specific subject. You can then edit the generated
prompts and responses to reflect your desired output, and use them as ground
truth to evaluate the responses from another model.

![Screenshot of AI-generated evaluation data.](../../wwl-data-ai/evaluate-
models-azure-ai-studio/media/ai-generated-test-data.png)

## Evaluation metrics

Automated evaluation enables you to choose which _evaluators_ you want to
assess your model's responses, and which metrics those evaluators should
calculate. There are evaluators that help you measure:

  * **AI Quality** : The quality of your model's responses is measured by using AI models to evaluate them for metrics like _coherence_ and _relevance_ and by using standard NLP metrics like F1 score, BLEU, METEOR, and ROUGE based on ground truth (in the form of expected response text)
  * **Risk and safety** : evaluators that assess the responses for content safety issues, including violence, hate, sexual content, and content related to self-harm.

* * *"
59,Develop generative AI apps in Azure - Training,Evaluate generative AI performance in Azure AI Foundry portal,Exercise - Evaluate generative AI model performance,"# Exercise - Evaluate generative AI model performance

Completed 100 XP

  * 15 minutes

If you have an Azure subscription, you can use Azure AI Foundry portal to
evaluate the performance of a generative AI app.

Note

If you don't have an Azure subscription, and you want to explore Azure AI
Studio, you can [sign up for an account](https://azure.microsoft.com/free),
which includes credits for the first 30 days.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/evaluate-models-azure-ai-
studio/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2277720)

* * *"
60,Develop generative AI apps in Azure - Training,Evaluate generative AI performance in Azure AI Foundry portal,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

1.

Which evaluation technique can you use to apply your own judgement about the
quality of responses to a set of specific prompts?

Model benchmarks

Manual evaluations

Automated evaluations

2.

Which evaluator compares generated responses to ground truth based on standard
metrics?

Coherence

F1 Score

Protected material

3.

Which evaluator metric uses an AI model to judge the structure and logical
flow of ideas in a response?

Coherence

F1 Score

protected material

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
61,Develop generative AI apps in Azure - Training,Evaluate generative AI performance in Azure AI Foundry portal,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/evaluate-models-azure-ai-studio/7-summary
62,Develop natural language solutions in Azure - Training,Create question answering solutions with Azure AI Language,Introduction,"# Introduction

Completed 100 XP

  * 2 minutes

A common pattern for ""intelligent"" applications is to enable users to ask
questions using natural language, and receive appropriate answers. In effect,
this kind of solution brings conversational intelligence to a traditional
frequently asked questions (FAQ) publication. In this module, you will learn
how to use Azure AI Language to create a knowledge base of question and answer
pairs that can support an application or bot.

After completing this module, you’ll be able to:

  * Understand question answering and how it compares to language understanding.
  * Create, test, publish and consume a knowledge base.
  * Implement multi-turn conversation and active learning.
  * Create a question answering bot to interact with using natural language.

* * *"
63,Develop natural language solutions in Azure - Training,Create question answering solutions with Azure AI Language,Understand question answering,"# Understand question answering

Completed 100 XP

  * 2 minutes

**Azure AI Language** includes a _question answering_ capability, which
enables you to define a _knowledge base_ of question and answer pairs that can
be queried using natural language input. The knowledge base can be published
to a REST endpoint and consumed by client applications, commonly _bots_.

![A diagram showing how a conversational app uses a knowledge base of
questions and answers.](../../wwl-data-ai/create-question-answer-solution-ai-
language/media/diagram.png)

The knowledge base can be created from existing sources, including:

  * Web sites containing frequently asked question (FAQ) documentation.
  * Files containing structured text, such as brochures or user guides.
  * Built-in _chit chat_ question and answer pairs that encapsulate common conversational exchanges.

Note

The question answering capability of Azure AI Language is a newer version of
the **QnA Service** , which still exists as a standalone service. To learn how
to migrate a QnA Maker knowledge base to Azure AI Language, see the [migration
guide](/en-us/azure/ai-services/language-service/question-answering/how-
to/migrate-qnamaker).

* * *"
64,Develop natural language solutions in Azure - Training,Create question answering solutions with Azure AI Language,Compare question answering to Azure AI Language understanding,"# Compare question answering to Azure AI Language understanding

Completed 100 XP

  * 2 minutes

A question answering knowledge base is a form of language model, which raises
the question of when to use question answering, and when to use the
_conversational language understanding_ capabilities of Azure AI Language.

The two features are similar in that they both enable you to define a language
model that can be queried using natural language expressions. However, there
are some differences in the use cases that they are designed to address, as
shown in the following table:

Expand table

| Question answering | Language understanding  
---|---|---  
Usage pattern | User submits a question, expecting an answer | User submits an
utterance, expecting an appropriate response or action  
Query processing | Service uses natural language understanding to match the
question to an answer in the knowledge base | Service uses natural language
understanding to interpret the utterance, match it to an intent, and identify
entities  
Response | Response is a static answer to a known question | Response
indicates the most likely intent and referenced entities  
Client logic | Client application typically presents the answer to the user |
Client application is responsible for performing appropriate action based on
the detected intent  
  
The two services are in fact complementary. You can build comprehensive
natural language solutions that combine language understanding models and
question answering knowledge bases.

* * *"
65,Develop natural language solutions in Azure - Training,Create question answering solutions with Azure AI Language,Create a knowledge base,"# Create a knowledge base

Completed 100 XP

  * 2 minutes

To create a question answering solution, you can use the REST API or SDK to
write code that defines, trains, and publishes the knowledge base. However,
it's more common to use the [Language Studio](https://language.azure.com) web
interface to define and manage a knowledge base.

To create a knowledge base you:

  1. Sign in to Azure portal.

  2. Search for **Azure AI services** using the search field at the top of the portal.

  3. Select **Create** under the **Language Service** resource.

  4. Create a resource in your Azure subscription:

    * Enable the _question answering_ feature.
    * Create or select an **Azure AI Search** resource to host the knowledge base index.
  5. In Language Studio, select your Azure AI Language resource and create a **Custom question answering** project.

  6. Add one or more data sources to populate the knowledge base:

    * URLs for web pages containing FAQs.
    * Files containing structured text from which questions and answers can be derived.
    * Predefined _chit-chat_ datasets that include common conversational questions and responses in a specified style.
  7. Edit question and answer pairs in the portal.

* * *"
66,Develop natural language solutions in Azure - Training,Create question answering solutions with Azure AI Language,Implement multi-turn conversation,"# Implement multi-turn conversation

Completed 100 XP

  * 2 minutes

Although you can often create an effective knowledge base that consists of
individual question and answer pairs, sometimes you might need to ask follow-
up questions to elicit more information from a user before presenting a
definitive answer. This kind of interaction is referred to as a _multi-turn_
conversation.

![A diagram showing a multi-turn conversation.](../../wwl-data-ai/create-
question-answer-solution-ai-language/media/multi-turn-conversation.png)

You can enable multi-turn responses when importing questions and answers from
an existing web page or document based on its structure, or you can explicitly
define follow-up prompts and responses for existing question and answer pairs.

For example, suppose an initial question for a travel booking knowledge base
is ""How can I cancel a reservation?"". A reservation might refer to a hotel or
a flight, so a follow-up prompt is required to clarify this detail. The answer
might consist of text such as ""Cancellation policies depend on the type of
reservation"" and include follow-up prompts with links to answers about
canceling flights and canceling hotels.

When you define a follow-up prompt for multi-turn conversation, you can link
to an existing answer in the knowledge base or define a new answer
specifically for the follow-up. You can also restrict the linked answer so
that it is only ever displayed in the context of the multi-turn conversation
initiated by the original question.

* * *"
67,Develop natural language solutions in Azure - Training,Create question answering solutions with Azure AI Language,Test and publish a knowledge base,"# Test and publish a knowledge base

Completed 100 XP

  * 1 minute

After you have defined a knowledge base, you can train its natural language
model, and test it before publishing it for use in an application or bot.

## Testing a knowledge base

You can test your knowledge base interactively in Language Studio, submitting
questions and reviewing the answers that are returned. You can inspect the
results to view their confidence scores as well as other potential answers.

![Screenshot of the test pane of the custom question answering project in the
Language studio.](../../wwl-data-ai/create-question-answer-solution-ai-
language/media/test-new-small.png)

## Deploying a knowledge base

When you're happy with the performance of your knowledge base, you can deploy
it to a REST endpoint that client applications can use to submit questions and
receive answers. You can deploy it directly from Language Studio.

* * *"
68,Develop natural language solutions in Azure - Training,Create question answering solutions with Azure AI Language,Use a knowledge base,"# Use a knowledge base

Completed 100 XP

  * 3 minutes

To consume the published knowledge base, you can use the REST interface.

The minimal request body for the function contains a question, like this:

JSON Copy

    
    
    {
      ""question"": ""What do I need to do to cancel a reservation?"",
      ""top"": 2,
      ""scoreThreshold"": 20,
      ""strictFilters"": [
        {
          ""name"": ""category"",
          ""value"": ""api""
        }
      ]
    }
    

Expand table

Property | Description  
---|---  
`question` | Question to send to the knowledge base.  
`top` | Maximum number of answers to be returned.  
`scoreThreshold` | Score threshold for answers returned.  
`strictFilters` | Limit to only answers that contain the specified metadata.  
  
The response includes the closest question match that was found in the
knowledge base, along with the associated answer, the confidence score, and
other metadata about the question and answer pair:

JSON Copy

    
    
    {
      ""answers"": [
        {
          ""score"": 27.74823341616769,
          ""id"": 20,
          ""answer"": ""Call us on 555 123 4567 to cancel a reservation."",
          ""questions"": [
            ""How can I cancel a reservation?""
          ],
          ""metadata"": [
            {
              ""name"": ""category"",
              ""value"": ""api""
            }
          ]
        }
      ]
    }
    

* * *"
69,Develop natural language solutions in Azure - Training,Create question answering solutions with Azure AI Language,Improve question answering performance,"# Improve question answering performance

Completed 100 XP

  * 6 minutes

After creating and testing a knowledge base, you can improve its performance
with _active learning_ and by defining _synonyms_.

## Use active learning

Active learning can help you make continuous improvements to get better at
answering user questions correctly over time. People often ask questions that
are phrased differently, but ultimately have the same meaning. Active learning
can help in situations like this because it enables you to consider alternate
questions to each question and answer pair. Active learning is enabled by
default.

To use active learning, you can do the following:

### Create your question and answer pairs

You create pairs of questions and answers in Language Studio for your project.
You can also import a file that contains question and answer pairs to upload
in bulk.

[ ![A screenshot showing how to import a file with question and answer
pairs.](../../wwl-data-ai/create-question-answer-solution-ai-
language/media/import-file-small.png) ](../../wwl-data-ai/create-question-
answer-solution-ai-language/media/import-file.png#lightbox)

### Review suggestions

Active learning then begins to offer alternate questions for each question in
your question and answer pairs. You access this from the Review suggestions
pane:

[ ![A screenshot of the Review suggestions pane.](../../wwl-data-ai/create-
question-answer-solution-ai-language/media/review-suggestions-small.png)
](../../wwl-data-ai/create-question-answer-solution-ai-language/media/review-
suggestions.png#lightbox)

You review, and then accept or reject these alternate phrases suggested for
each question by selecting the checkmark or delete symbol next to the
alternate phrase. You can bulk accept or reject suggestions using the **Accept
all suggestions** or **Reject all suggestions** option at the top.

You can also manually add alternate questions when you select **Add alternate
question** for a pair in the Edit knowledge base pane:

[ ![A screenshot showing the Add alternate question option on the Edit
knowledge base pane.](../../wwl-data-ai/create-question-answer-solution-ai-
language/media/add-alternate-questions-manual-small.png) ](../../wwl-data-
ai/create-question-answer-solution-ai-language/media/add-alternate-questions-
manual.png#lightbox)

Note

To learn more about active learning, see [Enrich your project with active
learning](/en-us/azure/ai-services/language-service/question-
answering/tutorials/active-learning).

## Define synonyms

Synonyms are useful when questions submitted by users might include multiple
different words to mean the same thing. For example, a travel agency customer
might refer to a ""reservation"" or a ""booking"". By defining these as synonyms,
the question answering service can find an appropriate answer regardless of
which term an individual customer uses.

To define synonyms, you use the REST API to submit synonyms in the following
JSON format:

JSON Copy

    
    
    {
        ""synonyms"": [
            {
                ""alterations"": [
                    ""reservation"",
                    ""booking""
                    ]
            }
        ]
    }
    

Note

To learn more about synonyms, see the [Improve quality of response with
synonyms](/en-us/azure/ai-services/language-service/question-
answering/tutorials/adding-synonyms).

* * *"
70,Develop natural language solutions in Azure - Training,Create question answering solutions with Azure AI Language,Exercise - Create a question answering solution,"# Exercise - Create a question answering solution

Completed 100 XP

  * 20 minutes

In this exercise, you use Azure AI Language to create your own question
answering model.

Note

To complete this lab, you need an **[Azure
subscription](https://azure.microsoft.com/free)**.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/create-question-answer-
solution-ai-language/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2322213)

Tip

After completing the exercise, if you've finished exploring Azure AI services,
delete the Azure resources that you created during the exercise.

* * *"
71,Develop natural language solutions in Azure - Training,Create question answering solutions with Azure AI Language,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

## Check your knowledge

1.

You want to create a knowledge base from an existing FAQ document. What should
you do?

Create an empty knowledge base and manually enter the FAQ questions and
answers.

Create a new knowledge base, importing the existing FAQ document.

Create a new knowledge base, selecting only the Professional chit-chat source.

2.

How can you add a multi-turn context for a question in an existing knowledge
base?

Add synonyms to the knowledge base.

Add alternative phrasing to the question.

Add a follow-up prompt to the question.

3.

How can you enable users to use your knowledge base through email?

Add Friendly Chit-chat to the knowledge base.

Enable Active Learning for the knowledge base and include the user's email
address as the userId parameter in responses.

Create a bot based on your knowledge base and configure an email channel.

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
72,Develop natural language solutions in Azure - Training,Create question answering solutions with Azure AI Language,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/create-question-answer-solution-ai-language/12-summary
73,Develop natural language solutions in Azure - Training,Build a conversational language understanding model,Introduction,"# Introduction

Completed 100 XP

  * 2 minutes

_Natural language processing_ (NLP) is a common AI problem in which software
must be able to work with text or speech in the natural language form that a
human user would write or speak. Within the broader area of NLP, _natural
language understanding_ (NLU) deals with the problem of determining semantic
meaning from natural language - usually by using a trained language model.

A common design pattern for a natural language understanding solution looks
like this:

![Diagram showing an app accepts natural language input, and uses a model to
determine semantic meaning before taking the appropriate action.](../../wwl-
data-ai/build-language-understanding-model/media/language-understanding-
app.png)

In this design pattern:

  1. An app accepts natural language input from a user.
  2. A language model is used to determine semantic meaning (the user's _intent_ ).
  3. The app performs an appropriate action.

**Azure AI Language** enables developers to build apps based on language
models that can be trained with a relatively small number of samples to
discern a user's intended meaning.

In this module, you'll learn how to use the service to create a natural
language understanding app using Azure AI Language.

After completing this module, you’ll be able to:

  * Provision an Azure AI Language resource.
  * Define intents, entities, and utterances.
  * Use patterns to differentiate similar utterances.
  * Use pre-built entity components.
  * Train, test, publish, and review a model.

* * *"
74,Develop natural language solutions in Azure - Training,Build a conversational language understanding model,Understand prebuilt capabilities of the Azure AI Language service,"# Understand prebuilt capabilities of the Azure AI Language service

Completed 100 XP

  * 6 minutes

The Azure AI Language service provides various features for understanding
human language. You can use each feature to better communicate with users,
better understand incoming communication, or use them together to provide more
insight into what the user is saying, intending, and asking about.

Azure AI Language service features fall into two categories: Pre-configured
features, and Learned features. Learned features require building and training
a model to correctly predict appropriate labels, which is covered in upcoming
units of this module.

This unit covers most of the capabilities of the Azure AI Language service,
but head over to the [Azure AI Language service
documentation](https://learn.microsoft.com/en-us/azure/cognitive-
services/language-service/overview) for a full list, including quickstarts and
a full explanation of everything available.

Using these features in your app requires sending your query to the
appropriate endpoint. The endpoint used to query a specific feature varies,
but all of them are prefixed with the Azure AI Language resource you created
in your Azure account, either when building your REST request or defining your
client using an SDK. Examples of each can be found in the next unit.

## Pre-configured features

The Azure AI Language service provides certain features without any model
labeling or training. Once you create your resource, you can send your data
and use the returned results within your app.

The following features are all pre-configured.

### Summarization

Summarization is available for both documents and conversations, and will
summarize the text into key sentences that are predicted to encapsulate the
input's meaning.

### Named entity recognition

Named entity recognition can extract and identify entities, such as people,
places, or companies, allowing your app to recognize different types of
entities for improved natural language responses. For example, given the text
""The waterfront pier is my favorite Seattle attraction"", _Seattle_ would be
identified and categorized as a location.

### Personally identifiable information (PII) detection

PII detection allows you to identify, categorize, and redact information that
could be considered sensitive, such as email addresses, home addresses, IP
addresses, names, and protected health information. For example, if the text
""email@contoso.com"" was included in the query, the entire email address can be
identified and redacted.

### Key phrase extraction

Key phrase extraction is a feature that quickly pulls the main concepts out of
the provided text. For example, given the text ""Text Analytics is one of the
features in Azure AI Services."", the service would extract _""Azure AI
Services""_ and _""Text Analytics""_.

### Sentiment analysis

Sentiment analysis identifies how positive or negative a string or document
is. For example, given the text ""Great hotel. Close to plenty of food and
attractions we could walk to"", the service would identify that as _positive_
with a relatively high confidence score.

### Language detection

Language detection takes one or more documents, and identifies the language
for each. For example, if the text of one of the documents was ""Bonjour"", the
service would identify that as _French_.

## Learned features

Learned features require you to label data, train, and deploy your model to
make it available to use in your application. These features allow you to
customize what information is predicted or extracted.

Note

Quality of data greatly impacts the model's accuracy. Be intentional about
what data is used, how well it is tagged or labeled, and how varied the
training data is. For details, see [recommendations for labeling data](/en-
us/azure/ai-services/language-service/conversational-language-
understanding/how-to/tag-utterances), which includes valuable guidelines for
tagging data. Also see the [evaluation metrics](/en-us/azure/ai-
services/language-service/custom-text-classification/concepts/evaluation-
metrics) that can assist in learning where your model needs improvement.

### Conversational language understanding (CLU)

CLU is one of the core custom features offered by Azure AI Language. CLU helps
users to build custom natural language understanding models to predict overall
intent and extract important information from incoming utterances. CLU does
require data to be tagged by the user to teach it how to predict intents and
entities accurately.

The exercise in this module will be building a CLU model and using it in your
app.

### Custom named entity recognition

Custom entity recognition takes custom labeled data and extracts specified
entities from unstructured text. For example, if you have various contract
documents that you want to extract involved parties from, you can train a
model to recognize how to predict them.

### Custom text classification

Custom text classification enables users to classify text or documents as
custom defined groups. For example, you can train a model to look at news
articles and identify the category they should fall into, such as _News_ or
_Entertainment_.

### Question answering

Question answering is a mostly pre-configured feature that provides answers to
questions provided as input. The data to answer these questions comes from
documents like FAQs or manuals.

For example, say you want to make a virtual chat assistant on your company
website to answer common questions. You could use a company FAQ as the input
document to create the question and answer pairs. Once deployed, your chat
assistant can pass input questions to the service, and get the answers as a
result.

For a complete list of capabilities and how to use them, see the Azure AI
Language [documentation](https://learn.microsoft.com/en-us/azure/ai-
services/language-service/overview).

* * *"
75,Develop natural language solutions in Azure - Training,Build a conversational language understanding model,Understand resources for building a conversational language understanding model,"# Understand resources for building a conversational language understanding
model

Completed 100 XP

  * 9 minutes

To use the Language Understanding service to develop a NLP solution, you'll
need to create a Language resource in Azure. That resource will be used for
both authoring your model and processing prediction requests from client
applications.

Tip

This module's lab covers building a model for conversational language
understanding. For more focused modules on custom text classification and
custom named entity recognition, see the custom solution modules in the
[Develop natural language solutions](https://learn.microsoft.com/en-
us/training/paths/develop-language-solutions-azure-ai) learning path.

## Build your model

For features that require a model for prediction, you'll need to build, train
and deploy that model before using it to make a prediction. This building and
training will teach the Azure AI Language service what to look for.

First, you'll need to create your Azure AI Language resource in the [Azure
portal](https://portal.azure.com/). Then:

  1. Search for **Azure AI services**.
  2. Find and select **Language Service**.
  3. Select **Create** under the **Language Service**.
  4. Fill out the necessary details, choosing the region closest to you geographically (for best performance) and giving it a unique name.

Once that resource has been created, you'll need a key and the endpoint. You
can find that on the left side under **Keys and Endpoint** of the resource
overview page.

### Use Language Studio

For a more visual method of building, training, and deploying your model, you
can use [Language Studio](https://aka.ms/languageStudio) to achieve each of
these steps. On the main page, you can choose to create a **Conversational
language understanding** project. Once the project is created, then go through
the same process as above to build, train, and deploy your model.

[ ![Screenshot of the Language Studio home page.](../../wwl-data-ai/build-
language-understanding-model/media/language-studio-conversational-small.png)
](../../wwl-data-ai/build-language-understanding-model/media/language-studio-
conversational.png#lightbox)

The lab in this module will walk through using Language Studio to build your
model. If you'd like to learn more, see the [Language Studio
quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-
service/language-studio)

### Use the REST API

One way to build your model is through the REST API. The pattern would be to
create your project, import data, train, deploy, then use your model.

These tasks are done asynchronously; you'll need to submit a request to the
appropriate URI for each step, and then send another request to get the status
of that job.

For example, if you want to deploy a model for a conversational language
understanding project, you'd submit the deployment job, and then check on the
deployment job status.

#### Authentication

For each call to your Azure AI Language resource, you authenticate the request
by providing the following header.

Expand table

Key | Value  
---|---  
`Ocp-Apim-Subscription-Key` | The key to your resource  
  
#### Request deployment

Submit a **POST** request to the following endpoint.

rest Copy

    
    
    {ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/deployments/{DEPLOYMENT-NAME}?api-version={API-VERSION}
    

Expand table

Placeholder | Value | Example  
---|---|---  
`{ENDPOINT}` | The endpoint of your Azure AI Language resource |
`https://<your-subdomain>.cognitiveservices.azure.com`  
`{PROJECT-NAME}` | The name for your project. This value is case-sensitive |
`myProject`  
`{DEPLOYMENT-NAME}` | The name for your deployment. This value is case-
sensitive | `staging`  
`{API-VERSION}` | The version of the API you're calling | `2022-05-01`  
  
Include the following `body` with your request.

JSON Copy

    
    
    {
      ""trainedModelLabel"": ""{MODEL-NAME}"",
    }
    

Expand table

Placeholder | Value  
---|---  
`{MODEL-NAME}` | The model name that will be assigned to your deployment. This
value is case-sensitive.  
  
Successfully submitting your request will receive a `202` response, with a
response header of `operation-location`. This header will have a URL with
which to request the status, formatted like this:

rest Copy

    
    
    {ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?api-version={API-VERSION}
    

#### Get deployment status

Submit a **GET** request to the URL from the response header above. The values
will already be filled out based on the initial deployment request.

rest Copy

    
    
    {ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?api-version={API-VERSION}
    

Expand table

Placeholder | Value  
---|---  
`{ENDPOINT}` | The endpoint for authenticating your API request  
`{PROJECT-NAME}` | The name for your project (case-sensitive)  
`{DEPLOYMENT-NAME}` | The name for your deployment (case-sensitive)  
`{JOB-ID}` | The ID for locating your model's training status, found in the
header value detailed above in the deployment request  
`{API-VERSION}` | The version of the API you're calling  
  
The response body will give the deployment status details. The `status` field
will have the value of _succeeded_ when the deployment is complete.

JSON Copy

    
    
    {
        ""jobId"":""{JOB-ID}"",
        ""createdDateTime"":""String"",
        ""lastUpdatedDateTime"":""String"",
        ""expirationDateTime"":""String"",
        ""status"":""running""
    }
    

For a full walkthrough of each step with example requests, see the
[conversational understanding quickstart](https://learn.microsoft.com/en-
us/azure/ai-services/language-service/conversational-language-
understanding/quickstart?pivots=rest-api#create-a-clu-project).

## Query your model

To query your model for a prediction, you can use SDKs in C# or Python, or use
the REST API.

### Query using SDKs

To query your model using an SDK, you first need to create your client. Once
you have your client, you then use it to call the appropriate endpoint.

C# Copy

    
    
    var languageClient = new TextAnalyticsClient(endpoint, credentials);
    var response = languageClient.ExtractKeyPhrases(document);
    

Python Copy

    
    
    language_client = TextAnalyticsClient(
                endpoint=endpoint, 
                credential=credentials)
    response = language_client.extract_key_phrases(documents = documents)[0]
    

Other language features, such as the conversational language understanding,
require the request be built and sent differently.

C# Copy

    
    
    var data = new
    {
        analysisInput = new
        {
            conversationItem = new
            {
                text = userText,
                id = ""1"",
                participantId = ""1"",
            }
        },
        parameters = new
        {
            projectName,
            deploymentName,
            // Use Utf16CodeUnit for strings in .NET.
            stringIndexType = ""Utf16CodeUnit"",
        },
        kind = ""Conversation"",
    };
    Response response = await client.AnalyzeConversationAsync(RequestContent.Create(data));
    

Python Copy

    
    
    result = client.analyze_conversation(
        task={
            ""kind"": ""Conversation"",
            ""analysisInput"": {
                ""conversationItem"": {
                    ""participantId"": ""1"",
                    ""id"": ""1"",
                    ""modality"": ""text"",
                    ""language"": ""en"",
                    ""text"": query
                },
                ""isLoggingEnabled"": False
            },
            ""parameters"": {
                ""projectName"": cls_project,
                ""deploymentName"": deployment_slot,
                ""verbose"": True
            }
        }
    )
    

### Query using the REST API

To query your model using REST, create a **POST** request to the appropriate
URL with the appropriate body specified. For built in features such as
language detection or sentiment analysis, you'll query the `analyze-text`
endpoint.

Tip

Remember each request needs to be authenticated with your Azure AI Language
resource key in the `Ocp-Apim-Subscription-Key` header

rest Copy

    
    
    {ENDPOINT}/language/:analyze-text?api-version={API-VERSION}
    

Expand table

Placeholder | Value  
---|---  
`{ENDPOINT}` | The endpoint for authenticating your API request  
`{API-VERSION}` | The version of the API you're calling  
  
Within the body of that request, you must specify the `kind` parameter, which
tells the service what type of language understanding you're requesting.

If you want to detect the language, for example, the JSON body would look
something like the following.

JSON Copy

    
    
    {
        ""kind"": ""LanguageDetection"",
        ""parameters"": {
            ""modelVersion"": ""latest""
        },
        ""analysisInput"":{
            ""documents"":[
                {
                    ""id"":""1"",
                    ""text"": ""This is a document written in English.""
                }
            ]
        }
    }
    

Other language features, such as the conversational language understanding,
require the request be routed to a different endpoint. For example, the
conversational language understanding request would be sent to the following.

rest Copy

    
    
    {ENDPOINT}/language/:analyze-conversations?api-version={API-VERSION}
    

Expand table

Placeholder | Value  
---|---  
`{ENDPOINT}` | The endpoint for authenticating your API request  
`{API-VERSION}` | The version of the API you're calling  
  
That request would include a JSON body similar to the following.

JSON Copy

    
    
    {
      ""kind"": ""Conversation"",
      ""analysisInput"": {
        ""conversationItem"": {
          ""id"": ""1"",
          ""participantId"": ""1"",
          ""text"": ""Sample text""
        }
      },
      ""parameters"": {
        ""projectName"": ""{PROJECT-NAME}"",
        ""deploymentName"": ""{DEPLOYMENT-NAME}"",
        ""stringIndexType"": ""TextElement_V8""
      }
    }
    

Expand table

Placeholder | Value  
---|---  
`{PROJECT-NAME}` | The name of the project where you built your model  
`{DEPLOYMENT-NAME}` | The name of your deployment  
  
### Sample response

The query response from an SDK will in the object returned, which varies
depending on the feature (such as in `response.key_phrases` or
`response.Value`). The REST API will return JSON that would be similar to the
following.

JSON Copy

    
    
    {
        ""kind"": ""KeyPhraseExtractionResults"",
        ""results"": {
            ""documents"": [{
                ""id"": ""1"",
                ""keyPhrases"": [""modern medical office"", ""Dr. Smith"", ""great staff""],
                ""warnings"": []
            }],
            ""errors"": [],
            ""modelVersion"": ""{VERSION}""
        }
    }
    

For other models like conversational language understanding, a sample response
to your query would be similar to the following.

JSON Copy

    
    
    {
      ""kind"": ""ConversationResult"",
      ""result"": {
        ""query"": ""String"",
        ""prediction"": {
          ""topIntent"": ""intent1"",
          ""projectKind"": ""Conversation"",
          ""intents"": [
            {
              ""category"": ""intent1"",
              ""confidenceScore"": 1
            },
            {
              ""category"": ""intent2"",
              ""confidenceScore"": 0
            }
          ],
          ""entities"": [
            {
              ""category"": ""entity1"",
              ""text"": ""text"",
              ""offset"": 7,
              ""length"": 4,
              ""confidenceScore"": 1
            }
          ]
        }
      }
    }
    

The SDKs for both Python and C# return JSON that is very similar to the REST
response.

For full documentation on features, including examples and how-to guides, see
the [Azure AI Language documentation](https://learn.microsoft.com/en-
us/azure/ai-services/language-service/) documentation pages.

* * *"
76,Develop natural language solutions in Azure - Training,Build a conversational language understanding model,"Define intents, utterances, and entities","# Define intents, utterances, and entities

Completed 100 XP

  * 5 minutes

_Utterances_ are the phrases that a user might enter when interacting with an
application that uses your language model. An _intent_ represents a task or
action the user wants to perform, or more simply the _meaning_ of an
utterance. You create a model by defining intents and associating them with
one or more utterances.

For example, consider the following list of intents and associated utterances:

  * **GetTime** : 
    * ""What time is it?""
    * ""What is the time?""
    * ""Tell me the time""
  * **GetWeather** : 
    * ""What is the weather forecast?""
    * ""Do I need an umbrella?""
    * ""Will it snow?""
  * **TurnOnDevice**
    * ""Turn the light on.""
    * ""Switch on the light.""
    * ""Turn on the fan""
  * **None** : 
    * ""Hello""
    * ""Goodbye""

In your model, you must define the intents that you want your model to
understand, so spend some time considering the _domain_ your model must
support and the kinds of actions or information that users might request. In
addition to the intents that you define, every model includes a **None**
intent that you should use to explicitly identify utterances that a user might
submit, but for which there is no specific action required (for example,
conversational greetings like ""hello"") or that fall outside of the scope of
the domain for this model.

After you've identified the intents your model must support, it's important to
capture various different example utterances for each intent. Collect
utterances that you think users will enter; including utterances meaning the
same thing but that are constructed in different ways. Keep these guidelines
in mind:

  * Capture multiple different examples, or alternative ways of saying the same thing
  * Vary the length of the utterances from short, to medium, to long
  * Vary the location of the _noun_ or _subject_ of the utterance. Place it at the beginning, the end, or somewhere in between
  * Use correct grammar and incorrect grammar in different utterances to offer good training data examples
  * The precision, consistency and completeness of your labeled data are key factors to determining model performance. 
    * Label **precisely** : Label each entity to its right type always. Only include what you want extracted, avoid unnecessary data in your labels.
    * Label **consistently** : The same entity should have the same label across all the utterances.
    * Label **completely** : Label all the instances of the entity in all your utterances.

_Entities_ are used to add specific context to intents. For example, you might
define a **TurnOnDevice** intent that can be applied to multiple devices, and
use entities to define the different devices.

Consider the following utterances, intents, and entities:

Expand table

Utterance | Intent | Entities  
---|---|---  
What is the time? | GetTime |  
What time is it in _London_? | GetTime | Location (London)  
What's the weather forecast for _Paris_? | GetWeather | Location (Paris)  
Will I need an umbrella _tonight_? | GetWeather | Time (tonight)  
What's the forecast for _Seattle tomorrow_? | GetWeather | Location (Seattle),
Time (tomorrow)  
Turn the _light_ on. | TurnOnDevice | Device (light)  
Switch on the _fan_. | TurnOnDevice | Device (fan)  
  
You can split entities into a few different component types:

  * **Learned** entities are the most flexible kind of entity, and should be used in most cases. You define a learned component with a suitable name, and then associate words or phrases with it in training utterances. When you train your model, it learns to match the appropriate elements in the utterances with the entity.
  * **List** entities are useful when you need an entity with a specific set of possible values - for example, days of the week. You can include synonyms in a list entity definition, so you could define a **DayOfWeek** entity that includes the values ""Sunday"", ""Monday"", ""Tuesday"", and so on; each with synonyms like ""Sun"", ""Mon"", ""Tue"", and so on.
  * **Prebuilt** entities are useful for common types such as numbers, datetimes, and names. For example, when prebuilt components are added, you will automatically detect values such as ""6"" or organizations such as ""Microsoft"". You can see this article for a list of [supported prebuilt entities](https://learn.microsoft.com/en-us/azure/ai-services/language-service/conversational-language-understanding/prebuilt-component-reference).

* * *"
77,Develop natural language solutions in Azure - Training,Build a conversational language understanding model,Use patterns to differentiate similar utterances,"# Use patterns to differentiate similar utterances

Completed 100 XP

  * 2 minutes

In some cases, a model might contain multiple intents for which utterances are
likely to be similar. You can use the pattern of utterances to disambiguate
the intents while minimizing the number of sample utterances.

For example, consider the following utterances:

  * ""Turn on the kitchen light""
  * ""Is the kitchen light on?""
  * ""Turn off the kitchen light""

These utterances are syntactically similar, with only a few differences in
words or punctuation. However, they represent three different intents (which
could be named **TurnOnDevice** , **GetDeviceStatus** , and **TurnOffDevice**
). Additionally, the intents could apply to a wide range of entity values. In
addition to ""kitchen light"", the intent could apply to ""living room light"",
television"", or any other device that the model might need to support.

To correctly train your model, provide a handful of examples of each intent
that specify the different formats of utterances.

  * **TurnOnDevice** : 
    * ""Turn on the {DeviceName}""
    * ""Switch on the {DeviceName}""
    * ""Turn the {DeviceName} on""
  * **GetDeviceStatus** : 
    * ""Is the {DeviceName} on[?]""
  * **TurnOffDevice** : 
    * ""Turn the {DeviceName} off""
    * ""Switch off the {DeviceName}""
    * ""Turn off the {DeviceName}""

When you teach your model with each different type of utterance, the Azure AI
Language service can learn how to categorize intents correctly based off
format and punctuation.

* * *"
78,Develop natural language solutions in Azure - Training,Build a conversational language understanding model,Use pre-built entity components,"# Use pre-built entity components

Completed 100 XP

  * 1 minute

You can create your own language models by defining all the intents and
utterances it requires, but often you can use prebuilt components to detect
common entities such as numbers, emails, URLs, or choices.

For a full list of prebuilt entities the Azure AI Language service can detect,
see the list of [supported prebuilt entity
components.](https://learn.microsoft.com/en-us/azure/ai-services/language-
service/conversational-language-understanding/prebuilt-component-reference)

Using prebuilt components allows you to let the Azure AI Language service
automatically detect the specified type of entity, and not have to train your
model with examples of that entity.

To add a prebuilt component, you can create an entity in your project, then
select **Add new prebuilt** to that entity to detect certain entities.

[ ![Screenshot of adding a prebuilt entity component.](../../wwl-data-
ai/build-language-understanding-model/media/add-prebuilt-entity-small.png)
](../../wwl-data-ai/build-language-understanding-model/media/add-prebuilt-
entity.png#lightbox)

You can have up to five prebuilt components per entity. Using prebuilt model
elements can significantly reduce the time it takes to develop a
conversational language understanding solution.

* * *"
79,Develop natural language solutions in Azure - Training,Build a conversational language understanding model,"Train, test, publish, and review a conversational language understanding model","# Train, test, publish, and review a conversational language understanding
model

Completed 100 XP

  * 1 minute

Creating a model is an iterative process with the following activities:

![Diagram that shows the train, test, publish, review cycle.](../../wwl-data-
ai/build-language-understanding-model/media/train-test-publish-review.png)

  1. Train a model to learn intents and entities from sample utterances.
  2. Test the model interactively or using a testing dataset with known labels
  3. Deploy a trained model to a public endpoint so client apps can use it
  4. Review predictions and iterate on utterances to train your model

By following this iterative approach, you can improve the language model over
time based on user input, helping you develop solutions that reflect the way
users indicate their intents using natural language.

* * *"
80,Develop natural language solutions in Azure - Training,Build a conversational language understanding model,Exercise - Build an Azure AI services conversational language understanding model,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/build-language-understanding-model/7-exercise
81,Develop natural language solutions in Azure - Training,Build a conversational language understanding model,Module assessment,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/build-language-understanding-model/8-knowledge-check
82,Develop natural language solutions in Azure - Training,Build a conversational language understanding model,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/build-language-understanding-model/9-summary
83,Develop natural language solutions in Azure - Training,Create custom text classification solutions,Introduction,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/custom-text-classification/1-introduction
84,Develop natural language solutions in Azure - Training,Create custom text classification solutions,Understand types of classification projects,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/custom-text-classification/2-understand-types-of-classification-projects
85,Develop natural language solutions in Azure - Training,Create custom text classification solutions,Understand how to build text classification projects,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/custom-text-classification/3-understand-how-to-build-projects
86,Develop natural language solutions in Azure - Training,Create custom text classification solutions,Exercise - Classify text,"# Exercise - Classify text

Completed 100 XP

  * 35 minutes

In this exercise, you use Azure AI Language to build a custom text
classification model.

Note

To complete this lab, you need an **[Azure
subscription](https://azure.microsoft.com/free)**.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/custom-text-
classification/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2322137)

Tip

After completing the exercise, if you've finished exploring Azure AI services,
delete the Azure resources that you created during the exercise.

* * *"
87,Develop natural language solutions in Azure - Training,Create custom text classification solutions,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

## Check your knowledge

1.

You want to train a model to classify book summaries by their genre, and some
of your favorite books are both mystery and thriller. Which type of project
should you build?

A single label classification project

A multiple label classification project

A varied label classification project

2.

You just got notification your training job is complete. What is your next
step?

Label more data

Deploy your model

View your model details

3.

You want to submit a classification task via the API. How do you get the
results of the classification?

The result is in the response of the classification request.

Call an endpoint with your deployment name to get the most recent
classification.

Call the URL provided in the header of the request response.

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
88,Develop natural language solutions in Azure - Training,Create custom text classification solutions,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/custom-text-classification/6-summary
89,Develop natural language solutions in Azure - Training,Custom named entity recognition,Introduction,"# Introduction

Completed 100 XP

  * 1 minute

Custom _named entity recognition_ (NER), otherwise known as custom entity
extraction, is one of the many features for _natural language processing_
(NLP) offered by Azure AI Language service. Custom NER enables developers to
extract predefined entities from text documents, without those documents being
in a known format - such as legal agreements or online ads.

An entity is a person, place, thing, event, skill, or value.

In this module, you'll learn how to use the Azure AI Language service to
extract entities from unstructured documents.

After completing this module, you'll be able to:

  * Understand custom named entities and how they're labeled.
  * Build a custom named entity extraction project.
  * Label data, train, and deploy an entity extraction model.
  * Submit extraction tasks from your own app.

* * *"
90,Develop natural language solutions in Azure - Training,Custom named entity recognition,Understand custom named entity recognition,"# Understand custom named entity recognition

Completed 100 XP

  * 7 minutes

Custom NER is an Azure API service that looks at documents, identifies, and
extracts user defined entities. These entities could be anything from names
and addresses from bank statements to knowledge mining to improve search
results.

Custom NER is part of Azure AI Language in Azure AI services.

## Custom vs built-in NER

Azure AI Language provides certain built-in entity recognition, to recognize
things such as a person, location, organization, or URL. Built-in NER allows
you to set up the service with minimal configuration, and extract entities. To
call a built-in NER, create your service and call the endpoint for that NER
service like this:

rest Copy

    
    
    <YOUR-ENDPOINT>/language/analyze-text/jobs?api-version=<API-VERSION>
    

Expand table

Placeholder | Value | Example  
---|---|---  
`<YOUR-ENDPOINT>` | The endpoint for your API request | `https://<your-
resource>.cognitiveservices.azure.com`  
`<API-VERSION>` | The version of the API you are calling | `2023-05-01`  
  
The body of that call will contain the document(s) the entities are extracted
from, and the headers contain your service key.

The response from the call above contains an array of entities recognized,
such as:

JSON Copy

    
    
    <...>
    ""entities"":[
        {
            ""text"":""Seattle"",
            ""category"":""Location"",
            ""subcategory"":""GPE"",
            ""offset"":45,
            ""length"":7,
            ""confidenceScore"":0.99
        },
        {
            ""text"":""next week"",
            ""category"":""DateTime"",
            ""subcategory"":""DateRange"",
            ""offset"":104,
            ""length"":9,
            ""confidenceScore"":0.8
        }
    ]
    <...>
    

Examples of when to use the built-in NER include finding locations, names, or
URLs in long text documents.

Tip

A full list of recognized entity categories is available in the [NER
docs](https://aka.ms/ner-categories).

Custom NER, which is the focus of the rest of this module, is available when
the entities you want to extract aren't part of the built-in service or you
only want to extract specific entities. You can make your custom NER model as
simple or complex as is required for your app.

Examples of when you'd want custom NER include specific legal or bank data,
knowledge mining to enhance catalog search, or looking for specific text for
audit policies. Each one of these projects requires a specific set of entities
and data it needs to extract.

## Azure AI Language project life cycle

[ ![Conceptual diagram showing a project steps to define entities, tag data,
train model, view model, improve model, deploy model, and extract
entities.](../../wwl-data-ai/custom-name-entity-recognition/media/extraction-
development-lifecycle.png) ](../../wwl-data-ai/custom-name-entity-
recognition/media/extraction-development-lifecycle.png#lightbox)

Creating an entity extraction model typically follows a similar path to most
Azure AI Language service features:

  1. **Define entities** : Understanding the data and entities you want to identify, and try to make them as clear as possible. For example, defining exactly which parts of a bank statement you want to extract.
  2. **Tag data** : Label, or tag, your existing data, specifying what text in your dataset corresponds to which entity. This step is important to do accurately and completely, as any wrong or missed labels will reduce the effectiveness of the trained model. A good variation of possible input documents is useful. For example, label bank name, customer name, customer address, specific loan or account terms, loan or account amount, and account number.
  3. **Train model** : Train your model once your entities are labeled. Training teaches your model how to recognize the entities you label.
  4. **View model** : After your model is trained, view the results of the model. This page includes a score of 0 to 1 that is based on the precision and recall of the data tested. You can see which entities worked well (such as customer name) and which entities need improvement (such as account number).
  5. **Improve model** : Improve your model by seeing which entities failed to be identified, and which entities were incorrectly extracted. Find out what data needs to be added to your model's training to improve performance. This page shows you how entities failed, and which entities (such as account number) need to be differentiated from other similar entities (such as loan amount).
  6. **Deploy model** : Once your model performs as desired, deploy your model to make it available via the API. In our example, you can send to requests to the model when it's deployed to extract bank statement entities.
  7. **Extract entities** : Use your model for extracting entities. The lab covers how to use the API, and you can view the [API reference](https://aka.ms/ct-runtime-swagger) for more details.

## Considerations for data selection and refining entities

For the best performance, you'll need to use both high quality data to train
the model and clearly defined entity types.

High quality data will let you spend less time refining and yield better
results from your model.

  * **Diversity** \- use as diverse of a dataset as possible without losing the real-life distribution expected in the real data. You'll want to use sample data from as many sources as possible, each with their own formats and number of entities. It's best to have your dataset represent as many different sources as possible.
  * **Distribution** \- use the appropriate distribution of document types. A more diverse dataset to train your model will help your model avoid learning incorrect relationships in the data.
  * **Accuracy** \- use data that is as close to real world data as possible. Fake data works to start the training process, but it likely will differ from real data in ways that can cause your model to not extract correctly.

Entities need to also be carefully considered, and defined as distinctly as
possible. Avoid ambiguous entities (such as two names next to each other on a
bank statement), as it will make the model struggle to differentiate. If
having some ambiguous entities is required, make sure to have more examples
for your model to learn from so it can understand the difference.

Keeping your entities distinct will also go a long way in helping your model's
performance. For example, trying to extract something like ""Contact info"" that
could be a phone number, social media handle, or email address would require
several examples to correctly teach your model. Instead, try to break them
down into more specific entities such as ""Phone"", ""Email"", and ""Social media""
and let the model label whichever type of contact information it finds.

## How to extract entities

To submit an extraction task, the API requires the JSON body to specify which
task to execute. For custom NER, the task for the JSON payload is
`CustomEntityRecognition`.

Your payload will look similar to the following JSON:

JSON Copy

    
    
    {
        ""displayName"": ""string"",
        ""analysisInput"": {
            ""documents"": [
                {
                    ""id"": ""doc1"", 
                    ""text"": ""string""
                },
                {
                    ""id"": ""doc2"",
                    ""text"": ""string""
                }
            ]
        },
        ""tasks"": [
            {
                ""kind"": ""CustomEntityRecognition"",
                ""taskName"": ""MyRecognitionTaskName"",
                ""parameters"": {
                ""projectName"": ""MyProject"",
                ""deploymentName"": ""MyDeployment""
                }
            }
        ]
    }
    

## Project limits

The Azure AI Language service enforces the following restrictions:

  * **Training** \- at least 10 files, and not more than 100,000
  * **Deployments** \- 10 deployment names per project
  * **APIs**
    * **Authoring** \- this API creates a project, trains, and deploys your model. Limited to 10 POST and 100 GET per minute
    * **Analyze** \- this API does the work of actually extracting the entities; it requests a task and retrieves the results. Limited to 20 GET or POST
  * **Projects** \- only 1 storage account per project, 500 projects per resource, and 50 trained models per project
  * **Entities** \- each entity can be up to 500 characters. You can have up to 200 entity types.

See the [Service limits for Azure AI Language](/en-us/azure/ai-
services/language-service/concepts/data-limits) page for detailed information.

* * *"
91,Develop natural language solutions in Azure - Training,Custom named entity recognition,Label your data,"# Label your data

Completed 100 XP

  * 4 minutes

Labeling, or tagging, your data correctly is an important part of the process
to create a custom entity extraction model. Labels identify examples of
specific entities in text used to train the model. Three things to focus on
are:

  * **Consistency** \- Label your data the same way across all files for training. Consistency allows your model to learn without any conflicting inputs.
  * **Precision** \- Label your entities consistently, without unnecessary extra words. Precision ensures only the correct data is included in your extracted entity.
  * **Completeness** \- Label your data completely, and don't miss any entities. Completeness helps your model always recognize the entities present.

[ ![Screenshot of labeling an entity in Language Studio.](../../wwl-data-
ai/custom-name-entity-recognition/media/tag-entity-screenshot.png)
](../../wwl-data-ai/custom-name-entity-recognition/media/tag-entity-
screenshot.png#lightbox)

## How to label your data

Language Studio is the most straight forward method for labeling your data.
Language Studio allows you to see the file, select the beginning and end of
your entity, and specify which entity it is.

Each label that you identify gets saved into a file that lives in your storage
account with your dataset, in an auto-generated JSON file. This file then gets
used by the model to learn how to extract custom entities. It's possible to
provide this file when creating your project (if you're importing the same
labels from a different project, for example) however it must be in the
[Accepted custom NER data formats](/en-us/azure/ai-services/language-
service/custom-named-entity-recognition/concepts/data-formats). For example:

JSON Copy

    
    
    {
      ""projectFileVersion"": ""{DATE}"",
      ""stringIndexType"": ""Utf16CodeUnit"",
      ""metadata"": {
        ""projectKind"": ""CustomEntityRecognition"",
        ""storageInputContainerName"": ""{CONTAINER-NAME}"",
        ""projectName"": ""{PROJECT-NAME}"",
        ""multilingual"": false,
        ""description"": ""Project-description"",
        ""language"": ""en-us"",
        ""settings"": {}
      },
      ""assets"": {
        ""projectKind"": ""CustomEntityRecognition"",
        ""entities"": [
          {
            ""category"": ""Entity1""
          },
          {
            ""category"": ""Entity2""
          }
        ],
        ""documents"": [
          {
            ""location"": ""{DOCUMENT-NAME}"",
            ""language"": ""{LANGUAGE-CODE}"",
            ""dataset"": ""{DATASET}"",
            ""entities"": [
              {
                ""regionOffset"": 0,
                ""regionLength"": 500,
                ""labels"": [
                  {
                    ""category"": ""Entity1"",
                    ""offset"": 25,
                    ""length"": 10
                  },
                  {
                    ""category"": ""Entity2"",
                    ""offset"": 120,
                    ""length"": 8
                  }
                ]
              }
            ]
          },
          {
            ""location"": ""{DOCUMENT-NAME}"",
            ""language"": ""{LANGUAGE-CODE}"",
            ""dataset"": ""{DATASET}"",
            ""entities"": [
              {
                ""regionOffset"": 0,
                ""regionLength"": 100,
                ""labels"": [
                  {
                    ""category"": ""Entity2"",
                    ""offset"": 20,
                    ""length"": 5
                  }
                ]
              }
            ]
          }
        ]
      }
    }
    
    

Expand table

Field | Description  
---|---  
`documents` | Array of labeled documents  
`location` | Path to file within container connected to the project  
`language` | Language of the file  
`entities` | Array of present entities in the current document  
`regionOffset` | Inclusive character position for start of text  
`regionLength` | Length in characters of the data used in training  
`category` | Name of entity to extract  
`labels` | Array of labeled entities in the files  
`offset` | Inclusive character position for start of entity  
`length` | Length in characters of the entity  
`dataset` | Which dataset the file is assigned to  
  
* * *"
92,Develop natural language solutions in Azure - Training,Custom named entity recognition,Train and evaluate your model,"# Train and evaluate your model

Completed 100 XP

  * 3 minutes

Training and evaluating your model is an iterative process of adding data and
labels to your training dataset to teach the model more accurately. To know
what types of data and labels need to be improved, Language Studio provides
scoring in the **View model details** page on the left hand pane.

[ ![Screenshot of the View model scoring tab.](../../wwl-data-ai/custom-name-
entity-recognition/media/model-scoring-new.png) ](../../wwl-data-ai/custom-
name-entity-recognition/media/model-scoring-new.png#lightbox)

Individual entities and your overall model score are broken down into three
metrics to explain how they're performing and where they need to improve.

Expand table

Metric | Description  
---|---  
Precision | The ratio of successful entity recognitions to all attempted
recognitions. A high score means that as long as the entity is recognized,
it's labeled correctly.  
Recall | The ratio of successful entity recognitions to the actual number of
entities in the document. A high score means it finds the entity or entities
well, regardless of if it assigns them the right label  
F1 score | Combination of precision and recall providing a single scoring
metric  
  
Scores are available both per entity and for the model as a whole. You may
find an entity scores well, but the whole model doesn't.

## How to interpret metrics

Ideally we want our model to score well in both precision and recall, which
means the entity recognition works well. If both metrics have a low score, it
means the model is both struggling to recognize entities in the document, and
when it does extract that entity, it doesn't assign it the correct label with
high confidence.

If precision is low but recall is high, it means that the model recognizes the
entity well but doesn't label it as the correct entity type.

If precision is high but recall is low, it means that the model doesn't always
recognize the entity, but when the model extracts the entity, the correct
label is applied.

## Confusion matrix

On the same **View model details** page, there's another tab on the top for
the _Confusion matrix_. This view provides a visual table of all the entities
and how each performed, giving a complete view of the model and where it's
falling short.

[ ![Screenshot of a sample confusion matrix.](../../wwl-data-ai/custom-name-
entity-recognition/media/model-confusion-matrix-new.png) ](../../wwl-data-
ai/custom-name-entity-recognition/media/model-confusion-matrix-
new.png#lightbox)

The confusion matrix allows you to visually identify where to add data to
improve your model's performance.

* * *"
93,Develop natural language solutions in Azure - Training,Custom named entity recognition,Exercise - Extract custom entities,"# Exercise - Extract custom entities

Completed 100 XP

  * 35 minutes

In this exercise, you use Azure AI Language to build a custom named entity
recognition model.

Note

To complete this lab, you need an **[Azure
subscription](https://azure.microsoft.com/free)**.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/custom-name-entity-
recognition/media/launch-
exercise.png)](https://microsoftlearning.github.io/mslearn-ai-
language/Instructions/Labs/05-extract-custom-entities.html)

Tip

After completing the exercise, if you've finished exploring Azure AI services,
delete the Azure resources that you created during the exercise.

* * *"
94,Develop natural language solutions in Azure - Training,Custom named entity recognition,Module assessment,"# Module assessment

Completed 200 XP

  * 2 minutes

## Check your knowledge

1.

You've trained your model and you're seeing that it doesn't recognize your
entities. What metric score is likely low to indicate that issue?

Recall

Precision

F1 score

2.

You just finished labeling your data. How and where is that file stored to
train your model?

JSON file, in my storage account container for the project

XML file, in my local project folder

YAML file, anywhere in my Azure account

3.

You train your model with only one source of documents, even though real
extraction tasks will come from several sources. What data quality metric do
you need to increase?

Distribution

Accuracy

Diversity

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
95,Develop natural language solutions in Azure - Training,Custom named entity recognition,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/custom-name-entity-recognition/7-summary
96,Develop natural language solutions in Azure - Training,Translate text with Azure AI Translator service,Introduction,"# Introduction

Completed 100 XP

  * 1 minute

There are many commonly used languages throughout the world, and the ability
to exchange information between speakers of different languages is often a
critical requirement for global solutions.

The Azure AI Translator provides an API for translating text between 90
supported languages.

After completing this module, you'll be able to:

  * Provision an Azure AI Translator resource
  * Understand language detection, translation, and transliteration
  * Specify translation options
  * Define and run custom translations

* * *"
97,Develop natural language solutions in Azure - Training,Translate text with Azure AI Translator service,Provision an Azure AI Translator resource,"# Provision an Azure AI Translator resource

Completed 100 XP

  * 2 minutes

**Azure AI Translator** provides a multilingual text translation API that you
can use for:

  * Language detection.
  * One-to-many translation.
  * Script transliteration (converting text from its native script to an alternative script).

![Diagram showing an Azure AI Translator resource performing language
detection, one-to-many translation, and transliteration.](../../wwl-data-
ai/translate-text-with-translator-service/media/translator-resource.png)

## Azure resource for Azure AI Translator

To use the Azure AI Translator service, you must provision a resource for it
in your Azure subscription. You can provision a single-service Azure AI
Translator resource, or you can use the Text Translation API in a multi-
service Azure AI Services resource.

After you provision a suitable resource in your Azure subscription, you can
use the **location** where you deployed the resource and one of its
**subscription keys** to call the Azure AI Translator APIs from your code. You
can call the APIs by submitting requests in JSON format to the REST interface,
or by using any of the available programming language-specific SDKs.

Note

The code examples in the subsequent units in this module show the JSON
requests and responses exchanged with the REST interface. When using an SDK,
the JSON requests are abstracted by appropriate objects and methods that
encapsulate the same data values. You'll get a chance to try the SDK for C# or
Python for yourself in the exercise later in the module.

* * *"
98,Develop natural language solutions in Azure - Training,Translate text with Azure AI Translator service,"Understand language detection, translation, and transliteration","# Understand language detection, translation, and transliteration

Completed 100 XP

  * 3 minutes

Let's explore the capabilities of **Azure AI Translator**. These capabilities
include:

## Language detection

You can use the **Detect** function of the REST API to detect the language in
which text is written.

For example, you could submit the following text to the
`https://api.cognitive.microsofttranslator.com/detect?api-version=3.0`
endpoint using curl.

Here's the text we want to translate:

JSON Copy

    
    
    { 'Text' : 'こんにちは' }
    

Here's a call using curl to the endpoint to detect the language of our text:

Bash Copy

    
    
    curl -X POST ""https://api.cognitive.microsofttranslator.com/detect?api-version=3.0"" -H ""Ocp-Apim-Subscription-Region: <your-service-region>"" -H ""Ocp-Apim-Subscription-Key: <your-key>"" -H ""Content-Type: application/json"" -d ""[{ 'Text' : 'こんにちは' }]
    

The response to this request looks as follows, indicating that the text is
written in Japanese:

JSON Copy

    
    
    [
      {
        ""language"": ""ja"",
        ""score"": 1.0,
        ""isTranslationSupported"": true,
        ""isTransliterationSupported"": true
        
        
       }
    ]
    

## Translation

To translate text from one language to another, use the **Translate**
function; specifying a single **from** parameter to indicate the source
language, and one or more **to** parameters to specify the languages into
which you want the text translated.

For example, you could submit the same JSON we previously used to detect the
language, specifying a **from** parameter of **ja** (Japanese) and two **to**
parameters with the values **en** (English) and **fr** (French). To do this,
you'd call:

Bash Copy

    
    
    curl -X POST ""https://api.cognitive.microsofttranslator.com/translate?api-version=3.0&from=ja&to=fr&to=en"" -H ""Ocp-Apim-Subscription-Key: <your-key>"" -H ""Ocp-Apim-Subscription-Region: <your-service-region>"" -H ""Content-Type: application/json; charset=UTF-8"" -d ""[{ 'Text' : 'こんにちは' }]""
    

This would produce the following result:

JSON Copy

    
    
    [
      {""translations"": 
        [
          {""text"": ""Hello"", ""to"": ""en""},   
          {""text"": ""Bonjour"", ""to"": ""fr""}
        ]
      }
    ]
    

## Transliteration

Our Japanese text is written using Hiragana script, so rather than translate
it to a different language, you may want to transliterate it to a different
script - for example to render the text in Latin script (as used by English
language text).

To accomplish this, we can submit the Japanese text to the **Transliterate**
function with a **fromScript** parameter of **Jpan** and a **toScript**
parameter of **Latn** :

Bash Copy

    
    
    curl -X POST ""https://api.cognitive.microsofttranslator.com/transliterate?api-version=3.0&fromScript=Jpan&toScript=Latn"" -H ""Ocp-Apim-Subscription-Key: <your-key>"" -H ""Ocp-Apim-Subscription-Region: <your-service-region>"" -H ""Content-Type: application/json"" -d ""[{ 'Text' : 'こんにちは' }]""
    

The response would give you the following result:

JSON Copy

    
    
    [
        {
            ""script"": ""Latn"",
            ""text"": ""Kon'nichiwa""
        }
    ]
    

* * *"
99,Develop natural language solutions in Azure - Training,Translate text with Azure AI Translator service,Specify translation options,"# Specify translation options

Completed 100 XP

  * 4 minutes

The **Translate** function of the API supports numerous parameters that affect
the output.

## Word alignment

In written English (using Latin script), spaces are used to separate words.
However, in some other languages (and more specifically, scripts) this is not
always the case.

For example, translating ""Smart Services"" from **en** (English) to **zh**
(Simplified Chinese) produces the result ""智能服务"", and it's difficult to
understand the relationship between the characters in the source text and the
corresponding characters in the translation. To resolve this problem, you can
specify the **includeAlignment** parameter with a value of **true** in your
call to produce the following result:

JSON Copy

    
    
    [
       {
          ""translations"":[
             {
                ""text"":""智能服务"",
                ""to"":""zh-Hans"",
                ""alignment"":{
                   ""proj"":""0:4-0:1 6:13-2:3""
                }
             }
          ]
       }
    ]
    

These results tell us that characters 0 to 4 in the source correspond to
characters 0 to 1 in the translation, while characters 6 to 13 in the source
correspond to characters 2 to 3 in the translation.

## Sentence length

Sometimes it might be useful to know the length of a translation, for example
to determine how best to display it in a user interface. You can get this
information by setting the **includeSentenceLength** parameter to **true**.

For example, specifying this parameter when translating the English ( **en** )
text ""Hello world"" to French ( **fr** ) produces the following results:

JSON Copy

    
    
    [
       {
          ""translations"":[
             {
                ""text"":""Salut tout le monde"",
                ""to"":""fr"",
                ""sentLen"":{""srcSentLen"":[12],""transSentLen"":[20]}
             }
          ]
       }
    ]
    

## Profanity filtering

Sometimes text contains profanities, which you might want to obscure or omit
altogether in a translation. You can handle profanities by specifying the
**profanityAction** parameter, which can have one of the following values:

  * **NoAction** : Profanities are translated along with the rest of the text.
  * **Deleted** : Profanities are omitted in the translation.
  * **Marked** : Profanities are indicated using the technique indicated in the **profanityMarker** parameter (if supplied). The default value for this parameter is **Asterisk** , which replaces characters in profanities with ""*"". As an alternative, you can specify a **profanityMarker** value of **Tag** , which causes profanities to be enclosed in XML tags.

For example, translating the English ( **en** ) text ""JSON is ▇▇▇▇ great!""
(where the blocked out word is a profanity) to German ( **de** ) with a
**profanityAction** of **Marked** and a **profanityMarker** of **Asterisk**
produces the following result:

JSON Copy

    
    
    [
       {
          ""translations"":[
             {
                ""text"":""JSON ist *** erstaunlich."",
                ""to"":""de""
             }
          ]
       }
    ]
    

Note

To learn more about the translation options, including some not described
here, see the [Azure AI Translator API documentation](/en-us/azure/ai-
services/translator/reference/v3-0-translate).

* * *"
100,Develop natural language solutions in Azure - Training,Translate text with Azure AI Translator service,Define custom translations,"# Define custom translations

Completed 100 XP

  * 5 minutes

While the default translation model used by Azure AI Translator is effective
for general translation, you may need to develop a translation solution for
businesses or industries in that have specific vocabularies of terms that
require custom translation.

To solve this problem, you can create a custom model that maps your own sets
of source and target terms for translation. To create a custom model, use the
Custom Translator portal to:

  1. [Create a workspace](/en-us/azure/ai-services/translator/custom-translator/quickstart) linked to your Azure AI Translator resource.
  2. [Create a project](/en-us/azure/ai-services/translator/custom-translator/quickstart).
  3. [Upload training data files](/en-us/azure/ai-services/translator/custom-translator/quickstart) and [train a model](/en-us/azure/ai-services/translator/custom-translator/quickstart).
  4. [Test your model](/en-us/azure/ai-services/translator/custom-translator/quickstart) and [publish your model](/en-us/azure/ai-services/translator/custom-translator/quickstart).
  5. Make translation calls to the API.

[ ![Screenshot showing the Custom Translator portal.](../../wwl-data-
ai/translate-text-with-translator-service/media/custom-translator-new-
small.png) ](../../wwl-data-ai/translate-text-with-translator-
service/media/custom-translator-new.png#lightbox)

Your custom model is assigned a unique **category Id** (highlighted in the
screenshot), which you can specify in **translate** calls to your Azure AI
Translator resource by using the **category** parameter, causing translation
to be performed by your custom model instead of the default model.

## How to call the API

To initiate a translation, you send a **POST** request to the following
request URL:

HTTP Copy

    
    
    https://api.cognitive.microsofttranslator.com/translate?api-version=3.0
    

Your request needs to include a couple of parameters:

  * `api-version`: The required version of the API.
  * `to`: The target language to translate to. For example: `to=fr` for French.
  * `category`: Your **category Id**.

Your request must also include a number of required headers:

  * `Ocp-Apim-Subscription-Key`. Header for your client key. For example: `Ocp-Apim-Subscription-Key=<your-client-key>`.
  * `Content-Type`. The content type of the payload. The required format is: `Content-Type: application/json; charset=UTF-8`.

The request body should contain an array that includes a JSON object with a
`Text` property that specifies the text that you want to translate:

JSON Copy

    
    
    [
        {""Text"":""Where can I find my employee details?""}
    ]
    

There are different ways you can send your request to the API, including using
the C#, Python, and curl. For instance, to make a quick call, you can send a
POST request using curl:

Bash Copy

    
    
    curl -X POST ""https://api.cognitive.microsofttranslator.com/translate?api-version=3.0&from=en&to=nl&category=<category-id>"" -H ""Ocp-Apim-Subscription-Key: <your-key"" -H ""Content-Type: application/json; charset=UTF-8"" -d ""[{'Text':'Where can I find my employee details?'}]""
    

The request above makes a call to translate a sentence from English to Dutch.

### Response returned

The response returns a response code of `200` if the request was successful.
It also returns a response body that contains the translated text, like this:

JSON Copy

    
    
    [
        {
            ""translations"":[
                {""text"":""Waar vind ik mijn personeelsgegevens?"",""to"":""nl""}
            ]
        }
    ]
    

If the request wasn't successful, then a number of different status codes may
be returned depending on the error type, such as `400` (missing or invalid
query parameters). See [Response status codes](/en-us/azure/ai-
services/translator/reference/v3-0-translate?tabs=curl) for a full list of
codes and their explanation.

Note

For more information about custom translation, see [Quickstart: Build,
publish, and translate with custom models](/en-us/azure/ai-
services/translator/custom-translator/quickstart).

* * *"
101,Develop natural language solutions in Azure - Training,Translate text with Azure AI Translator service,Exercise - Translate text with the Azure AI Translator service,"# Exercise - Translate text with the Azure AI Translator service

Completed 100 XP

  * 30 minutes

In this exercise, you build an app that translates text between languages.

Note

To complete this lab, you need an **[Azure
subscription](https://azure.microsoft.com/free)**.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/translate-text-with-
translator-service/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2322068)

Tip

After completing the exercise, if you've finished exploring Azure AI services,
delete the Azure resources that you created during the exercise.

* * *"
102,Develop natural language solutions in Azure - Training,Translate text with Azure AI Translator service,Module assessment,"# Module assessment

Completed 200 XP

  * 2 minutes

1.

What function of Azure AI Translator should you use to convert the Chinese
word ""你好"" to the English word ""Hello""?

Detect

Translate

Transliterate

2.

What function of Azure AI Translator should you use to convert the Russian
word ""спасибо"" in Cyrillic characters to ""spasibo"" in Latin characters?

Detect

Translate

Transliterate

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
103,Develop natural language solutions in Azure - Training,Translate text with Azure AI Translator service,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/translate-text-with-translator-service/8-summary
104,Develop natural language solutions in Azure - Training,Create speech-enabled apps with Azure AI services,Introduction,"# Introduction

Completed 100 XP

  * 2 minutes

Azure AI Speech provides APIs that you can use to build speech-enabled
applications. This includes:

  * **Speech to text** : An API that enables _speech recognition_ in which your application can accept spoken input.
  * **Text to speech** : An API that enables _speech synthesis_ in which your application can provide spoken output.
  * **Speech Translation** : An API that you can use to translate spoken input into multiple languages.
  * **Keyword Recognition** : An API that enables your application to recognize keywords or short phrases.
  * **Intent Recognition** : An API that uses conversational language understanding to determine the semantic meaning of spoken input.

This module focuses on speech recognition and speech synthesis, which are core
capabilities of any speech-enabled application.

Note

The code examples in this module are provided in Python, but you can use any
of the available Azure AI Speech SDK packages to develop speech-enabled
applications in your preferred language. Available SDK packages include:

  * [azure-cognitiveservices-speech for Python](https://pypi.org/project/azure-cognitiveservices-speech)
  * [Microsoft.CognitiveServices.Speech for Microsoft .NET](https://www.nuget.org/packages/Microsoft.CognitiveServices.Speech)
  * [microsoft-cognitiveservices-speech-sdk for JavaScript](https://www.npmjs.com/package/microsoft-cognitiveservices-speech-sdk)
  * [Microsoft Cognitive Services Speech SDK For Java](https://mvnrepository.com/artifact/com.microsoft.cognitiveservices.speech/client-sdk)

* * *"
105,Develop natural language solutions in Azure - Training,Create speech-enabled apps with Azure AI services,Provision an Azure resource for speech,"# Provision an Azure resource for speech

Completed 100 XP

  * 2 minutes

Before you can use Azure AI Speech, you need to create an Azure AI Speech
resource in your Azure subscription. You can use either a dedicated Azure AI
Speech resource or a multi-service Azure AI Services or Azure AI Foundry
resource.

After you create your resource, you'll need the following information to use
it from a client application through one of the supported SDKs:

  * The _location_ in which the resource is deployed (for example, _eastus_ )
  * One of the _keys_ assigned to your resource.

You can view of these values on the **Keys and Endpoint** page for your
resource in the Azure portal.

While the specific syntax and parameters can vary between language-specific
SDKs, most interactions with the Azure AI Speech service start with the
creation of a **SpeechConfig** object that encapsulates the connection to your
Azure AI Speech resource.

For example, the following Python code instantiates a SpeechConfig object
based on an Azure AI Speech resource in the East US region:

Python Copy

    
    
    import azure.cognitiveservices.speech as speech_sdk
    
    speech_config = speech_sdk.SpeechConfig(your_project_key, 'eastus')
    

Note

This example assumes that the Speech SDK package for python has been
installed, like this:

`pip install azure-cognitiveservices-speech`

* * *"
106,Develop natural language solutions in Azure - Training,Create speech-enabled apps with Azure AI services,Use the Azure AI Speech to Text API,"# Use the Azure AI Speech to Text API

Completed 100 XP

  * 5 minutes

The Azure AI Speech service supports speech recognition through the following
features:

  * **Real-time transcription** : Instant transcription with intermediate results for live audio inputs.
  * **Fast transcription** : Fastest synchronous output for situations with predictable latency.
  * **Batch transcription** : Efficient processing for large volumes of prerecorded audio.
  * **Custom speech** : Models with enhanced accuracy for specific domains and conditions.

## Using the Azure AI Speech SDK

While the specific details vary, depending on the SDK being used (Python, C#,
and so on); there's a consistent pattern for using the **Speech to text** API:

![A diagram showing how a SpeechRecognizer object is created from a
SpeechConfig and AudioConfig, and its RecognizeOnceAsync method is used to
call the Speech API.](../../wwl-data-ai/create-speech-enabled-
apps/media/speech-to-text.png)

  1. Use a **SpeechConfig** object to encapsulate the information required to connect to your Azure AI Speech resource. Specifically, its **location** and **key**.
  2. Optionally, use an **AudioConfig** to define the input source for the audio to be transcribed. By default, this is the default system microphone, but you can also specify an audio file.
  3. Use the **SpeechConfig** and **AudioConfig** to create a **SpeechRecognizer** object. This object is a proxy client for the **Speech to text** API.
  4. Use the methods of the **SpeechRecognizer** object to call the underlying API functions. For example, the **RecognizeOnceAsync()** method uses the Azure AI Speech service to asynchronously transcribe a single spoken utterance.
  5. Process the response from the Azure AI Speech service. In the case of the **RecognizeOnceAsync()** method, the result is a **SpeechRecognitionResult** object that includes the following properties: 
    * Duration
    * OffsetInTicks
    * Properties
    * Reason
    * ResultId
    * Text

If the operation was successful, the **Reason** property has the enumerated
value **RecognizedSpeech** , and the **Text** property contains the
transcription. Other possible values for **Result** include **NoMatch**
(indicating that the audio was successfully parsed but no speech was
recognized) or **Canceled** , indicating that an error occurred (in which
case, you can check the **Properties** collection for the
**CancellationReason** property to determine what went wrong).

* * *"
107,Develop natural language solutions in Azure - Training,Create speech-enabled apps with Azure AI services,Use the text to speech API,"# Use the text to speech API

Completed 100 XP

  * 4 minutes

Similarly to its **Speech to text** APIs, the Azure AI Speech service offers
other REST APIs for speech synthesis:

  * The **Text to speech** API, which is the primary way to perform speech synthesis.
  * The **Batch synthesis** API, which is designed to support batch operations that convert large volumes of text to audio - for example to generate an audio-book from the source text.

You can learn more about the REST APIs in the [Text to speech REST API
documentation](/en-us/azure/ai-services/speech-service/batch-synthesis). In
practice, most interactive speech-enabled applications use the Azure AI Speech
service through a (programming) language-specific SDK.

## Using the Azure AI Speech SDK

As with speech recognition, in practice most interactive speech-enabled
applications are built using the Azure AI Speech SDK.

The pattern for implementing speech synthesis is similar to that of speech
recognition:

![A diagram showing how a SpeechSynthesizer object is created from a
SpeechConfig and AudioConfig, and its SpeakTextAsync method is used to call
the Speech API.](../../wwl-data-ai/create-speech-enabled-apps/media/text-to-
speech.png)

  1. Use a **SpeechConfig** object to encapsulate the information required to connect to your Azure AI Speech resource. Specifically, its **location** and **key**.
  2. Optionally, use an **AudioConfig** to define the output device for the speech to be synthesized. By default, this is the default system speaker, but you can also specify an audio file, or by explicitly setting this value to a null value, you can process the audio stream object that is returned directly.
  3. Use the **SpeechConfig** and **AudioConfig** to create a **SpeechSynthesizer** object. This object is a proxy client for the **Text to speech** API.
  4. Use the methods of the **SpeechSynthesizer** object to call the underlying API functions. For example, the **SpeakTextAsync()** method uses the Azure AI Speech service to convert text to spoken audio.
  5. Process the response from the Azure AI Speech service. In the case of the **SpeakTextAsync** method, the result is a **SpeechSynthesisResult** object that contains the following properties: 
    * AudioData
    * Properties
    * Reason
    * ResultId

When speech has been successfully synthesized, the **Reason** property is set
to the **SynthesizingAudioCompleted** enumeration and the **AudioData**
property contains the audio stream (which, depending on the **AudioConfig**
may have been automatically sent to a speaker or file).

* * *"
108,Develop natural language solutions in Azure - Training,Create speech-enabled apps with Azure AI services,Configure audio format and voices,"# Configure audio format and voices

Completed 100 XP

  * 3 minutes

When synthesizing speech, you can use a **SpeechConfig** object to customize
the audio that is returned by the Azure AI Speech service.

## Audio format

The Azure AI Speech service supports multiple output formats for the audio
stream that is generated by speech synthesis. Depending on your specific
needs, you can choose a format based on the required:

  * Audio file type
  * Sample-rate
  * Bit-depth

For example, the following Python code sets the speech output format for a
previously defined **SpeechConfig** object named _speech_config_ :

Python Copy

    
    
    speech_config.set_speech_synthesis_output_format(SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm)
    

For a full list of supported formats and their enumeration values, see the
[Azure AI Speech SDK documentation](/en-us/python/api/azure-cognitiveservices-
speech/azure.cognitiveservices.speech.speechsynthesisoutputformat).

## Voices

The Azure AI Speech service provides multiple voices that you can use to
personalize your speech-enabled applications. Voices are identified by names
that indicate a locale and a person's name - for example `en-GB-George`.

The following Python example code sets the voice to be used

Python Copy

    
    
    speech_config.speech_synthesis_voice_name = ""en-GB-George""
    

For information about voices, see the [Azure AI Speech SDK documentation](/en-
us/azure/ai-services/speech-service/language-support?tabs=tts).

* * *"
109,Develop natural language solutions in Azure - Training,Create speech-enabled apps with Azure AI services,Use Speech Synthesis Markup Language,"# Use Speech Synthesis Markup Language

Completed 100 XP

  * 3 minutes

While the Azure AI Speech SDK enables you to submit plain text to be
synthesized into speech, the service also supports an XML-based syntax for
describing characteristics of the speech you want to generate. This **Speech
Synthesis Markup Language** (SSML) syntax offers greater control over how the
spoken output sounds, enabling you to:

  * Specify a speaking style, such as ""excited"" or ""cheerful"" when using a neural voice.
  * Insert pauses or silence.
  * Specify _phonemes_ (phonetic pronunciations), for example to pronounce the text ""SQL"" as ""sequel"".
  * Adjust the _prosody_ of the voice (affecting the pitch, timbre, and speaking rate).
  * Use common ""say-as"" rules, for example to specify that a given string should be expressed as a date, time, telephone number, or other form.
  * Insert recorded speech or audio, for example to include a standard recorded message or simulate background noise.

For example, consider the following SSML:

XML Copy

    
    
    <speak version=""1.0"" xmlns=""http://www.w3.org/2001/10/synthesis"" 
                         xmlns:mstts=""https://www.w3.org/2001/mstts"" xml:lang=""en-US""> 
        <voice name=""en-US-AriaNeural""> 
            <mstts:express-as style=""cheerful""> 
              I say tomato 
            </mstts:express-as> 
        </voice> 
        <voice name=""en-US-GuyNeural""> 
            I say <phoneme alphabet=""sapi"" ph=""t ao m ae t ow""> tomato </phoneme>. 
            <break strength=""weak""/>Lets call the whole thing off! 
        </voice> 
    </speak>
    

This SSML specifies a spoken dialog between two different neural voices, like
this:

  * **Ariana** ( _cheerfully_ ): ""I say tomato:
  * **Guy** : ""I say tomato (pronounced _tom-ah-toe_ ) ... Let's call the whole thing off!""

To submit an SSML description to the Speech service, you can use an
appropriate method of a **SpeechSynthesizer** object, like this:

Python Copy

    
    
    speech_synthesizer.speak_ssml('<speak>...');
    

For more information about SSML, see the [Azure AI Speech SDK
documentation](/en-us/azure/ai-services/speech-service/speech-synthesis-
markup).

* * *"
110,Develop natural language solutions in Azure - Training,Create speech-enabled apps with Azure AI services,Exercise - Create a speech-enabled app,"# Exercise - Create a speech-enabled app

Completed 100 XP

  * 30 minutes

In this exercise, build a speech enabled app for both speech recognition and
synthesis.

Note

To complete this lab, you need an **[Azure
subscription](https://azure.microsoft.com/free)**.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/create-speech-enabled-
apps/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2322214)

Tip

After completing the exercise, if you've finished exploring Azure AI services,
delete the Azure resources that you created during the exercise.

* * *"
111,Develop natural language solutions in Azure - Training,Create speech-enabled apps with Azure AI services,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

1.

What information do you need from your Azure AI Speech service resource to
consume it using the Azure AI Speech SDK?

The location and one of the keys

The primary and secondary keys

The endpoint and one of the keys

2.

Which object should you use to specify that the speech input to be transcribed
to text is in an audio file?

SpeechConfig

AudioConfig

SpeechRecognizer

3.

How can you change the voice used in speech synthesis?

Specify a SpeechSynthesisOutputFormat enumeration in the SpeechConfig object.

Set the SpeechSynthesisVoiceName property of the SpeechConfig object to the
desired voice name.

Specify a filename in the AudioConfig object.

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
112,Develop natural language solutions in Azure - Training,Create speech-enabled apps with Azure AI services,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/create-speech-enabled-apps/9-summary
113,Develop natural language solutions in Azure - Training,Translate speech with the Azure AI Speech service,Introduction,"# Introduction

Completed 100 XP

  * 2 minutes

Translation of speech builds on speech recognition by recognizing and
transcribing spoken input in a specified language, and returning translations
of the transcription in one or more other languages.

In this module, you'll learn how to:

  * Provision Azure resources for speech translation.
  * Generate text translation from speech.
  * Synthesize spoken translations.

The units in the module include important conceptual information about Azure
AI Speech and how to use its API through one of the supported software
development kits (SDKs), after which you're able to try Azure AI Speech for
yourself in a hands-on exercise. To complete the hands-on exercise, you'll
need a Microsoft Azure subscription. If you don't already have one, you can
sign up for a free trial at <https://azure.com/free>

* * *"
114,Develop natural language solutions in Azure - Training,Translate speech with the Azure AI Speech service,Provision an Azure resource for speech translation,"# Provision an Azure resource for speech translation

Completed 100 XP

  * 3 minutes

The Azure AI Speech service provides robust, machine learning and artificial
intelligence-based speech translation services, enabling developers to add
end-to-end, real-time, speech translations to their applications or services.
You can use either a dedicated Azure AI Speech resource or a multi-service
Azure AI Services resource.

Before you can use the service, you need to create an Azure AI Speech resource
in your Azure subscription.

After creating your Azure resource, you'll need the following information to
use it from a client application through one of the supported SDKs:

  * The **location** in which the resource is deployed (for example, eastus)
  * One of the **keys** assigned to your resource.

You can view of these values on the **Keys and Endpoint** page for your
resource in the Azure portal.

* * *"
115,Develop natural language solutions in Azure - Training,Translate speech with the Azure AI Speech service,Translate speech to text,"# Translate speech to text

Completed 100 XP

  * 3 minutes

The pattern for speech translation using the Azure AI Speech SDK is similar to
speech recognition, with the addition of information about the source and
target languages for translation:

[ ![A TranslationRecognizer object is created from a SpeechConfig,
TranslationConfig, and AudioConfig; and its RecognizeOnceAsync method is used
to call the Speech API.](../../wwl-data-ai/translate-speech-speech-
service/media/translate-speech-small.png) ](../../wwl-data-ai/translate-
speech-speech-service/media/translate-speech.png#lightbox)

  1. Use a **SpeechTranslationConfig** object to encapsulate the information required to connect to your Azure AI Speech resource. Specifically, its location and key.

  2. The **SpeechTranslationConfig** object is also used to specify the speech recognition language (the language in which the input speech is spoken) and the target languages into which it should be translated.

  3. Optionally, use an **AudioConfig** to define the input source for the audio to be transcribed. By default, this is the default system microphone, but you can also specify an audio file.

  4. Use the **SpeechTranslationConfig** , and **AudioConfig** to create a **TranslationRecognizer** object. This object is a proxy client for the Azure AI Speech translation API.

  5. Use the methods of the **TranslationRecognizer** object to call the underlying API functions. For example, the **RecognizeOnceAsync** () method uses the Azure AI Speech service to asynchronously translate a single spoken utterance.

  6. Process the response from Azure AI Speech. In the case of the **RecognizeOnceAsync** () method, the result is a **SpeechRecognitionResult** object that includes the following properties:

    * Duration
    * OffsetInTicks
    * Properties
    * Reason
    * ResultId
    * Text
    * Translations

If the operation was successful, the **Reason** property has the enumerated
value **RecognizedSpeech** , the **Text** property contains the transcription
in the original language. You can also access a **Translations** property
which contains a dictionary of the translations (using the two-character ISO
language code, such as ""en"" for English, as a key).

* * *"
116,Develop natural language solutions in Azure - Training,Translate speech with the Azure AI Speech service,Synthesize translations,"# Synthesize translations

Completed 100 XP

  * 3 minutes

The **TranslationRecognizer** returns translated transcriptions of spoken
input - essentially translating audible speech to text.

You can also synthesize the translation as speech to create speech-to-speech
translation solutions. There are two ways you can accomplish this.

## Event-based synthesis

When you want to perform 1:1 translation (translating from one source language
into a single target language), you can use event-based synthesis to capture
the translation as an audio stream. To do this, you need to:

Specify the desired voice for the translated speech in the
**TranslationConfig**. Create an event handler for the
**TranslationRecognizer** object's **Synthesizing** event. In the event
handler, use the **GetAudio** () method of the **Result** parameter to
retrieve the byte stream of translated audio. The specific code used to
implement an event handler varies depending on the programming language you're
using. See the [C#](/en-us/azure/ai-services/speech-service/get-started-
speech-translation?pivots=programming-language-csharp) and [Python](/en-
us/azure/ai-services/speech-service/get-started-speech-
translation?pivots=programming-language-python) examples in the Speech SDK
documentation.

## Manual synthesis

Manual synthesis is an alternative approach to event-based synthesis that
doesn't require you to implement an event handler. You can use manual
synthesis to generate audio translations for one or more target languages.

Manual synthesis of translations is essentially just the combination of two
separate operations in which you:

  1. Use a **TranslationRecognizer** to translate spoken input into text transcriptions in one or more target languages.
  2. Iterate through the **Translations** dictionary in the result of the translation operation, using a **SpeechSynthesizer** to synthesize an audio stream for each language.

* * *"
117,Develop natural language solutions in Azure - Training,Translate speech with the Azure AI Speech service,Exercise - Translate speech,"# Exercise - Translate speech

Completed 100 XP

  * 30 minutes

In this exercise, build an app that recognizes and translates speech into a
specific language.

Note

To complete this lab, you need an **[Azure
subscription](https://azure.microsoft.com/free)**.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/translate-speech-speech-
service/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2322069)

Tip

After completing the exercise, if you've finished exploring Azure AI services,
delete the Azure resources that you created during the exercise.

* * *"
118,Develop natural language solutions in Azure - Training,Translate speech with the Azure AI Speech service,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

1.

Which SDK object should you use to specify the language(s) into which you want
speech translated?

SpeechConfig

SpeechTranslationConfig

AudioConfig

2.

Which SDK object should you use as a proxy for the Translation API of Azure AI
Speech service?

TranslationRecognizer

SpeechRecognizer

SpeechSynthesizer

3.

When translating speech, in which cases can you use the Synthesizing event to
synthesize the translations and speech?

Only when translating to a single target language.

Only when translating to multiple target languages.

When translating to one or more target languages.

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
119,Develop natural language solutions in Azure - Training,Translate speech with the Azure AI Speech service,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/translate-speech-speech-service/7-summary
120,Develop natural language solutions in Azure - Training,Develop an audio-enabled generative AI application,Introduction,"# Introduction

Completed 100 XP

  * 1 minute

Generative AI models make it possible to build intelligent chat-based
applications that can understand and reason over input. Traditionally, text
input is the primary mode of interaction with AI models, but multimodal models
are increasingly becoming available. These models make it possible for chat
applications to respond to audio input as well as text.

In this module, we'll discuss audio-enabled generative AI and explore how you
can use Azure AI Foundry to create generative AI solutions that respond to
prompts that include a mix of text and audio data.

* * *"
121,Develop natural language solutions in Azure - Training,Develop an audio-enabled generative AI application,Deploy a multimodal model,"# Deploy a multimodal model

Completed 100 XP

  * 3 minutes

To handle prompts that include audio, you need to deploy a _multimodal_
generative AI model - in other words, a model that supports not only text-
based input, but audio-based input as well. Multimodal models available in
Azure AI Foundry include (among others):

  * Microsoft **Phi-4-multimodal-instruct**
  * OpenAI **gpt-4o**
  * OpenAI **gpt-4o-mini**

Tip

To learn more about available models in Azure AI Foundry, see the **[Model
catalog and collections in Azure AI Foundry portal](/en-us/azure/ai-
foundry/how-to/model-catalog-overview)** article in the Azure AI Foundry
documentation.

## Testing multimodal models with audio-based prompts

After deploying a multimodal model, you can test it in the chat playground in
Azure AI Foundry portal. Some models allow you to include audio attachments in
the playground, either by uploading a file or recording a message.

![Screenshot of the chat playground with an audio-based prompt.](../../wwl-
data-ai/develop-generative-ai-audio-apps/media/audio-prompt.png)

In the chat playground, you can upload a local audio file and add text to the
message to elicit a response from a multimodal model.

* * *"
122,Develop natural language solutions in Azure - Training,Develop an audio-enabled generative AI application,Develop an audio-based chat app,"# Develop an audio-based chat app

Completed 100 XP

  * 5 minutes

To develop a client app that engages in audio-based chats with a multimodal
model, you can use the same basic techniques used for text-based chats. You
require a connection to the endpoint where the model is deployed, and you use
that endpoint to submit prompts that consists of messages to the model and
process the responses.

The key difference is that prompts for an audio-based chat include multi-part
user messages that contain both a _text_ content item and an _audio_ content
item.

![Diagram of a multi-part prompt being submitted to a model.](../../wwl-data-
ai/develop-generative-ai-audio-apps/media/multi-part-prompt.png)

The JSON representation of a prompt that includes a multi-part user message
looks something like this:

JSON Copy

    
    
    { 
        ""messages"": [ 
            { ""role"": ""system"", ""content"": ""You are a helpful assistant."" }, 
            { ""role"": ""user"", ""content"": [  
                { 
                    ""type"": ""text"", 
                    ""text"": ""Transcribe this audio:"" 
                },
                { 
                    ""type"": ""audio_url"",
                    ""audio_url"": {
                        ""url"": ""https://.....""
                    }
                }
            ] } 
        ]
    } 
    

The audio content item can be:

  * A URL to an audio file in a web site.
  * Binary audio data

When using binary data to submit a local audio file, the **audio_url** content
takes the form of a base64 encoded value in a data URL format:

JSON Copy

    
    
    {
        ""type"": ""audio_url"",
        ""audio_url"": {
           ""url"": ""data:audio/mp3;base64,<binary_audio_data>""
        }
    }
    

Depending on the model type, and where you deployed it, you can use Microsoft
Azure AI Model Inference or OpenAI APIs to submit audio-based prompts. These
libraries also provide language-specific SDKs that abstract the underlying
REST APIs.

In the exercise that follows in this module, you can use the Python or .NET
SDK for the Azure AI Model Inference API and the OpenAI API to develop an
audio-enabled chat application.

* * *"
123,Develop natural language solutions in Azure - Training,Develop an audio-enabled generative AI application,Exercise - Develop an audio-enabled chat app,"# Exercise - Develop an audio-enabled chat app

Completed 100 XP

  * 30 minutes

If you have an Azure subscription, you can complete this exercise to develop
an audio-enabled chat app.

Note

If you don't have an Azure subscription, you can [sign up for an
account](https://azure.microsoft.com/free), which includes credits for the
first 30 days.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/develop-generative-ai-audio-
apps/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2320123)

* * *"
124,Develop natural language solutions in Azure - Training,Develop an audio-enabled generative AI application,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

1.

Which kind of model can you use to respond to audio input?

Only OpenAI GPT models

Embedding models

Multimodal models

2.

How can you submit a prompt that asks a model to analyze an audio file?

Submit one prompt with an audio-based message followed by another prompt with
a text-based message.

Submit a prompt that contains a multi-part user message, containing both text
content and audio content.

Submit the audio file as the system message and the instruction or question as
the user message.

3.

How can you include an audio in a message?

As a URL or as binary data

Only as a URL

Only as binary data

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
125,Develop natural language solutions in Azure - Training,Develop an audio-enabled generative AI application,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/develop-generative-ai-audio-apps/6-summary
126,Develop natural language solutions in Azure - Training,Develop an Azure AI Voice Live agent,Introduction,"# Introduction

Completed 100 XP

  * 3 minutes

Voice-enabled applications are transforming how we interact with technology,
and this module guides you through building a real-time, interactive voice
solutions using advanced APIs and tools. The Azure AI Voice live API is a
solution enabling low-latency, high-quality speech to speech interactions for
voice agents. The API is designed for developers seeking scalable and
efficient voice-driven experiences as it eliminates the need to manually
orchestrate multiple components.

After completing this module, you'll be able to:

  * Implement the Azure AI Voice Live API to enable real-time, bidirectional communication.
  * Set up and configure the agent session.
  * Develop and manage event handlers to create dynamic and interactive user experiences.
  * Build and deploy a Python-based web app with real-time voice interaction capabilities to Azure.

* * *"
127,Develop natural language solutions in Azure - Training,Develop an Azure AI Voice Live agent,Explore the Azure Voice Live API,"# Explore the Azure Voice Live API

Completed 100 XP

  * 5 minutes

The Voice live API enables developers to create voice-enabled applications
with real-time, bidirectional communication. This unit explores its
architecture, configuration, and implementation.

## Key features of the Voice Live API

The Voice live API provides real-time communication using WebSocket
connections. It supports advanced features such as speech recognition, text-
to-speech synthesis, avatar streaming, and audio processing.

  * JSON-formatted events manage conversations, audio streams, and responses.
  * Events are categorized into client events (sent from client to server) and server events (sent from server to client).

Key features include:

  * Real-time audio processing with support for multiple formats like PCM16 and G.711.
  * Advanced voice options, including OpenAI voices and Azure custom voices.
  * Avatar integration using WebRTC for video and animation.
  * Built-in noise reduction and echo cancellation.

Note

Voice Live API is optimized for Azure AI Foundry resources. We recommend using
Azure AI Foundry resources for full feature availability and best Azure AI
Foundry integration experience.

For a table of supported models and regions, visit the [Voice Live API
overview](/en-us/azure/ai-services/speech-service/voice-live#supported-models-
and-regions).

## Connect to the Voice Live API

The Voice live API supports two authentication methods: Microsoft Entra
(keyless) and API key. Microsoft Entra uses token-based authentication for an
Azure AI Foundry resource. You apply a retrieved authentication token using a
`Bearer` token with the `Authorization` header.

For the recommended keyless authentication with Microsoft Entra ID, you need
to assign the **Cognitive Services User** role to your user account or a
managed identity. You generate a token using the Azure CLI or Azure SDKs. The
token must be generated with the `https://ai.azure.com/.default` scope, or the
legacy `https://cognitiveservices.azure.com/.default` scope. Use the token in
the `Authorization` header of the WebSocket connection request, with the
format `Bearer <token>`.

For key access, an API key can be provided in one of two ways. You can use an
`api-key` connection header on the prehandshake connection. This option isn't
available in a browser environment. Or, you can use an `api-key` query string
parameter on the request URI. Query string parameters are encrypted when using
https/wss.

Note

The `api-key` connection header on the prehandshake connection isn't available
in a browser environment.

### WebSocket endpoint

The endpoint to use varies depending on how you want to access your resources.
You can access resources through a connection to the AI Foundry project
(Agent), or through a connection to the model.

  * **Project connection:** The endpoint is `wss://<your-ai-foundry-resource-name>.services.ai.azure.com/voice-live/realtime?api-version=2025-10-01`
  * **Model connection:** The endpoint is `wss://<your-ai-foundry-resource-name>.cognitiveservices.azure.com/voice-live/realtime?api-version=2025-10-01`.

The endpoint is the same for all models. The only difference is the required
`model` query parameter, or, when using the Agent service, the `agent_id` and
`project_id` parameters.

## Voice Live API events

Client and server events facilitate communication and control within the Voice
live API. Key client events include:

  * `session.update`: Modify session configurations.
  * `input_audio_buffer.append`: Add audio data to the buffer.
  * `response.create`: Generate responses via model inference.

Server events provide feedback and status updates:

  * `session.updated`: Confirm session configuration changes.
  * `response.done`: Indicate response generation completion.
  * `conversation.item.created`: Notify when a new conversation item is added.

For a full list of client/server events, visit [Voice live API Reference](/en-
us/azure/ai-services/speech-service/voice-live-api-reference).

Note

Proper handling of events ensures seamless interaction between client and
server.

### Configure session settings for the Voice live API

Often, the first event sent by the caller on a newly established Voice live
API session is the `session.update` event. This event controls a wide set of
input and output behavior. Session settings can be updated dynamically using
the `session.update` event. Developers can configure voice types, modalities,
turn detection, and audio formats.

Example configuration:

JSON Copy

    
    
    {
      ""type"": ""session.update"",
      ""session"": {
        ""modalities"": [""text"", ""audio""],
        ""voice"": {
          ""type"": ""openai"",
          ""name"": ""alloy""
        },
        ""instructions"": ""You are a helpful assistant. Be concise and friendly."",
        ""input_audio_format"": ""pcm16"",
        ""output_audio_format"": ""pcm16"",
        ""input_audio_sampling_rate"": 24000,
        ""turn_detection"": {
          ""type"": ""azure_semantic_vad"",
          ""threshold"": 0.5,
          ""prefix_padding_ms"": 300,
          ""silence_duration_ms"": 500
        },
        ""temperature"": 0.8,
        ""max_response_output_tokens"": ""inf""
      }
    }
    

Tip

Use Azure semantic VAD for intelligent turn detection and improved
conversational flow.

### Implement real-time audio processing with the Voice live API

Real-time audio processing is a core feature of the Voice live API. Developers
can append, commit, and clear audio buffers using specific client events.

  * **Append audio:** Add audio bytes to the input buffer.
  * **Commit audio:** Process the audio buffer for transcription or response generation.
  * **Clear audio:** Remove audio data from the buffer.

Noise reduction and echo cancellation can be configured to enhance audio
quality. For example:

JSON Copy

    
    
    {
      ""type"": ""session.update"",
      ""session"": {
        ""input_audio_noise_reduction"": {
          ""type"": ""azure_deep_noise_suppression""
        },
        ""input_audio_echo_cancellation"": {
          ""type"": ""server_echo_cancellation""
        }
      }
    }
    

Note

Noise reduction improves VAD accuracy and model performance by filtering input
audio.

### Integrate avatar streaming using the Voice live API

The Voice live API supports WebRTC-based avatar streaming for interactive
applications. Developers can configure video, animation, and blendshape
settings.

  * Use the `session.avatar.connect` event to provide the client's SDP offer.
  * Configure video resolution, bitrate, and codec settings.
  * Define animation outputs such as blendshapes and visemes.

Example configuration:

JSON Copy

    
    
    {
      ""type"": ""session.avatar.connect"",
      ""client_sdp"": ""<client_sdp>""
    }
    

Tip

Use high-resolution video settings for enhanced visual quality in avatar
interactions.

* * *"
128,Develop natural language solutions in Azure - Training,Develop an Azure AI Voice Live agent,Explore the AI Voice Live client library for Python,"# Explore the AI Voice Live client library for Python

Completed 100 XP

  * 5 minutes

The Azure AI Voice Live client library for Python provides a real-time,
speech-to-speech client for Azure AI Voice Live API. It opens a WebSocket
session to stream microphone audio to the service and receives server events
for responsive conversations.

Important

As of version 1.0.0, this SDK is async-only. The synchronous API is deprecated
to focus exclusively on async patterns. All examples and samples use
async/await syntax.

In this unit, you learn how to use the SDK to implement authentication and
handle events. You also see a minimal example of creating a session. For a
full reference to the Voice Live package, visit the [voice live Package
reference](/en-us/python/api/azure-ai-voicelive/azure.ai.voicelive?view=azure-
python).

## Implement authentication

You can implement authentication with an API key or a Microsoft Entra ID
token. The following code sample shows an API key implementation. It assumes
environment variables are set in a `.env` file, or directly in your
environment.

Python Copy

    
    
    import asyncio
    from azure.core.credentials import AzureKeyCredential
    from azure.ai.voicelive import connect
    
    async def main():
        async with connect(
            endpoint=""your-endpoint"",
            credential=AzureKeyCredential(""your-api-key""),
            model=""gpt-4o""
        ) as connection:
            # Your async code here
            pass
    
    asyncio.run(main())
    

For production applications, Microsoft Entra authentication is recommended.
The following code sample shows implementing the `DefaultAzureCredential` for
authentication:

Python Copy

    
    
    import asyncio
    from azure.identity.aio import DefaultAzureCredential
    from azure.ai.voicelive import connect
    
    async def main():
        credential = DefaultAzureCredential()
        
        async with connect(
            endpoint=""your-endpoint"",
            credential=credential,
            model=""gpt-4o""
        ) as connection:
            # Your async code here
            pass
    
    asyncio.run(main())
    

## Handling events

Proper handling of events ensures a more seamless interaction between the
client and agent. For example, when handling a user interrupting the voice
agent you need to cancel agent audio playback immediately in the client. If
you don't, the client continues to play the last agent response until the
interrupt is processed in the API - resulting in the agent ""talking over"" the
user.

The following code sample shows some basic event handling:

Python Copy

    
    
    async for event in connection:
        if event.type == ServerEventType.SESSION_UPDATED:
            print(f""Session ready: {event.session.id}"")
            # Start audio capture
            
        elif event.type == ServerEventType.INPUT_AUDIO_BUFFER_SPEECH_STARTED:
            print(""User started speaking"")
            # Stop playback and cancel any current response
            
        elif event.type == ServerEventType.RESPONSE_AUDIO_DELTA:
            # Play the audio chunk
            audio_bytes = event.delta
            
        elif event.type == ServerEventType.ERROR:
            print(f""Error: {event.error.message}"")
    

## Minimal example

The following code sample shows authenticating to the API and configuring the
session.

Python Copy

    
    
    import asyncio
    from azure.core.credentials import AzureKeyCredential
    from azure.ai.voicelive.aio import connect
    from azure.ai.voicelive.models import (
        RequestSession, Modality, InputAudioFormat, OutputAudioFormat, ServerVad, ServerEventType
    )
    
    API_KEY = ""your-api-key""
    ENDPOINT = ""your-endpoint""
    MODEL = ""gpt-4o""
    
    async def main():
        async with connect(
            endpoint=ENDPOINT,
            credential=AzureKeyCredential(API_KEY),
            model=MODEL,
        ) as conn:
            session = RequestSession(
                modalities=[Modality.TEXT, Modality.AUDIO],
                instructions=""You are a helpful assistant."",
                input_audio_format=InputAudioFormat.PCM16,
                output_audio_format=OutputAudioFormat.PCM16,
                turn_detection=ServerVad(
                    threshold=0.5, 
                    prefix_padding_ms=300, 
                    silence_duration_ms=500
                ),
            )
            await conn.session.update(session=session)
    
            # Process events
            async for evt in conn:
                print(f""Event: {evt.type}"")
                if evt.type == ServerEventType.RESPONSE_DONE:
                    break
    
    asyncio.run(main())
    

* * *"
129,Develop natural language solutions in Azure - Training,Develop an Azure AI Voice Live agent,Exercise - Develop an Azure AI Voice Live agent,"# Exercise - Develop an Azure AI Voice Live agent

Completed 100 XP

  * 30 minutes

In this exercise, you complete a Flask-based Python web app based that enables
real-time voice interactions with an agent. You add the code to initialize the
session, and handle session events. You use a deployment script that: deploys
the AI model; creates an image of the app in Azure Container Registry (ACR)
using ACR tasks; and then creates an Azure App Service instance that pulls the
image. To test the app, you need an audio device with microphone and speaker
capabilities.

While this exercise is based on Python, you can develop similar applications
other language-specific SDKs; including:

  * [Azure VoiceLive client library for .NET](https://www.nuget.org/packages/Azure.AI.VoiceLive/)

Tasks performed in this exercise:

  * Download the base files for the app
  * Add code to complete the web app
  * Review the overall code base
  * Update and run the deployment script
  * View and test the application

This exercise takes approximately **30** minutes to complete.

## Before you start

To complete the exercise, you need:

  * An Azure subscription. If you don't already have one, you can sign up for one <https://azure.microsoft.com/>.
  * An audio device with microphone and speaker capabilities.

## Get started

Select the **Launch Exercise** button to open the exercise instructions in a
new browser window. When you're finished with the exercise, return here to:

  * Complete the module
  * Earn a badge for completing this module

  
[ ![Button to launch exercise.](../../wwl-data-ai/develop-voice-live-
agent/media/launch-exercise.png)
](https://go.microsoft.com/fwlink/?linkid=2338402)

* * *"
130,Develop natural language solutions in Azure - Training,Develop an Azure AI Voice Live agent,Module assessment,"# Module assessment

Completed 200 XP

  * 5 minutes

1.

What are the two authentication methods supported by the Voice Live API?

OAuth 2.0 and JWT (JSON Web Tokens)

Basic authentication and API keys

Microsoft Entra (keyless) and API key

2.

Which scope is required when generating a token for Microsoft Entra
authentication?

`https://cognitiveservices.azure.com/.default`

`https://management.azure.com/.default`

`https://graph.microsoft.com/.default`

3.

Which protocol is used for avatar streaming integration in Voice Live API?

HTTP/2

WebRTC

gRPC

4.

Which event should be handled to stop audio playback when a user interrupts
the voice agent?

`ServerEventType.RESPONSE_AUDIO_DELTA`

`ServerEventType.INPUT_AUDIO_BUFFER_SPEECH_STARTED`

`ServerEventType.SESSION_UPDATED`

5.

What is the recommended authentication method for production applications
using the SDK?

API key authentication

Microsoft Entra authentication with DefaultAzureCredential

Basic username/password authentication

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
131,Develop natural language solutions in Azure - Training,Develop an Azure AI Voice Live agent,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/develop-voice-live-agent/6-summary
132,Develop computer vision solutions in Azure - Training,Analyze images,Introduction,"# Introduction

Completed 100 XP

  * 3 minutes

Computer Vision is a branch of artificial intelligence (AI) in which software
interprets visual input, often from images or video feeds. In Microsoft Azure,
you can use the **Azure AI Vision** service to implement multiple computer
vision scenarios, including:

  * Image analysis
  * Optical character recognition (OCR)
  * Face detection and analysis
  * Video analysis

In this module, we'll focus on _image analysis_ , and explore how to build
applications that use the Azure AI Vision service to analyze and extract and
infer insights from images.

![Diagram of the Azure AI Vision service performing image analysis
tasks.](../../wwl-data-ai/analyze-images/media/computer-vision.png)

As shown in this conceptual diagram, the Azure AI Vision service provides
services that you can use to analyze images and:

  1. Generate a _caption_ for an image based on its contents.
  2. Suggest appropriate _tags_ to associate with an image.
  3. Detect and locate common _objects_ in an image.
  4. Detect and locate _people_ in an image.

* * *"
133,Develop computer vision solutions in Azure - Training,Analyze images,Provision an Azure AI Vision resource,"# Provision an Azure AI Vision resource

Completed 100 XP

  * 3 minutes

To use Azure AI Vision image analysis services, you need to provision an Azure
AI Vision resource in your Azure subscription. You can choose from multiple
provisioning options:

![Diagram of AI Foundry containing AI services, which in turn contain AI
Vision.](../../wwl-data-ai/analyze-images/media/ai-vision-resources.png)

  1. Create an **Azure AI Foundry** _project_ and an associated _hub_. By default, an Azure AI Foundry hub includes an **Azure AI services** multi-service resource, which includes Azure AI Vision. Azure AI Foundry projects are recommended for development of AI solutions on Azure that combine generative AI, agents, and pre-built Azure AI services, or which involve collaborative development by a team of software engineers and service operators.
  2. If you don't need all of the functionality in an Azure AI Foundry hub, you can create an **Azure AI services** multi-service resource in your Azure subscription. You can then use this resource to access Azure AI Vision services and other AI services through a single endpoint and key.
  3. If you only need to use Azure AI Vision functionality, or you're just experimenting with the service, you can create a standalone **Computer Vision** resource in your Azure subscription. One benefit of this approach is that the standalone service provides a free tier that you can use to explore the service at no cost.

Tip

If you're unfamiliar with Azure AI Foundry and Azure AI services, consider
completing the **[Plan and prepare to develop AI solutions on Azure](/en-
us/training/modules/prepare-azure-ai-development/)** module.

## Connecting to your resource

After you've deployed your resource, you can use the [Azure AI Vision REST
API](https://learn.microsoft.com/en-us/rest/api/computervision/image-analysis)
or a language-specific SDK (such as the [Python
SDK](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-vision-
imageanalysis-readme) or [Microsoft .NET SDK](https://learn.microsoft.com/en-
us/dotnet/api/overview/azure/ai.vision.imageanalysis-readme)) to connect to it
from a client application.

Every Azure AI Vision resource provides an _endpoint_ to which client
applications must connect. You can find the endpoint for your resource in the
Azure portal, or if you're working in an Azure AI Foundry project, in the
Azure AI Foundry portal. The endpoint is in the form of a URL, and typically
looks something like this:

Copy

    
    
    https://<resource_name>.cognitiveservices.azure.com/
    

To connect to the endpoint, client applications must be authenticated. Options
for authentication include:

  * **Key-based authentication** : Client applications are authenticated by passing an authorization key (which you can find and manage in the portal).
  * **Microsoft Entra ID authentication** : Client applications are authenticated by using a Microsoft Entra ID token for credentials that have permission to access the Azure AI Vision resource in Azure.

When developing and testing an application, it's common to use key-based
authentication or Microsoft Entra ID authentication based on your own Azure
credentials. In production, consider using Microsoft Entra ID authentication
based on a managed identity for your Azure application or use Azure Key Vault
to store authorization keys securely.

Note

When using an Azure AI services resource in an Azure AI Foundry project, you
can use the Azure AI Foundry SDK to connect to the project using Microsoft
Entra ID authentication, and then retrieve the connection information for your
Azure AI services resource, including the authorization key, from the project.

* * *"
134,Develop computer vision solutions in Azure - Training,Analyze images,Analyze an image,"# Analyze an image

Completed 100 XP

  * 3 minutes

Choose your development language

C# Python

After connecting to your Azure AI Vision resource endpoint, your client
application can use the service to perform image analysis tasks.

Note the following requirements for image analysis:

  * The image must be presented in JPEG, PNG, GIF, or BMP format.
  * The file size of the image must be less than 4 megabytes (MB).
  * The dimensions of the image must be greater than 50 x 50 pixels.

## Submitting an image for analysis

To analyze an image, you can use the **[Analyze
Image](https://learn.microsoft.com/en-us/rest/api/computervision/image-
analysis/analyze-image)** REST method or the equivalent method in the SDK for
your preferred programming language, specifying the visual features you want
to include in the analysis.

Python Copy

    
    
    from azure.ai.vision.imageanalysis import ImageAnalysisClient
    from azure.ai.vision.imageanalysis.models import VisualFeatures
    from azure.core.credentials import AzureKeyCredential
    
    client = ImageAnalysisClient(
        endpoint=""<YOUR_RESOURCE_ENDPOINT>"",
        credential=AzureKeyCredential(""<YOUR_AUTHORIZATION_KEY>"")
    )
    
    result = client.analyze(
        image_data=<IMAGE_DATA_BYTES>, # Binary data from your image file
        visual_features=[VisualFeatures.CAPTION, VisualFeatures.TAGS],
        gender_neutral_caption=True,
    )
    

Note

In this code example, the client app uses key-based authentication. To use
Microsoft Entra ID authentication, you can use a **TokenCredential** instead
of an **AzureKeyCredential**.

The code example submits the image data as a binary object (which would
typically be read from an image file). You can also analyze an image based on
a URL by using the **analyze_from_url** method.

Available visual features are contained in the `VisualFeatures` enumeration:

  * VisualFeatures.TAGS: Identifies tags about the image, including objects, scenery, setting, and actions
  * VisualFeatures.OBJECTS: Returns the bounding box for each detected object
  * VisualFeatures.CAPTION: Generates a caption of the image in natural language
  * VisualFeatures.DENSE_CAPTIONS: Generates more detailed captions for the objects detected
  * VisualFeatures.PEOPLE: Returns the bounding box for detected people
  * VisualFeatures.SMART_CROPS: Returns the bounding box of the specified aspect ratio for the area of interest
  * VisualFeatures.READ: Extracts readable text

C# Copy

    
    
    using Azure.AI.Vision.ImageAnalysis;
    
    ImageAnalysisClient client = new ImageAnalysisClient(
        ""<YOUR_RESOURCE_ENDPOINT>"",
        new AzureKeyCredential(""<YOUR_AUTHORIZATION_KEY>""));
    
    ImageAnalysisResult result = client.Analyze(
        <IMAGE_DATA_BYTES>, // Binary data from your image file
        VisualFeatures.Caption | VisualFeatures.Tags,
        new ImageAnalysisOptions { GenderNeutralCaption = true });
    

Note

In this code example, the client app uses key-based authentication. To use
Microsoft Entra ID authentication, you can use a **TokenCredential** instead
of an **AzureKeyCredential**.

The code example submits the image data as a binary object (which would
typically be read from an image file). You can also analyze an image based on
a URL.

Available visual features are contained in the `VisualFeatures` enumeration:

  * VisualFeatures.Tags: Identifies tags about the image, including objects, scenery, setting, and actions
  * VisualFeatures.Objects: Returns the bounding box for each detected object
  * VisualFeatures.Caption: Generates a caption of the image in natural language
  * VisualFeatures.DenseCaptions: Generates more detailed captions for the objects detected
  * VisualFeatures.People: Returns the bounding box for detected people
  * VisualFeatures.SmartCrops: Returns the bounding box of the specified aspect ratio for the area of interest
  * VisualFeatures.Read: Extracts readable text

Specifying the visual features you want analyzed in the image determines what
information the response will contain. Most responses will contain a bounding
box (if a location in the image is reasonable) or a confidence score (for
features such as tags or captions).

## Processing the response

This method returns a JSON document containing the requested information. The
JSON response for image analysis looks similar to this example, depending on
your requested features:

JSON Copy

    
    
    {
      ""apim-request-id"": ""abcde-1234-5678-9012-f1g2h3i4j5k6"",
      ""modelVersion"": ""<version>"",
      ""denseCaptionsResult"": {
        ""values"": [
          {
            ""text"": ""a house in the woods"",
            ""confidence"": 0.7055229544639587,
            ""boundingBox"": {
              ""x"": 0,
              ""y"": 0,
              ""w"": 640,
              ""h"": 640
            }
          },
          {
            ""text"": ""a trailer with a door and windows"",
            ""confidence"": 0.6675070524215698,
            ""boundingBox"": {
              ""x"": 214,
              ""y"": 434,
              ""w"": 154,
              ""h"": 108
            }
          }
        ]
      },
      ""metadata"": {
        ""width"": 640,
        ""height"": 640
      }
    }
    

* * *"
135,Develop computer vision solutions in Azure - Training,Analyze images,Exercise - Analyze images,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/analyze-images/5-exercise-computer-vision
136,Develop computer vision solutions in Azure - Training,Analyze images,Module assessment,"# Module assessment

Completed 200 XP

  * 2 minutes

## Check your knowledge

1.

What is the purpose of Azure AI Vision Image Analysis?

To provide reporting and auditing of virtual machine templates in your Azure
subscriptions

To extract information about visual features in images

To support video conferencing and web-cam communication

2.

You want to use Azure AI Vision Image Analysis to generate suggested text
descriptions for an image. Which visual feature should you specify?

Tags

DenseCaptions

Read

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
137,Develop computer vision solutions in Azure - Training,Analyze images,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/analyze-images/7-summary
138,Develop computer vision solutions in Azure - Training,Read text in images,Introduction,"# Introduction

Completed 100 XP

  * 2 minutes

We live in a digital world, in which data is increasingly captured as images.
Often, those images contain text, which you need to be able to extract from
their pixelated format in the image for processing, indexing, and other tasks.
Everyday examples include:

  * Meeting a new business associate and taking a photograph of their business card to store their contact details digitally.
  * Scanning a document or ID card to include in an application for a government or commercial service.
  * Taking a photo of a menu or recipe to store it in a digital notebook.
  * Photographing street signs or store fronts so you can submit the text they contain to a translation app.
  * Digitizing handwritten notes using a cellphone camera.

![Diagram of an image containing text being read by the Azure AI Vision image
analysis service.](../../wwl-data-ai/read-text-images-documents-with-computer-
vision-service/media/optical-character-recognition.png)

In this module, we'll explore the _optical character recognition_ (OCR)
capabilities of the Azure AI Vision _Image Analysis_ API, which makes these
scenarios, and more, possible.

* * *"
139,Develop computer vision solutions in Azure - Training,Read text in images,Explore Azure AI options for reading text,"# Explore Azure AI options for reading text

Completed 100 XP

  * 3 minutes

There are multiple Azure AI services that read text from documents and images,
each optimized for results depending on the input and the specific
requirements of your application.

## Azure AI Vision

![Azure AI Vision icon](../../wwl-data-ai/read-text-images-documents-with-
computer-vision-service/media/ai-vision.png)

Azure AI Vision includes an _image analysis_ capability that supports _optical
character recognition_ (OCR). Consider using Azure AI Vision in the following
scenarios:

  * **Text location and extraction from scanned documents** : Azure AI Vision is a great solution for general, unstructured documents that have been scanned as images. For example, reading text in labels, menus, or business cards.
  * **Finding and reading text in photographs** : Examples include photo's that include street signs and store names.
  * **Digital asset management (DAM)** : Azure AI Vision includes functionality for analyzing images beyond extracting text; including object detection, describing or categorizing an image, generating smart-cropped thumbnails and more. These capabilities make it a useful service when you need to catalog, index, or analyze large volumes of digital image-based content.

## Azure AI Document Intelligence

![Azure AI Document Intelligence icon](../../wwl-data-ai/read-text-images-
documents-with-computer-vision-service/media/document-intelligence.png)

Azure AI Document Intelligence is a service that you can use to extract
information from complex digital documents. Azure AI Document Intelligence is
designed for extracting text, key-value pairs, tables, and structures from
documents automatically and accurately. Key considerations for choosing Azure
AI Document Intelligence include:

  * **Form processing** : Azure AI Document Intelligence is specifically designed to extract data from forms, invoices, receipts, and other structured documents.
  * **Prebuilt models** : Azure AI Document Intelligence provides prebuilt models for common document types to reduce complexity and integrate into workflows or applications.
  * **Custom models** : Creating custom models tailored to your specific documents, makes Azure AI Document Intelligence a flexible solution that can be used in many business scenarios.

## Azure AI Content Understanding

![Azure AI Content Understanding icon](../../wwl-data-ai/read-text-images-
documents-with-computer-vision-service/media/content-understanding.png)

Azure AI Content Understanding is a service that you can use to analyze and
extract information from multiple kinds of content; including documents,
images, audio streams, and video. It is suitable for:

  * **Multimodal content extraction** : Extracting content and structured fields from documents, forms, audio, video, and images.
  * **Custom content analysis scenarios** : Support for customizable analyzers enables you to extract specific content or fields tailored to business needs.

Note

In the rest of this module, we'll focus on the OCR image analysis feature in
**Azure AI Vision**. To learn more about Azure AI Document Intelligence and
Azure AI Content understanding, consider completing the following training
modules:

  * [Plan an Azure AI Document Intelligence solution](/en-us/training/modules/plan-form-recognizer-solution/)
  * [Analyze content with Azure AI Content Understanding](/en-us/training/modules/analyze-content-ai/)

* * *"
140,Develop computer vision solutions in Azure - Training,Read text in images,Read text with Azure AI Vision Image Analysis,"# Read text with Azure AI Vision Image Analysis

Completed 100 XP

  * 6 minutes

Choose your development language

C# Python

To use Azure AI Vision for image analysis, including optical character
recognition, you must provision an Azure AI Vision resource in an Azure
subscription. The resource can be:

  * An **Azure AI Services** multi-service resource (either deployed as part of an Azure AI Foundry hub and project, or as a standalone resource).
  * A **Computer Vision** resource.

To use your deployed resource in an application, you must connect to its
_endpoint_ using either key-based authentication or Microsoft Entra ID
authentication. You can find the endpoint for your resource in the Azure
portal, or if you're working in an Azure AI Foundry project, in the Azure AI
Foundry portal. The endpoint is in the form of a URL, and typically looks
something like this:

Copy

    
    
    https://<resource_name>.cognitiveservices.azure.com/
    

After establishing a connection, you can use the OCR feature by calling the
**ImageAnalysis** function (via the REST API or with an equivalent SDK
method), passing the image URL or binary data, and optionally specifying the
language the text is written in (with a default value of **en** for English).

rest Copy

    
    
    https://<endpoint>/computervision/imageanalysis:analyze?features=read&...
    

To use the Azure AI Vision Python SDK to extract text from an image, install
the **azure-ai-vision-imageanalysis** package. Then, in your code, use either
key-based authentication or Microsoft Entra ID authentication to connect an
**ImageAnalysisClient** object to an Azure AI Vision resource. To find and
read text in an image, call the **analyze** (or **analyze_from_url** ) method,
specifying the **VisualFeatures.READ** enumeration.

Python Copy

    
    
    from azure.ai.vision.imageanalysis import ImageAnalysisClient
    from azure.ai.vision.imageanalysis.models import VisualFeatures
    from azure.core.credentials import AzureKeyCredential
    
    client = ImageAnalysisClient(
        endpoint=""<YOUR_RESOURCE_ENDPOINT>"",
        credential=AzureKeyCredential(""<YOUR_AUTHORIZATION_KEY>"")
    )
    
    result = client.analyze(
        image_data=<IMAGE_DATA_BYTES>, # Binary data from your image file
        visual_features=[VisualFeatures.READ],
        language=""en"",
    )
    

To use the Azure AI Vision .NET SDK to extract text from an image, install the
**Azure.AI.Vision.ImageAnalysis** package. Then, in your code, use either key-
based authentication or Microsoft Entra ID authentication to connect an
**ImageAnalysisClient** object to an Azure AI Vision resource. To find and
read text in an image, call the **Analyze** method, specifying the
**VisualFeatures.Read** enumeration.

C# Copy

    
    
    using Azure.AI.Vision.ImageAnalysis;
    
    ImageAnalysisClient client = new ImageAnalysisClient(
        ""<YOUR_RESOURCE_ENDPOINT>"",
        new AzureKeyCredential(""<YOUR_AUTHORIZATION_KEY>""));
    
    ImageAnalysisResult result = client.Analyze(
        <IMAGE_DATA_BYTES>, // Binary data from your image file
        VisualFeatures.Read,
        new ImageAnalysisOptions { Language = t""en"" });
    

The results of the Read OCR function are returned synchronously, either as
JSON or the language-specific object of a similar structure. These results are
broken down in _blocks_ (with the current service only using one block), then
_lines_ , and then _words_. Additionally, the text values are included at both
the _line_ and _word_ levels, making it easier to read entire lines of text if
you don't need to extract text at the individual _word_ level.

JSON Copy

    
    
    {
        ""metadata"":
        {
            ""width"": 500,
            ""height"": 430
        },
        ""readResult"":
        {
            ""blocks"":
            [
                {
                    ""lines"":
                    [
                        {
                            ""text"": ""Hello World!"",
                            ""boundingPolygon"":
                            [
                                {""x"":251,""y"":265},
                                {""x"":673,""y"":260},
                                {""x"":674,""y"":308},
                                {""x"":252,""y"":318}
                            ],
                            ""words"":
                            [
                                {
                                    ""text"":""Hello"",
                                    ""boundingPolygon"":
                                    [
                                        {""x"":252,""y"":267},
                                        {""x"":307,""y"":265},
                                        {""x"":307,""y"":318},
                                        {""x"":253,""y"":318}
                                    ],
                                ""confidence"":0.996
                                },
                                {
                                    ""text"":""World!"",
                                    ""boundingPolygon"":
                                    [
                                        {""x"":318,""y"":264},
                                        {""x"":386,""y"":263},
                                        {""x"":387,""y"":316},
                                        {""x"":319,""y"":318}
                                    ],
                                    ""confidence"":0.99
                                }
                            ]
                        },
                    ]
                }
            ]
        }
    }
    

* * *"
141,Develop computer vision solutions in Azure - Training,Read text in images,Exercise - Read text in images,"# Exercise - Read text in images

Completed 100 XP

  * 30 minutes

Now it's your turn to try using the OCR capabilities of Azure AI Vision.

In this exercise, you use the Azure AI Vision Image Analysis SDK to develop a
client application that extracts text from images.

Note

To complete this lab, you need an **[Azure
subscription](https://azure.microsoft.com/free)** in which you have
administrative access.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/read-text-images-documents-
with-computer-vision-service/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2320100)

Tip

After completing the exercise, if you've finished exploring Azure AI services,
delete the Azure resources that you created during the exercise.

* * *"
142,Develop computer vision solutions in Azure - Training,Read text in images,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

1.

Which service should you use to locate and read text in signs within a
photograph of a street?

Azure AI Language Named Entity Recognition

Azure AI Document Intelligence

Azure AI Vision Image Analysis

2.

Which visual feature enumeration should you use to return OCR results from an
image analysis call?

VisualFeatures.Caption

VisualFeatures.Read

VisualFeatures.Tags

3.

Text location information in an image is returned at which levels by Azure AI
Vision Image Analysis?

The location of individual _words_ only.

A single _block_ containing all of the text in the image.

A _block_ containing the location of _lines_ of text as well as individual
_words_.

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
143,Develop computer vision solutions in Azure - Training,Read text in images,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/read-text-images-documents-with-computer-vision-service/7-summary
144,Develop computer vision solutions in Azure - Training,"Detect, analyze, and recognize faces",Introduction,"# Introduction

Completed 100 XP

  * 1 minute

Face detection, analysis, and recognition are all common computer vision
challenges for AI systems. The ability to detect when a person is present,
analyze a person's facial features, or recognize an individual based on their
face is a key way in which AI systems can exhibit human-like behavior and
build empathy with users.

In this module, you'll explore how the Azure AI Vision _Face API_ enables you
to build solutions that analyze faces in images.

![Diagram of faces being detected from an image by the Azure AI Vision Face
API.](../../wwl-data-ai/detect-analyze-recognize-faces/media/face-api.png)

Note

Access to the full capabilities of the Face API is restricted in accordance
with Microsoft's responsible AI policies. For details, see **[Limited Access
to Face API](/en-us/legal/cognitive-services/computer-vision/limited-access-
identity)**. This module describes some capabilities that require explicit
access. The practical exercise in the module is based on unrestricted features
of the service.

* * *"
145,Develop computer vision solutions in Azure - Training,"Detect, analyze, and recognize faces","Plan a face detection, analysis, or recognition solution","# Plan a face detection, analysis, or recognition solution

Completed 100 XP

  * 5 minutes

The **Face** service provides comprehensive facial detection, analysis, and
recognition capabilities.

![Diagram of Face service capabilities.](../../wwl-data-ai/detect-analyze-
recognize-faces/media/face-service.png)

The Face service provides functionality that you can use for:

  1. _Face detection_ \- for each detected face, the results include an ID that identifies the face and the bounding box coordinates indicating its location in the image.
  2. _Face attribute analysis_ \- you can return a wide range of facial attributes, including: 
    * Head pose ( _pitch_ , _roll_ , and _yaw_ orientation in 3D space)
    * Glasses ( _No glasses_ , _Reading glasses_ , _Sunglasses_ , or _Swimming Goggles_ )
    * Mask (the presence of a face mask)
    * Blur ( _low_ , _medium_ , or _high_ )
    * Exposure ( _under exposure_ , _good exposure_ , or _over exposure_ )
    * Noise (visual noise in the image)
    * Occlusion (objects obscuring the face)
    * Accessories (glasses, headwear, mask)
    * QualityForRecognition ( _low_ , _medium_ , or _high_ )
  3. _Facial landmark location_ \- coordinates for key landmarks in relation to facial features (for example, eye corners, pupils, tip of nose, and so on)
  4. _Face comparison_ \- you can compare faces across multiple images for similarity (to find individuals with similar facial features) and verification (to determine that a face in one image is the same person as a face in another image)
  5. _Facial recognition_ \- you can train a model with a collection of faces belonging to specific individuals, and use the model to identify those people in new images.
  6. _Facial liveness_ \- liveness can be used to determine if the input video is a real stream or a fake to prevent bad-intentioned individuals from spoofing a facial recognition system.

## Face detection and recognition models

The Azure AI Vision Face API is built on face detection and recognition models
that have been pre-trained. Multiple versions of these models are available,
each with specific strengths and capabilities. For example, newer models
exhibit greater accuracy when working with small images; but may not provide
the same breadth of facial analysis capabilities. When you use the service in
an application, you must select the model you want to use based on your
requirements.

Tip

For guidance about selecting a _detection_ model, see **[Specify a face
detection model](/en-us/azure/ai-services/computer-vision/how-to/specify-
detection-model)**. For guidance about how to select a _recognition_ model,
see **[Specify a face recognition model](/en-us/azure/ai-services/computer-
vision/how-to/specify-recognition-model)**.

* * *"
146,Develop computer vision solutions in Azure - Training,"Detect, analyze, and recognize faces",Detect and analyze faces,"# Detect and analyze faces

Completed 100 XP

  * 5 minutes

Choose your development language

C# Python

To use the Azure AI Vision Face API, you must provision a resource for the
service in an Azure subscription. You can provision **Face** as a single-
service resource, or you can use the Face API in a multi-service **Azure AI
Services** resource; which can be provisioned as a standalone resource or as
part of an Azure AI Foundry hub.

To use your resource from a client application you must connect to its
_endpoint_ using either _key-based authentication_ or _Microsoft Entra AI
authentication_. When using the REST interface you can provide the
authentication key or token in the request header. When using a language-
specific SDK (for example, the Python **azure-ai-vision-face** package or the
Microsoft .NET **Azure.AI.Vision.Face** package), you use a **FaceClient**
object to connect to the service.

Python Copy

    
    
    from azure.ai.vision.face import FaceClient
    from azure.ai.vision.face.models import *
    from azure.core.credentials import AzureKeyCredential
    
    face_client = FaceClient(
        endpoint=""<YOUR_RESOURCE_ENDPOINT>"",
        credential=AzureKeyCredential(""<YOUR_RESOURCE_KEY>""))
    

C# Copy

    
    
    using Azure;
    using Azure.AI.Vision.Face;
    
    FaceClient faceClient = new FaceClient(
        new Uri(""<YOUR_RESOURCE_ENDPOINT>""),
        new AzureKeyCredential(""<YOUR_RESOURCE_KEY>""));
    

To detect and analyze faces in an image, you must specify the model-specific
features you want the service to return, and then use the client to call the
**Detect** method.

Python Copy

    
    
    # Specify facial features to be retrieved
    features = [FaceAttributeTypeDetection01.HEAD_POSE,
                FaceAttributeTypeDetection01.OCCLUSION,
                FaceAttributeTypeDetection01.ACCESSORIES]
    
    # Use client to detect faces in an image
    with open(""<IMAGE_FILE_PATH>"", mode=""rb"") as image_data:
        detected_faces = face_client.detect(
            image_content=image_data.read(),
            detection_model=FaceDetectionModel.DETECTION01,
            recognition_model=FaceRecognitionModel.RECOGNITION01,
            return_face_id=True,
            return_face_attributes=features,
        )
    

C# Copy

    
    
    // Specify facial features to be retrieved
    FaceAttributeType[] features = new FaceAttributeType[]
    {
        FaceAttributeType.Detection01.HeadPose,
        FaceAttributeType.Detection01.Occlusion,
        FaceAttributeType.Detection01.Accessories
    };
    
    // Use client to detect faces in an image
    using (var imageData = File.OpenRead(imageFile))
    {    
        var response = await faceClient.DetectAsync(
            BinaryData.FromStream(imageData),
            FaceDetectionModel.Detection01,
            FaceRecognitionModel.Recognition01,
            returnFaceId: false,
            returnFaceAttributes: features);
        IReadOnlyList<FaceDetectionResult> detected_faces = response.Value;
    }
    

The response from the service depends on:

  * The model-specific features requested.
  * The number of faces detected in the image.

A response for an image containing a single face might look similar to the
following example:

JSON Copy

    
    
    [
        {
            'faceRectangle': {'top': 174, 'left': 247, 'width': 246, 'height': 246}
            'faceAttributes':
            {
                'headPose':{'pitch': 3.7, 'roll': -7.7, 'yaw': -20.9},
                'accessories':
                    [
                        {'type': 'glasses', 'confidence': 1.0}
                    ],
                'occlusion':{'foreheadOccluded': False, 'eyeOccluded': False, 'mouthOccluded': False}
            }
        }
    ]
    

* * *"
147,Develop computer vision solutions in Azure - Training,"Detect, analyze, and recognize faces",Verify and identify faces,"# Verify and identify faces

Completed 100 XP

  * 6 minutes

In addition to detecting and analyzing faces, you can use the Azure AI Vision
Face service to compare and recognize faces.

Important

Usage of facial recognition, comparison, and verification requires approval
through a **[Limited Access policy](https://aka.ms/cog-services-limited-
access)**.

## Verifying faces

When a face is detected by the Face service, a unique ID is assigned to it and
retained in the service resource for 24 hours. The ID is a GUID, with no
indication of the individual's identity other than their facial features.

While the detected face ID is cached, subsequent images can be used to compare
the new faces to the cached identity and determine if they're _similar_ (in
other words, they share similar facial features) or to _verify_ that the same
person appears in two images.

![Diagram of a detected face matched in two images.](../../wwl-data-ai/detect-
analyze-recognize-faces/media/face-matching.png)

This ability to compare faces anonymously can be useful in systems where it's
important to confirm that the same person is present on two occasions, without
the need to know the actual identity of the person. For example, by taking
images of people as they enter and leave a secured space to verify that
everyone who entered leaves.

## Identifying faces

For scenarios where you need to positively identify individuals, you can train
a facial recognition model using face images.

To train a facial recognition model with the Face service:

  1. Create a **Person Group** that defines the set of individuals you want to identify (for example, _employees_ ).
  2. Add a **Person** to the **Person Group** for each individual you want to identify.
  3. Add detected faces from multiple images to each **person** , preferably in various poses. The IDs of these faces will no longer expire after 24 hours (so they're now referred to as _persisted_ faces).
  4. Train the model.

![Diagram of person groups containing Person records with persisted
faces.](../../wwl-data-ai/detect-analyze-recognize-faces/media/person-
groups.png)

The trained model is stored in your Face (or Azure AI Services) resource, and
can be used by client applications to:

  * _Identify_ individuals in images.
  * _Verify_ the identity of a detected face.
  * Analyze new images to find faces that are _similar_ to a known, persisted face.

Tip

To learn more about using face verification and identification to implement a
facial recognition solution, see **[Face recognition](/en-us/azure/ai-
services/computer-vision/concept-face-recognition)** in the Azure AI Vision
Face documentation.

* * *"
148,Develop computer vision solutions in Azure - Training,"Detect, analyze, and recognize faces",Responsible AI considerations for face-based solutions,"# Responsible AI considerations for face-based solutions

Completed 100 XP

  * 3 minutes

While all applications of artificial intelligence require considerations for
responsible, system that rely on facial or other biometric data can be
particularly problematic.

When building a solution that uses facial data, considerations include (but
aren't limited to):

  * **Data privacy and security**. Facial data is personally identifiable, and should be considered sensitive and private. You should ensure that you have implemented adequate protection for facial data used for model training and inferencing.
  * **Transparency**. Ensure that users are informed about how their facial data is used, and who will have access to it.
  * **Fairness and inclusiveness**. Ensure that your face-based system can't be used in a manner that is prejudicial to individuals based on their appearance, or to unfairly target individuals.

Note

You can find details of the responsible AI measures taken in the
implementation of the Face API in **[Data and privacy for Face](/en-
us/legal/cognitive-services/face/data-privacy-security)**.

* * *"
149,Develop computer vision solutions in Azure - Training,"Detect, analyze, and recognize faces",Exercise - Detect and analyze faces,"# Exercise - Detect and analyze faces

Completed 100 XP

  * 30 minutes

Now it's your turn to try using the Azure AI Vision Face service.

In this exercise, you use the Azure AI Vision Face API to develop a client
application that detects and analyzes faces in an image.

Note

To complete this lab, you need an **[Azure
subscription](https://azure.microsoft.com/free)** in which you have
administrative access.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/detect-analyze-recognize-
faces/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2320103)

Tip

After completing the exercise, if you've finished exploring Azure AI services,
delete the Azure resources that you created during the exercise.

* * *"
150,Develop computer vision solutions in Azure - Training,"Detect, analyze, and recognize faces",Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

## Check your knowledge

1.

You need to create a facial recognition solution to identify named employees.
Which service should you use?

Azure AI Vision Image Analysis.

Azure AI Vision Custom Vision.

Azure AI Vision Face.

2.

What should you return when analyzing an image to see the spatial points
related to facial features?

Accessories

Landmarks

HeadPose

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
151,Develop computer vision solutions in Azure - Training,"Detect, analyze, and recognize faces",Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/detect-analyze-recognize-faces/10-summary
152,Develop computer vision solutions in Azure - Training,Classify images,Introduction,"# Introduction

Completed 100 XP

  * 1 minute

_Image classification_ is a common computer vision problem that requires
software to analyze an image and categorize (or _classify_ ) it.

For example, an unattended checkout system in a grocery store might use a
camera to scan each item a customer adds to their cart, and use image
classification to identify the product in the image.

![Diagram of an image classification solution for groceries.](../../wwl-data-
ai/classify-images/media/image-classification.png)

In this module, you'll learn how the **Azure AI Custom Vision** service
enables you to build your own computer vision models for image classification.

* * *"
153,Develop computer vision solutions in Azure - Training,Classify images,Azure AI Custom Vision,"# Azure AI Custom Vision

Completed 100 XP

  * 5 minutes

The **Azure AI Custom Vision** service enables you to build your own computer
vision models for _image classification_ or _object detection_.

To use the Custom Vision service to create a solution, you need _two_ Custom
Vision resources in your Azure subscription:

  * An **Azure AI Custom Vision _training_** resource - used to train a custom model based on your own training images.
  * An **Azure AI Custom Vision _prediction_** resource - used to generate predictions from new images based on your trained model.

When you provision the Azure AI Custom Vision service in an Azure
subscription, you can choose to create one or both of these resources. This
separation of training and prediction provides flexibility. For example, you
can use a _training_ resource in one region to train your model using your own
image data; and then deploy one or more prediction resources in other regions
to support computer vision applications that need to use your model.

Each resource has its own unique _endpoint_ and authentication _keys_ ; which
are used by client applications to connect and authenticate to the service.

## The Custom Vision portal

Azure AI Custom Vision provides a web-based portal, in which you can train,
publish, and test custom vision models.

![Screenshot of the Azure AI Custom Vision portal.](../../wwl-data-
ai/classify-images/media/custom-vision-portal.png)

You can sign into the Custom Vision portal at <https://www.customvision.ai/>
using your Azure credentials and use it to create image classification or
object detection projects that use Azure AI Custom Vision resources in your
Azure subscription.

Each project has a unique _project ID_ ; which is used by client applications
to perform training or prediction tasks using code.

## Custom Vision SDKs

You can write code to train and consume custom models by using the Azure AI
Custom Vision language-specific SDKs.

For example, Microsoft C# developers can use the
[Microsoft.Azure.CognitiveServices.Vision.CustomVision.Training](https://www.nuget.org/packages/Microsoft.Azure.CognitiveServices.Vision.CustomVision.Training)
and
[Microsoft.Azure.CognitiveServices.Vision.CustomVision.Prediction](https://www.nuget.org/packages/Microsoft.Azure.CognitiveServices.Vision.CustomVision.Prediction)
Microsoft .NET packages for training and prediction respectively.

Python developers can perform both training and prediction tasks by using the
[azure-cognitiveservices-vision-customvision](https://pypi.org/project/azure-
cognitiveservices-vision-customvision) package.

* * *"
154,Develop computer vision solutions in Azure - Training,Classify images,Train an image classification model,"# Train an image classification model

Completed 100 XP

  * 3 minutes

_Image classification_ is a computer vision technique in which a model is
trained to predict a class label for an image based on its contents. Usually,
the class label relates to the main _subject_ of the image.

For example, the following images have been classified based on the type of
fruit they contain.

![Photographs of fruit classified as Apple, Banana, and Orange.](../../wwl-
data-ai/classify-images/media/classified-fruit.png)

Models can be trained for multiclass classification (in other words, there are
multiple classes, but each image can belong to only one class) or multilabel
classification (in other words, an image might be associated with multiple
labels).

## Training an image classification model

To train an image classification model with the Azure AI Custom Vision
service, you can use the Azure AI Custom Vision portal, the Azure AI Custom
Vision REST API or SDK, or a combination of both approaches.

In most cases, you'll typically use the Azure AI Custom Vision portal to train
your model.

![Screenshot of the Azure AI Custom Vision portal.](../../wwl-data-
ai/classify-images/media/train-classifier.png)

The portal provides a graphical interface that you can use to:

  1. Create an image classification project for your model and associate it with a training resource.
  2. Upload images, assigning class label tags to them.
  3. Review and edit tagged images.
  4. Train and evaluate a classification model.
  5. Test a trained model.
  6. Publish a trained model to a prediction resource.

The REST API and SDKs enable you to perform the same tasks by writing code,
which is useful if you need to automate model training and publishing as part
of a DevOps process.

* * *"
155,Develop computer vision solutions in Azure - Training,Classify images,Create an image classification client application,"# Create an image classification client application

Completed 100 XP

  * 3 minutes

Choose your development language

C# Python

After you've trained an image classification model, you can use the Azure AI
Custom Vision SDK to develop a client application that submits new images to
be classified.

Python Copy

    
    
    from msrest.authentication import ApiKeyCredentials
    from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient
    
    
     # Authenticate a client for the prediction API
    credentials = ApiKeyCredentials(in_headers={""Prediction-key"": ""<YOUR_PREDICTION_RESOURCE_KEY>""})
    prediction_client = CustomVisionPredictionClient(endpoint=""<YOUR_PREDICTION_RESOURCE_ENDPOINT>"",
                                                     credentials=credentials)
    
    # Get classification predictions for an image
    image_data = open(""<PATH_TO_IMAGE_FILE>""), ""rb"").read()
    results = prediction_client.classify_image(""<YOUR_PROJECT_ID>"",
                                               ""<YOUR_PUBLISHED_MODEL_NAME>"",
                                               image_data)
    
    # Process predictions
    for prediction in results.predictions:
        if prediction.probability > 0.5:
            print(image, ': {} ({:.0%})'.format(prediction.tag_name, prediction.probability))
    
    

C# Copy

    
    
    using System;
    using System.IO;
    using Microsoft.Azure.CognitiveServices.Vision.CustomVision.Prediction;
    
    // Authenticate a client for the prediction API
    CustomVisionPredictionClient prediction_client = new CustomVisionPredictionClient(new ApiKeyServiceClientCredentials(""<YOUR_PREDICTION_RESOURCE_KEY>""))
    {
        Endpoint = ""<YOUR_PREDICTION_RESOURCE_ENDPOINT>""
    };
    
    // Get classification predictions for an image
    MemoryStream image_data = new MemoryStream(File.ReadAllBytes(""<PATH_TO_IMAGE_FILE>""));
    var result = prediction_client.ClassifyImage(""<YOUR_PROJECT_ID>"",
                                                 ""<YOUR_PUBLISHED_MODEL_NAME>"",
                                                 image_data);
    
    // Process predictions
    foreach (var prediction in result.Predictions)
    {
        if (prediction.Probability > 0.5)
        {
            Console.WriteLine($""{prediction.TagName} ({prediction.Probability})"");
        }
    }
    
    

* * *"
156,Develop computer vision solutions in Azure - Training,Classify images,Exercise - Classify images,"# Exercise - Classify images

Completed 100 XP

  * 45 minutes

Now it's your turn to try using the Azure AI Custom Vision service.

In this exercise, you train and publish a custom image classification model,
and use the Azure AI Custom Vision SDK to test it.

Note

To complete this lab, you need an **[Azure
subscription](https://azure.microsoft.com/free)** in which you have
administrative access.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/classify-images/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2320009)

When you finish the exercise, don't forget to come back and complete the
knowledge check to earn points for completing this module!

Tip

After completing the exercise, if you've finished exploring Azure AI services,
delete the Azure resources that you created during the exercise.

* * *"
157,Develop computer vision solutions in Azure - Training,Classify images,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

## Check your knowledge

1.

What Azure AI Custom Vision resources should you create in Azure to create a
custom vision solution?

A single Computer Vision resource

A Custom Vision resource for image classification, and another for object
detection

A Custom Vision resource for training, and another for prediction

2.

You want to train a model that can categorize an image as ""cat"" or ""dog"" based
on its subject. What kind of Azure AI Custom Vision project should you create?

Image classification (multiclass)

Image classification (multilabel)

Object detection

3.

What information does a client application need to connect to Azure AI Custom
Vision and classify an image?

The endpoint and key for the Custom Vision training resource

The endpoint and key for the Custom Vision prediction resource

The endpoint and key for the Custom Vision prediction resource, the project ID
for your image classification project, and the name of your deployed model

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
158,Develop computer vision solutions in Azure - Training,Classify images,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/classify-images/7-summary
159,Develop computer vision solutions in Azure - Training,Detect objects in images,Introduction,"# Introduction

Completed 100 XP

  * 1 minute

_Object detection_ is a common computer vision problem that requires software
to identify the location of specific classes of object in an image.

![Diagram of an object detection model identifying objects in an
image.](../../wwl-data-ai/detect-objects-images/media/object-detection.png)

For example, an automated checkout system in a grocery store might use a
camera to monitor a checkout conveyer belt on which there might be multiple
different items at any one time. The system could use object detection to
identify which items are on the belt, and where in the image they appear.

In this module, you'll learn how to use the Azure AI Custom Vision service to
create object detection models.

* * *"
160,Develop computer vision solutions in Azure - Training,Detect objects in images,Use Azure AI Custom Vision for object detection,"# Use Azure AI Custom Vision for object detection

Completed 100 XP

  * 5 minutes

To use the Custom Vision service to create an object detection solution, you
need _two_ Custom Vision resources in your Azure subscription:

  * An **Azure AI Custom Vision _training_** resource - used to train a custom model based on your own training images.
  * An **Azure AI Custom Vision _prediction_** resource - used to generate predictions from new images based on your trained model.

When you provision the Azure AI Custom Vision service in an Azure
subscription, you can choose to create one or both of these resources. This
separation of training and prediction provides flexibility. For example, you
can use a _training_ resource in one region to train your model using your own
image data; and then deploy one or more prediction resources in other regions
to support computer vision applications that need to use your model.

Each resource has its own unique _endpoint_ and authentication _keys_ ; which
are used by client applications to connect and authenticate to the service.

## The Custom Vision portal

Azure AI Custom Vision provides a web-based portal, in which you can train,
publish, and test custom vision models.

![Screenshot of the Azure AI Custom Vision portal.](../../wwl-data-ai/detect-
objects-images/media/custom-vision-portal.png)

You can sign into the Custom Vision portal at <https://www.customvision.ai/>
using your Azure credentials and use it to create image classification or
object detection projects that use Azure AI Custom Vision resources in your
Azure subscription.

Each project has a unique _project ID_ ; which is used by client applications
to perform training or prediction tasks using code.

## Custom Vision SDKs

You can write code to train and consume custom models by using the Azure AI
Custom Vision language-specific SDKs.

For example, Microsoft C# developers can use the
[Microsoft.Azure.CognitiveServices.Vision.CustomVision.Training](https://www.nuget.org/packages/Microsoft.Azure.CognitiveServices.Vision.CustomVision.Training)
and
[Microsoft.Azure.CognitiveServices.Vision.CustomVision.Prediction](https://www.nuget.org/packages/Microsoft.Azure.CognitiveServices.Vision.CustomVision.Prediction)
Microsoft .NET packages for training and prediction respectively.

Python developers can perform both training and prediction tasks by using the
[azure-cognitiveservices-vision-customvision](https://pypi.org/project/azure-
cognitiveservices-vision-customvision) package.

* * *"
161,Develop computer vision solutions in Azure - Training,Detect objects in images,Train an object detector,"# Train an object detector

Completed 100 XP

  * 5 minutes

_Object detection_ is a form of computer vision in which a model is trained to
detect the presence and location of one or more classes of object in an image.

![Photograph with the location and type of fruits detected.](../../wwl-data-
ai/detect-objects-images/media/detected-objects.png)

There are two components to an object detection prediction:

  * The class label of each object detected in the image. For example, you might ascertain that an image contains an apple, an orange, and a banana.
  * The location of each object within the image, indicated as coordinates of a _bounding box_ that encloses the object.

To train an object detection model, you can use the Azure AI Custom Vision
portal to upload and label images before training, evaluating, testing, and
publishing the model; or you can use the REST API or a language-specific SDK
to write code that performs the training tasks.

## Image labeling

You can use Azure AI Custom Vision to create projects for _image
classification_ or _object detection_. The most significant difference between
training an image classification model and training an object detection model
is the labeling of the images with tags. While image classification requires
one or more tags that apply to the whole image, object detection requires that
each label consists of a tag and a _region_ that defines the bounding box for
each object in an image.

### Labeling images in the Azure AI Custom Vision portal

The Azure AI Custom Vision portal provides a graphical interface that you can
use to label your training images.

![Screenshot of tagged images in the Azure AI Custom Vision
portal.](../../wwl-data-ai/detect-objects-images/media/tagged-images.png)

The easiest option for labeling images for object detection is to use the
interactive interface in the Azure AI Custom Vision portal. This interface
automatically suggests regions that contain objects, to which you can assign
tags or adjust by dragging the bounding box to enclose the object you want to
label.

Additionally, after tagging an initial batch of images, you can train the
model. Subsequent labeling of new images can benefit from the _smart labeler_
tool in the portal, which can suggest not only the regions, but the classes of
object they contain.

### Alternative labeling approaches

Alternatively, you can use a custom or third-party labeling tool, or choose to
label images manually, to take advantage of other features, such as assigning
image labeling tasks to multiple team members.

If you choose to use a labeling tool other than the Azure AI Custom Vision
portal, you may need to adjust the output to match the measurement units
expected by the Azure AI Custom Vision API. Bounding boxes are defined by four
values that represent the left (X) and top (Y) coordinates of the top-left
corner of the bounding box, and the width and height of the bounding box.
These values are expressed as _proportional_ values relative to the source
image size. For example, consider this bounding box:

  * Left: 0.1
  * Top: 0.5
  * Width: 0.5
  * Height: 0.25

This defines a box in which the left is located 0.1 (one tenth) from the left
edge of the image, and the top is 0.5 (half the image height) from the top.
The box is half the width and a quarter of the height of the overall image.

The following image shows labeling information in JSON format for objects in
an image.

![Photograph with JSON labels for the objects it contains.](../../wwl-data-
ai/detect-objects-images/media/object-labels.png)

* * *"
162,Develop computer vision solutions in Azure - Training,Detect objects in images,Develop an object detection client application,"# Develop an object detection client application

Completed 100 XP

  * 5 minutes

Choose your development language

C# Python

After you've trained an object detection model, you can use the Azure AI
Custom Vision SDK to develop a client application that submits new images to
be analyzed.

Python Copy

    
    
    from msrest.authentication import ApiKeyCredentials
    from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient
    
    
     # Authenticate a client for the prediction API
    credentials = ApiKeyCredentials(in_headers={""Prediction-key"": ""<YOUR_PREDICTION_RESOURCE_KEY>""})
    prediction_client = CustomVisionPredictionClient(endpoint=""<YOUR_PREDICTION_RESOURCE_ENDPOINT>"",
                                                     credentials=credentials)
    
    # Get classification predictions for an image
    image_data = open(""<PATH_TO_IMAGE_FILE>"", ""rb"").read()
    results = prediction_client.detect_image(""<YOUR_PROJECT_ID>"",
                                               ""<YOUR_PUBLISHED_MODEL_NAME>"",
                                               image_data)
    
    # Process predictions
    for prediction in results.predictions:
        if prediction.probability > 0.5:
            left = prediction.bounding_box.left
            top = prediction.bounding_box.top 
            height = prediction.bounding_box.height
            width =  prediction.bounding_box.width
            print(f""{prediction.tag_name} ({prediction.probability})"")
            print(f""  Left:{left}, Top:{top}, Height:{height}, Width:{width}"")
    
    
    

C# Copy

    
    
    using System;
    using System.IO;
    using Microsoft.Azure.CognitiveServices.Vision.CustomVision.Prediction;
    
    // Authenticate a client for the prediction API
    CustomVisionPredictionClient prediction_client = new CustomVisionPredictionClient(new ApiKeyServiceClientCredentials(""<YOUR_PREDICTION_RESOURCE_KEY>""))
    {
        Endpoint = ""<YOUR_PREDICTION_RESOURCE_ENDPOINT>""
    };
    
    // Get classification predictions for an image
    MemoryStream image_data = new MemoryStream(File.ReadAllBytes(""<PATH_TO_IMAGE_FILE>""));
    var result = prediction_client.DetectImage(""<YOUR_PROJECT_ID>"",
                                                 ""<YOUR_PUBLISHED_MODEL_NAME>"",
                                                 image_data);
    
    // Process predictions
    foreach (var prediction in result.Predictions)
    {
        if (prediction.Probability > 0.5)
        {
            var left = prediction.BoundingBox.Left;
            var top = prediction.BoundingBox.Top;
            var height = prediction.BoundingBox.Height;
            var width =  prediction.BoundingBox.Width;
            Console.WriteLine($""{prediction.TagName} ({prediction.Probability})"");
            Console.WriteLine($""  Left:{left}, Top:{top}, Height:{height}, Width:{width}"");
        }
    }
    
    

* * *"
163,Develop computer vision solutions in Azure - Training,Detect objects in images,Exercise - Detect objects in images,"# Exercise - Detect objects in images

Completed 100 XP

  * 45 minutes

If you have an Azure subscription, you can use Azure AI Custom Vision to
create a custom object detection model for yourself.

In this exercise, you'll train an object detection model and test it from a
client application.

Note

If you don't have an Azure subscription, and you want to explore Azure AI
Studio, you can [sign up for an account](https://azure.microsoft.com/free),
which includes credits for the first 30 days.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/detect-objects-
images/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2320011)

Tip

After completing the exercise, if you've finished exploring Azure AI services,
delete the Azure resources that you created during the exercise.

* * *"
164,Develop computer vision solutions in Azure - Training,Detect objects in images,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

## Check your knowledge

1.

What does an object detection model predict?

The location and class of specific classes of object in an image.

The class of the main subject of an image.

The file type of an image.

2.

What must you do before taking advantage of the smart labeler tool when
creating an object detection model?

Create a JSON file containing bounding box coordinates.

Tag some images with objects of each class and train an initial object
detection model.

Train an image classification (multilabel) model.

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
165,Develop computer vision solutions in Azure - Training,Detect objects in images,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/detect-objects-images/7-summary
166,Develop computer vision solutions in Azure - Training,Analyze video,Introduction,"# Introduction

Completed 100 XP

  * 3 minutes

It's increasingly common for organizations and individuals to generate content
in video format. For example, you might use a cellphone to capture a live
event, or you might record a teleconference that combines webcam footage and
presentation of slides or documents. As a result, a great deal of information
is encapsulated in video files, and you may need to extract this information
for analysis or to support indexing for searchability.

In this module, you will learn how to use the **Azure Video Indexer** service
to analyze videos.

After completing this module, you’ll be able to:

  * Describe Azure Video Indexer capabilities.
  * Extract custom insights.
  * Use Azure Video Indexer widgets and APIs.

* * *"
167,Develop computer vision solutions in Azure - Training,Analyze video,Understand Azure Video Indexer capabilities,"# Understand Azure Video Indexer capabilities

Completed 100 XP

  * 3 minutes

The **Azure Video Indexer** service is designed to help you extract
information from videos. It provides functionality that you can use for:

  * _Facial recognition_ \- detecting the presence of individual people in the image. This requires [Limited Access](https://aka.ms/cog-services-limited-access) approval.
  * _Optical character recognition_ \- reading text in the video.
  * _Speech transcription_ \- creating a text transcript of spoken dialog in the video.
  * _Topics_ \- identification of key topics discussed in the video.
  * _Sentiment_ \- analysis of how positive or negative segments within the video are.
  * _Labels_ \- label tags that identify key objects or themes throughout the video.
  * _Content moderation_ \- detection of adult or violent themes in the video.
  * _Scene segmentation_ \- a breakdown of the video into its constituent scenes.

The Video Analyzer service provides a portal website that you can use to
upload, view, and analyze videos interactively.

![The Video Analyzer portal](../../wwl-data-ai/analyze-video/media/video-
indexer-portal.png)

* * *"
168,Develop computer vision solutions in Azure - Training,Analyze video,Extract custom insights,"# Extract custom insights

Completed 100 XP

  * 3 minutes

Azure Video Indexer includes predefined models that can recognize well-known
celebrities, do OCR, and transcribe spoken phrases into text. You can extend
the recognition capabilities of Video Analyzer by creating custom models for:

  * **People**. Add images of the faces of people you want to recognize in videos, and train a model. Video Indexer will then recognize these people in all of your videos. 

Note

This only works after [Limited Access](https://aka.ms/cog-services-limited-
access) approval, adhering to our Responsible AI standard.

  * **Language**. If your organization uses specific terminology that may not be in common usage, you can train a custom model to detect and transcribe it.
  * **Brands**. You can train a model to recognize specific names as brands, for example to identify products, projects, or companies that are relevant to your business.

* * *"
169,Develop computer vision solutions in Azure - Training,Analyze video,Use Video Analyzer widgets and APIs,"# Use Video Analyzer widgets and APIs

Completed 100 XP

  * 3 minutes

While you can perform all video analysis tasks in the Azure Video Indexer
portal, you may want to incorporate the service into custom applications.
There are two ways you can accomplish this.

## Azure Video Indexer widgets

The widgets used in the Azure Video Indexer portal to play, analyze, and edit
videos can be embedded in your own custom HTML interfaces. You can use this
technique to share insights from specific videos with others without giving
them full access to your account in the Azure Video Indexer portal.

![Video Analyzer widgets in a custom web page](../../wwl-data-ai/analyze-
video/media/widgets.png)

## Azure Video Indexer API

Azure Video Indexer provides a REST API that you can use to obtain information
about your account, including an access token.

HTTP Copy

    
    
    https://api.videoindexer.ai/Auth/<location>/Accounts/<accountId>/AccessToken
    

You can then use your token to consume the REST API and automate video
indexing tasks, creating projects, retrieving insights, and creating or
deleting custom models.

For example, a GET call to
`https://api.videoindexer.ai/<location>/Accounts/<accountId>/Customization/CustomLogos/Logos/<logoId>?<accessToken>`
REST endpoint returns the specified logo. In another example, you can send a
GET request to
`https://api.videoindexer.ai/<location>/Accounts/<accountId>/Videos?<accessToken>`,
which returns details of videos in your account, similar to the following JSON
example:

JSON Copy

    
    
    {
        ""accountId"": ""SampleAccountId"",
        ""id"": ""30e66ec1b1"",
        ""partition"": null,
        ""externalId"": null,
        ""metadata"": null,
        ""name"": ""test3"",
        ""description"": null,
        ""created"": ""2018-04-25T16=50=00.967+00=00"",
        ""lastModified"": ""2018-04-25T16=58=13.409+00=00"",
        ""lastIndexed"": ""2018-04-25T16=50=12.991+00=00"",
        ""privacyMode"": ""Private"",
        ""userName"": ""SampleUserName"",
        ""isOwned"": true,
        ""isBase"": true,
        ""state"": ""Processing"",
        ""processingProgress"": """",
        ""durationInSeconds"": 13,
        ""thumbnailVideoId"": ""30e66ec1b1"",
        ""thumbnailId"": ""55848b7b-8be7-4285-893e-cdc366e09133"",
        ""social"": {
            ""likedByUser"": false,
            ""likes"": 0,
            ""views"": 0
        },
        ""searchMatches"": [],
        ""indexingPreset"": ""Default"",
        ""streamingPreset"": ""Default"",
        ""sourceLanguage"": ""en-US""
    }
    

## Deploy with ARM template

Azure Resource Manager (ARM) templates are available to create the Azure AI
Video Indexer resource in your subscription, based on the parameters specified
in the template file.

For a full list of available APIs, see the [Video Indexer Developer
Portal](https://api-portal.videoindexer.ai/).

* * *"
170,Develop computer vision solutions in Azure - Training,Analyze video,Exercise - Analyze video,"# Exercise - Analyze video

Completed 100 XP

  * 30 minutes

Now it's your turn to try using the Azure AI Video Indexer service.

In this exercise, you analyze a video using the Azure AI Video Indexer portal
and its API.

Note

To complete this lab, you need an **[Azure
subscription](https://azure.microsoft.com/free)** in which you have
administrative access.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/analyze-video/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2320415)

Tip

After completing the exercise, if you've finished exploring Azure AI services,
delete the Azure resources that you created during the exercise.

* * *"
171,Develop computer vision solutions in Azure - Training,Analyze video,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

## Check your knowledge

1.

You want Azure Video Indexer to analyze a video. What must you do first?

Use the Azure AI Vision service to extract key frames from the video.

Upload the video to Azure Video Indexer and index it.

Store the video file in an Azure blob store container.

2.

You want Azure Video Indexer to recognize brands in videos recorded from
conference calls. What should you do?

Edit the Brands model to show brands suggested by Bing, and add any new brands
you want to detect.

Edit all recorded conference call videos to include a caption of each brand
seen on their first appearance.

Embed the Azure Video Indexer widgets in a custom web site.

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
172,Develop computer vision solutions in Azure - Training,Analyze video,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/analyze-video/7-summary
173,Develop computer vision solutions in Azure - Training,Develop a vision-enabled generative AI application,Introduction,"# Introduction

Completed 100 XP

  * 1 minute

Generative AI models enable you to develop chat-based applications that reason
over and respond to input. Often this input takes the form of a text-based
prompt, but increasingly multimodal models that can respond to visual input
are becoming available.

In this module, we'll discuss vision-enabled generative AI and explore how you
can use Azure AI Foundry to create generative AI solutions that respond to
prompts that include a mix of text and image data.

* * *"
174,Develop computer vision solutions in Azure - Training,Develop a vision-enabled generative AI application,Deploy a multimodal model,"# Deploy a multimodal model

Completed 100 XP

  * 3 minutes

To handle prompts that include images, you need to deploy a _multimodal_
generative AI model - in other words, a model that supports not only text-
based input, but image-based (and in some cases, audio-based) input as well.
Multimodal models available in Azure AI Foundry include (among others):

  * Microsoft **Phi-4-multimodal-instruct**
  * OpenAI **gpt-4o**
  * OpenAI **gpt-4o-mini**

Tip

To learn more about available models in Azure AI Foundry, see the **[Model
catalog and collections in Azure AI Foundry portal](/en-us/azure/ai-
foundry/how-to/model-catalog-overview)** article in the Azure AI Foundry
documentation.

## Testing multimodal models with image-based prompts

After deploying a multimodal model, you can test it in the chat playground in
Azure AI Foundry portal.

![Screenshot of the chat playground with an image-based prompt.](../../wwl-
data-ai/develop-generative-ai-vision-apps/media/image-prompt.png)

In the chat playground, you can upload an image from a local file and add text
to the message to elicit a response from a multimodal model.

* * *"
175,Develop computer vision solutions in Azure - Training,Develop a vision-enabled generative AI application,Develop a vision-based chat app,"# Develop a vision-based chat app

Completed 100 XP

  * 5 minutes

To develop a client app that engages in vision-based chats with a multimodal
model, you can use the same basic techniques used for text-based chats. You
require a connection to the endpoint where the model is deployed, and you use
that endpoint to submit prompts that consists of messages to the model and
process the responses.

The key difference is that prompts for a vision-based chat include multi-part
user messages that contain both a _text_ (or _audio_ where supported) content
item and an _image_ content item.

![Diagram of a multi-part prompt being submitted to a model.](../../wwl-data-
ai/develop-generative-ai-vision-apps/media/multi-part-prompt.png)

The JSON representation of a prompt that includes a multi-part user message
looks something like this:

JSON Copy

    
    
    { 
        ""messages"": [ 
            { ""role"": ""system"", ""content"": ""You are a helpful assistant."" }, 
            { ""role"": ""user"", ""content"": [  
                { 
                    ""type"": ""text"", 
                    ""text"": ""Describe this picture:"" 
                },
                { 
                    ""type"": ""image_url"",
                    ""image_url"": {
                        ""url"": ""https://.....""
                    }
                }
            ] } 
        ]
    } 
    

The image content item can be:

  * A URL to an image file in a web site.
  * Binary image data

When using binary data to submit a local image file, the **image_url** content
takes the form of a base64 encoded value in a data URL format:

JSON Copy

    
    
    {
        ""type"": ""image_url"",
        ""image_url"": {
           ""url"": ""data:image/jpeg;base64,<binary_image_data>""
        }
    }
    

Depending on the model type, and where you deployed it, you can use Microsoft
Azure AI Model Inference or OpenAI APIs to submit vision-based prompts. These
libraries also provide language-specific SDKs that abstract the underlying
REST APIs.

In the exercise that follows in this module, you can use the Python or .NET
SDK for the Azure AI Model Inference API and the OpenAI API to develop a
vision-enabled chat application.

* * *"
176,Develop computer vision solutions in Azure - Training,Develop a vision-enabled generative AI application,Exercise - Develop a vision-enabled chat app,"# Exercise - Develop a vision-enabled chat app

Completed 100 XP

  * 30 minutes

If you have an Azure subscription, you can complete this exercise to develop a
vision-enabled chat app.

Note

If you don't have an Azure subscription, you can [sign up for an
account](https://azure.microsoft.com/free), which includes credits for the
first 30 days.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/develop-generative-ai-vision-
apps/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2318613)

* * *"
177,Develop computer vision solutions in Azure - Training,Develop a vision-enabled generative AI application,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

1.

Which kind of model can you use to respond to visual input?

Only OpenAI GPT models

Embedding models

Multimodal models

2.

How can you submit a prompt that asks a model to analyze an image?

Submit one prompt with an image-based message followed by another prompt with
a text-based message.

Submit a prompt that contains a multi-part user message, containing both text
content and image content.

Submit the image as the system message and the instruction or question as the
user message.

3.

How can you include an image in a message?

As a URL or as binary data

Only as a URL

Only as binary data

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
178,Develop computer vision solutions in Azure - Training,Develop a vision-enabled generative AI application,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/develop-generative-ai-vision-apps/6-summary
179,Develop computer vision solutions in Azure - Training,Generate images with AI,Introduction,"# Introduction

Completed 100 XP

  * 1 minute

With Azure AI Foundry, you can use language models to generate content based
on natural language prompts. Often the generated content is in the form of
natural language text, but increasingly, models can generate other kinds of
content.

For example, the OpenAI DALL-E image generation model can create original
graphical content based on a description of a desired image.

![Diagram of a prompt requesting a model to create an image.](../../wwl-data-
ai/generate-images-azure-openai/media/image-generation.png)

The ability to use AI to generate graphics has many applications; including
the creation of illustrations or photorealistic images for articles or
marketing collateral, generation of unique product or company logos, or any
scenario where a desired image can be described.

In this module, you'll learn how to develop an application that uses
generative AI to generate original images.

* * *"
180,Develop computer vision solutions in Azure - Training,Generate images with AI,What are image-generation models?,"# What are image-generation models?

Completed 100 XP

  * 2 minutes

Azure AI Foundry supports multiple models that are capable of generating
images, including (but not limited to):

  * DALL-E 3
  * GPT-Image 1

Tip

For the latest information about model availability in Azure AI Foundry, view
the model catalog. See **[Model catalog and collections in Azure AI Foundry
portal](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/model-
catalog-overview)** for details.

Image generation models are generative AI model that can create graphical data
from natural language input. Put more simply, you can provide the model with a
description and it can generate an appropriate image.

For example, you might submit the following natural language prompt to an
image generation model:

_A squirrel on a motorcycle_

This prompt could result in the generation of graphical output such as the
following image:

![A picture of a squirrel on a motorcycle.](../../wwl-data-ai/generate-images-
azure-openai/media/squirrel-on-motorcycle.png)

The images generated are original; they aren't retrieved from a curated image
catalog. In other words, the model isn't a search system for _finding_
appropriate images - it is an artificial intelligence (AI) model that
_generates_ new images based on the data on which it was trained.

* * *"
181,Develop computer vision solutions in Azure - Training,Generate images with AI,Explore image-generation models in Azure AI Foundry portal,"# Explore image-generation models in Azure AI Foundry portal

Completed 100 XP

  * 3 minutes

To experiment with image generation models, you can create an Azure AI Foundry
project and use the _Images playground_ in Azure AI Foundry portal to submit
prompts and view the resulting generated images.

![A screenshot of the Images playground in Azure AI Studio.](../../wwl-data-
ai/generate-images-azure-openai/media/images-playground.png)

When using the playground, you can adjust the **settings** to control the
output. For example, when using a DALL-E model you can specify:

  * The resolution (size) of the generated images. Available sizes are `1024x1024` (which is the default value), `1792x1024`, or `1024x1792`.
  * The image style to be generated (such as `vivid` or `natural`).
  * The image quality (choose from `standard` or `hd`).

* * *"
182,Develop computer vision solutions in Azure - Training,Generate images with AI,Create a client application that uses an image generation model,"# Create a client application that uses an image generation model

Completed 100 XP

  * 3 minutes

You can use a REST API to consume DALL-E models from applications.
Alternatively, you can use a language-specific SDK (for example, the OpenAI
Python SDK or the Azure OpenAI .NET SDK) to abstract the REST methods.

You initiate the image generation process by submitting a request to the
service endpoint with the authorization key in the header. The request
contains parameters describing the image-generation requirements. For example,
parameters for a DALL-E model include:

  * **prompt** : The description of the image to be generated.
  * **n** : The number of images to be generated. DALL-E 3 only supports n=1.
  * **size** : The resolution of the image(s) to be generated ( _1024x1024_ , _1792x1024_ , or _1024x1792_ for DALL-E 3)
  * **quality** _Optional_ : The quality of the image ( _standard_ or _hd_ ). Defaults to _standard_.
  * **style** _Optional_ : The visual style of the image ( _natural_ or _vivid_ ). Defaults to _vivid_.

For example, the following JSON could be submitted via the REST API to a
DALL-E model, prompting it to generate an 1024 x 1024 image of a badger
wearing a tuxedo:

JSON Copy

    
    
    {
        ""prompt"": ""A badger wearing a tuxedo"",
        ""n"": 1,
        ""size"": ""1024x1024"",
        ""quality"": ""hd"", 
        ""style"": ""vivid""
    }
    

With DALL-E 3, the result from the request is processed synchronously with the
response containing the URL for the generated image. The response is similar
to the following JSON:

JSON Copy

    
    
    {
        ""created"": 1686780744,
        ""data"": [
            {
                ""url"": ""<URL of generated image>"",
                ""revised_prompt"": ""<prompt that was used>""
            }
        ]
    }
    

The **data** element includes the **url** value, which references a PNG image
file generated from the prompt that you can then view or download. The
response also contains a **revised prompt** that was used to generate the
image, which was updated by the system to achieve the most desirable results.
In this example, the image might look similar to the following image:

![A picture of a badger wearing a tuxedo.](../../wwl-data-ai/generate-images-
azure-openai/media/badger-wearing-tuxedo.png)

* * *"
183,Develop computer vision solutions in Azure - Training,Generate images with AI,Exercise - Generate images with AI,"# Exercise - Generate images with AI

Completed 100 XP

  * 20 minutes

Now it's your chance to use generative AI to create images. In this exercise,
you'll provision an Azure AI Foundry project and deploy a DALL-E model. Then,
you'll explore image generation in the Azure AI Foundry portal. Finally,
you'll use the Python or .NET SDK to consume the DALL-E model from a custom
application.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/generate-images-azure-
openai/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2310727)

Tip

After completing the exercise, if you've finished exploring Azure AI Services,
delete the Azure resources that you created during the exercise.

* * *"
184,Develop computer vision solutions in Azure - Training,Generate images with AI,Module assessment,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/generate-images-azure-openai/6-knowledge-check
185,Develop computer vision solutions in Azure - Training,Generate images with AI,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/generate-images-azure-openai/7-summary
186,Develop AI information extraction solutions in Azure AI-3002 - Training,Create a multimodal analysis solution with Azure AI Content Understanding,Introduction,"# Introduction

Completed 100 XP

  * 1 minute

Organizations today rely on information that is often locked up in content
assets such as documents, images, videos, and audio recordings. Extracting
information from this content can be challenging, laborious, and time-
consuming, and organizations often need to build solutions based on multiple
technologies for content analysis depending on the formats being used.

Azure AI Content Understanding is a multimodal service that simplifies the
creation of AI-powered analyzers that can extract information from content in
practically any format.

![Diagram of Azure AI Content Understanding extracting information from
documents, audio files, videos, and images.](../../wwl-data-ai/analyze-
content-ai/media/content-understanding.png)

In this module, you'll explore the capabilities of Azure AI Content
Understanding, and learn how to use it to build custom analyzers.

Note

Azure AI Content Understanding is currently in public preview. Details
described in this module are subject to change.

* * *"
187,Develop AI information extraction solutions in Azure AI-3002 - Training,Create a multimodal analysis solution with Azure AI Content Understanding,What is Azure AI Content Understanding?,"# What is Azure AI Content Understanding?

Completed 100 XP

  * 5 minutes

Azure AI Content Understanding is a generative AI service that you can use to
extract insights and data from multiple kinds of content. With Content
Understanding, you can quickly build applications that analyze complex data
and generate outputs that can be used to automate and optimize processes.

Content Understanding is a component of Azure AI services. To use it, you need
to provision an Azure AI services resource in your Azure subscription. You can
develop and manage a Content Understanding solution:

  * In the Azure AI Foundry portal
  * By using the Content Understanding REST API

## Multimodal content analysis

Content Understanding can extract information from common kinds of content,
enabling you to use a single service with a straightforward and consistent
development process to build multimodal content analysis solutions.

### Documents and forms

You can use Content Understanding to analyze documents and forms and retrieve
specific field values. For example, you could extract key data values from an
invoice to automate payment processing.

![Screenshot of an invoice being analyzed by Content
Understanding.](../../wwl-data-ai/analyze-content-ai/media/document-
analysis.png)

### Images

You can analyze images to infer information from visuals such as charts,
identify physical defects in products or other items, detect the presence of
specific objects or people, or determine other information visually.

![Screenshot of an image being analyzed by Content Understanding.](../../wwl-
data-ai/analyze-content-ai/media/image-analysis.png)

### Audio

Analysis of audio enables you to automate tasks like summarizing conference
calls, determining sentiment of recorded customer conversations, or extracting
key data from telephone messages.

![Screenshot of an audio file being analyzed by Content
Understanding.](../../wwl-data-ai/analyze-content-ai/media/audio-analysis.png)

### Video

Video accounts for a large volume of the data captured today, and you can use
Content Understanding to analyze and extract insights from video to support
many scenarios. For example, to extract key points from video conference
recordings, to summarize presentations, or to detect the presence of specific
activity in security footage.

![Screenshot of an video file being analyzed by Content
Understanding.](../../wwl-data-ai/analyze-content-ai/media/video-analysis.png)

* * *"
188,Develop AI information extraction solutions in Azure AI-3002 - Training,Create a multimodal analysis solution with Azure AI Content Understanding,Create a Content Understanding analyzer,"# Create a Content Understanding analyzer

Completed 100 XP

  * 5 minutes

Content Understanding solutions are based on the creation of an _analyzer_ ;
which is trained to extract specific information from a particular type of
content based on a _schema_ that you define.

The high-level process for creating a Content Understanding solution includes
the following steps:

![Diagram of a sample document being used to create a schema for an analyzer
that is consumed by a client application.](../../wwl-data-ai/analyze-content-
ai/media/analyzer.png)

  1. Create an Azure AI services resource.
  2. Define a Content Understanding schema for the information to be extracted. This can be based on a content sample and an analyzer template.
  3. Build an analyzer based on the completed schema.
  4. Use the analyzer to extract or generate fields from new content.

Numerous analyzer templates are provided to help you develop an appropriate
analyzer for your needs quickly. Additionally, because of the generative AI
capabilities of Content Understanding, you can use minimal training data to
define a schema by example. In many cases, the service accurately identifies
the data values in the sample content that map to the schema elements
automatically, though you can also explicitly label fields in content such as
documents to improve the performance of your analyzer.

## Creating an analyzer with Azure AI Foundry

While you can provision an Azure AI services resource and develop a complete
Content Understanding solution through the REST API, the preferred approach
for AI development projects is to use Azure AI Foundry. Specifically, you can
use the Azure AI Foundry portal to create a project, define a Content
Understanding schema, and build and test an analyzer.

### Creating a Content Understanding project

In Azure AI Foundry, you can create a project in an existing AI hub, or you
can create a new hub as you create the project. In addition to the AI hub
itself, creating a hub provisions the Azure resources needed to support one or
more projects; including an Azure AI services resource, storage, and a key
vault resource to store sensitive details like credentials and keys.

![Screenshot of the Create new project user interface in Azure AI
Foundry.](../../wwl-data-ai/analyze-content-ai/media/new-project.png)

Note

Content Understanding schemas can only be created in Azure locations where the
service is supported. For more information, see **[Content Understanding
region and language support](/en-us/azure/ai-services/content-
understanding/language-region-support)**.

### Defining a schema

After creating a project, the first step in building an analyzer is to define
a schema for the content the analyzer will process, and the information it
will extract. Azure AI Foundry provides a schema editor interface in which you
can upload a file (document, image, audio, or video) on which the schema
should be based. You can then apply an appropriate schema template and define
the specific fields you want the analyzer to identify.

![Screenshot of the Define schema user interface in Azure AI
Foundry.](../../wwl-data-ai/analyze-content-ai/media/define-schema.png)

Note

The templates and field types available in a schema depend on the content type
of the file on which the schema is based. Some content types support
additional optional functionality, such as extracting barcodes and formulae
from text in documents. For more information about using Content Understanding
with different content types, see the following articles in the product
documentation:

  * **[Content Understanding document solutions](/en-us/azure/ai-services/content-understanding/document/overview)**
  * **[Content Understanding image solutions](/en-us/azure/ai-services/content-understanding/image/overview)**
  * **[Content Understanding audio solutions](/en-us/azure/ai-services/content-understanding/audio/overview)**
  * **[Content Understanding video solutions](/en-us/azure/ai-services/content-understanding/video/overview)**

### Testing

You can test the analyzer schema at any time during the development process by
running analysis on the sample file used to define the schema or other
uploaded files. The test results include the extracted field values and the
JSON format output returned by the analyzer to client applications.

![Screenshot of the Test analyzer user interface in Azure AI
Foundry.](../../wwl-data-ai/analyze-content-ai/media/document-analysis.png)

### Building an analyzer

When you're satisfied with the performance of your schema, you can build your
analyzer. Building an analyzer makes it accessible to client applications
through Content Understanding endpoint for the Azure AI services resource
associated with your project.

![Screenshot of the Build analyzer user interface in Azure AI
Foundry.](../../wwl-data-ai/analyze-content-ai/media/build-analyzer.png)

After building your analyzer, you can continue to test it in the Azure AI
Foundry portal, and refine the schema to create new named versions with
different capabilities.

* * *"
189,Develop AI information extraction solutions in Azure AI-3002 - Training,Create a multimodal analysis solution with Azure AI Content Understanding,Use the Content Understanding REST API,"# Use the Content Understanding REST API

Completed 100 XP

  * 5 minutes

The Content Understanding REST API provides a programmatic interface that you
can use to create, manage, and consume analyzers.

To use the REST API, your client application submits HTTP calls to the Content
Understanding endpoint for your Azure AI services resource, passing one of the
authorization keys in the header. You can obtain the endpoint and keys in the
Azure portal or in the Azure AI Foundry portal. You can also use the Azure AI
Foundry API to connect to the project and retrieve the endpoint and key for
your Azure AI Services resource programmatically.

![Screenshot of the Test analyzer user interface in Azure AI
Foundry.](../../wwl-data-ai/analyze-content-ai/media/azure-ai-services.png)

## Using the REST API to analyze content

One of the most common uses of the REST API is to submit content to an
existing analyzer that you have previously built, and retrieve the results of
analysis. The analysis request returns an operation ID value that represents
an asynchronous task. Your client application must then use another request to
pass the operation ID back to the endpoint and retrieve the operation status -
potentially polling multiple times until the operation is complete and the
results are returned in JSON format.

For example, to analyze a document, a client application might submit a POST
request to the `analyze` function containing the following JSON body:

JSON Copy

    
    
    POST {endpoint}/contentunderstanding/analyzers/{analyzer}:analyze?api-version={api version}
    {
      ""url"": ""https://host.com/doc.pdf""
    }
    

Note

You can specify a URL for the content file location, or you can include the
binary contents of the file.

Assuming the request is authenticated and initiated successfully, the response
will be similar to this example:

HTTP Copy

    
    
    Operation-Id: 1234abcd-1234-abcd-1234-abcd1234abcd
    Operation-Location: {endpoint}/contentunderstanding/analyzers/{analyzer}/results/1234abcd-1234-abcd-1234-abcd1234abcd?api-version={api version}
    {
      ""id"": ""1234abcd-1234-abcd-1234-abcd1234abcd"",
      ""status"": ""NotStarted""
    }
    

Your client application must then use the operation ID that has been returned
to check the status of the operation until it has succeeded (or failed) by
submitting a GET request to the `results` function.

HTTP Copy

    
    
    GET {endpoint}/contentunderstanding/analyzers/{analyzer}/results/1234abcd-1234-abcd-1234-abcd1234abcd?api-version={api version}
    

When the operation has completed successfully, the response contains a JSON
payload representing the results of the analysis. The specific results depend
on the content and schema.

Note

For more information about the Content Understanding REST API, see the
**[reference documentation](/en-us/rest/api/contentunderstanding/operation-
groups)**.

* * *"
190,Develop AI information extraction solutions in Azure AI-3002 - Training,Create a multimodal analysis solution with Azure AI Content Understanding,Exercise - Extract information from multimodal content,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/analyze-content-ai/05-exercise
191,Develop AI information extraction solutions in Azure AI-3002 - Training,Create a multimodal analysis solution with Azure AI Content Understanding,Module assessment,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/analyze-content-ai/06-knowledge-check
192,Develop AI information extraction solutions in Azure AI-3002 - Training,Create a multimodal analysis solution with Azure AI Content Understanding,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/analyze-content-ai/07-summary
193,Develop AI information extraction solutions in Azure AI-3002 - Training,Create an Azure AI Content Understanding client application,Introduction,"# Introduction

Completed 100 XP

  * 1 minute

Azure AI Content Understanding is a multimodal service that simplifies the
creation of AI-powered analyzers that can extract information from multiple
content formats, including documents, images, audio files, and videos.

Tip

To learn how to build Azure AI Content Understanding analyzers, complete the
**[Create a multimodal analysis solution with Azure AI Content
Understanding](/en-us/training/modules/analyze-content-ai/)** module.

You can develop client applications that use Azure AI Content Understanding
analyzers by using the Azure AI Content Understanding REST API; which is the
focus of this module.

![Diagram of Azure AI Content Understanding and a client
application.](../../wwl-data-ai/analyze-content-ai-api/media/content-
understanding.png)

In this module, you'll learn how to write code that uses the REST API to
submit a content file to an analyzer and process the results.

Note

Azure AI Content Understanding is currently in public preview. Details
described in this module are subject to change.

* * *"
194,Develop AI information extraction solutions in Azure AI-3002 - Training,Create an Azure AI Content Understanding client application,Prepare to use the AI Content Understanding REST API,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/analyze-content-ai-api/02-prepare-content-understanding
195,Develop AI information extraction solutions in Azure AI-3002 - Training,Create an Azure AI Content Understanding client application,Create a Content Understanding analyzer,"# Create a Content Understanding analyzer

Completed 100 XP

  * 5 minutes

In most scenarios, you should consider creating and testing analyzers using
the visual interface in the Azure AI Foundry portal. However, in some cases
you might want to create an analyzer by submitting a JSON definition of the
schema for your desired content fields to the REST API.

## Defining a schema for an analyzer

Analyzers are based on schemas that define the fields you want to extract or
generate from a content file. At its simplest, a schema is a set of fields,
which can be specified in a JSON document, as shown in this example of an
analyzer definition:

JSON Copy

    
    
    {
        ""description"": ""Simple business card"",
        ""baseAnalyzerId"": ""prebuilt-documentAnalyzer"",
        ""config"": {
            ""returnDetails"": true
        },
        ""fieldSchema"": {
            ""fields"": {
                ""ContactName"": {
                    ""type"": ""string"",
                    ""method"": ""extract"",
                    ""description"": ""Name on business card""
                },
                ""EmailAddress"": {
                    ""type"": ""string"",
                    ""method"": ""extract"",
                    ""description"": ""Email address on business card""
                }
            }
        }
    }
    

This example of a custom analyzer schema is based on the pre-built _document_
analyzer, and describes two fields that you would expect to find on a business
card: _ContactName_ and _EmailAddress_. Both fields are defined as string data
types, and are expected to be _extracted_ from a document (in other words, the
string values are expected to exist in the document so they can be ""read"";
rather than being fields that can be _generated_ by inferring information
about the document).

Note

This example is deliberately simple, with the minimal information needed to
create a working analyzer. In reality, the schema would likely include more
fields of different types, and the analyzer definition would include more
configuration settings. The JSON might even include a sample document. See the
[Azure AI Content Understanding REST API documentation](/en-
us/rest/api/contentunderstanding/content-analyzers/create-or-replace) for more
details.

## Using the REST API to create an analyzer

With your analyzer definition in place, you can use the REST API to submit it
to Azure AI Content Understanding to be created. The JSON data is submitted as
a `PUT` request to the endpoint with the API key in the request header to
start the analyzer creation operation.

The response from the `PUT` request includes a **Operation-Location** in the
header, which provides a _callback_ URL that you can use to check on the
status of the request by submitting a `GET` request.

You can use any HTTP-capable client tool or language to submit the request.
For example, the following Python code submits a request to create an analyzer
based on the contents of a file named _card.json_ (which is assumed to contain
the JSON definition described previously)

Python Copy

    
    
    import json
    import requests
    
    # Get the buisness card schema
    with open(""card.json"", ""r"") as file:
        schema_json = json.load(file)
    
    # Use a PUT request to submit the schema for a new analyzer
    analyzer_name = ""business_card_analyser""
    
    headers = {
        ""Ocp-Apim-Subscription-Key"": ""<YOUR_API_KEY>"",
        ""Content-Type"": ""application/json""}
    
    url = f""{<YOUR_ENDPOINT>}/contentunderstanding/analyzers/{analyzer_name}?api-version=2025-05-01-preview""
    
    response = requests.put(url, headers=headers, data=json.dumps(schema_json))
    
    # Get the response and extract the ID assigned to the operation
    callback_url = response.headers[""Operation-Location""]
    
    # Use a GET request to check the status of the operation
    result_response = requests.get(callback_url, headers=headers)
    
    # Keep polling until the operation is complete
    status = result_response.json().get(""status"")
    while status == ""Running"":
        result_response = requests.get(callback_url, headers=headers)
        status = result_response.json().get(""status"")
    
    print(""Done!"")
    

* * *"
196,Develop AI information extraction solutions in Azure AI-3002 - Training,Create an Azure AI Content Understanding client application,Analyze content,"# Analyze content

Completed 100 XP

  * 5 minutes

To analyze the contents of a file, you can use the Azure AI Content
Understanding REST API to submit it to the endpoint using a `POST` request.
You can specify the content as a URL (for a file hosted in an Internet-
accessible location) or as the binary contents of the file (for example, a
.pdf document, a .png image, an .mp3 audio file, or an .mp4 video file). The
request header must include the API key, and the endpoint address for the
**analyze** request includes the analyzer to be used.

As with the request to create an analyzer, the **analyze** request starts an
asynchronous operation. The `POST` request returns a unique operation ID,
which you can then use in a `GET` request to check the status of the analysis
operation.

For example, suppose you want to use the business card analyzer discussed
previously to extract the name and email address from the following scanned
business card image:

![Photograph of a business card for John Smith.](../../wwl-data-ai/analyze-
content-ai-api/media/business-card.png)

The following Python code submits a request for analysis, and then polls the
service until the operation is complete and the results are returned.

Python Copy

    
    
    import json
    import requests
    
    # Read the image data
    with open(""business-card.png"", ""rb"") as file:
            image_data = file.read()
        
    ## Use a POST request to submit the image data to the analyzer
    analyzer_name = ""business_card_analyser""
    
    headers = {
            ""Ocp-Apim-Subscription-Key"": ""<YOUR_API_KEY>"",
            ""Content-Type"": ""application/octet-stream""}
    
    url = f""{<YOUR_ENDPOINT>}/contentunderstanding/analyzers/{analyzer_name}:analyze?api-version=2025-05-01-preview""
    
    response = requests.post(url, headers=headers, data=image_data)
    
    # Get the response and extract the ID assigned to the analysis operation
    response_json = response.json()
    id_value = response_json.get(""id"")
    
    # Use a GET request to check the status of the analysis operation
    result_url = f""{<YOUR_ENDPOINT>}/contentunderstanding/analyzerResults/{id_value}?api-version=2025-05-01-preview""
    
    result_response = requests.get(result_url, headers=headers)
    
    # Keep polling until the analysis is complete
    status = result_response.json().get(""status"")
    while status == ""Running"":
            result_response = requests.get(result_url, headers=headers)
            status = result_response.json().get(""status"")
    
    # Get the analysis results
    if status == ""Succeeded"":
        result_json = result_response.json()
    
    
    

## Processing analysis results

The results in the response JSON depend on:

  * The kind of content the analyzer is designed to analyze (for example, document, video, image, or audio).
  * The schema for the analyzer.
  * The contents of the file that was analyzed.

For example, the response from the _document_ -based business card analyzer
when analyzing the business card described previously contain:

  * The extracted fields
  * The optical character recognition (OCR) layout of the document, including locations of lines of text, individual words, and paragraphs on each page.

Here's the complete JSON response for the business card analysis:

JSON Copy

    
    
    {
        ""id"": ""00000000-0000-0000-0000-a00000000000"",
        ""status"": ""Succeeded"",
        ""result"": {
            ""analyzerId"": ""biz_card_analyser_2"",
            ""apiVersion"": ""2025-05-01-preview"",
            ""createdAt"": ""2025-05-16T03:51:46Z"",
            ""warnings"": [],
            ""contents"": [
                {
                    ""markdown"": ""John Smith\nEmail: john@contoso.com\n"",
                    ""fields"": {
                        ""ContactName"": {
                            ""type"": ""string"",
                            ""valueString"": ""John Smith"",
                            ""spans"": [
                                {
                                    ""offset"": 0,
                                    ""length"": 10
                                }
                            ],
                            ""confidence"": 0.994,
                            ""source"": ""D(1,69,234,333,234,333,283,69,283)""
                        },
                        ""EmailAddress"": {
                            ""type"": ""string"",
                            ""valueString"": ""john@contoso.com"",
                            ""spans"": [
                                {
                                    ""offset"": 18,
                                    ""length"": 16
                                }
                            ],
                            ""confidence"": 0.998,
                            ""source"": ""D(1,179,309,458,309,458,341,179,341)""
                        }
                    },
                    ""kind"": ""document"",
                    ""startPageNumber"": 1,
                    ""endPageNumber"": 1,
                    ""unit"": ""pixel"",
                    ""pages"": [
                        {
                            ""pageNumber"": 1,
                            ""angle"": 0.03410444,
                            ""width"": 1000,
                            ""height"": 620,
                            ""spans"": [
                                {
                                    ""offset"": 0,
                                    ""length"": 35
                                }
                            ],
                            ""words"": [
                                {
                                    ""content"": ""John"",
                                    ""span"": {
                                        ""offset"": 0,
                                        ""length"": 4
                                    },
                                    ""confidence"": 0.992,
                                    ""source"": ""D(1,69,234,181,234,180,283,69,283)""
                                },
                                {
                                    ""content"": ""Smith"",
                                    ""span"": {
                                        ""offset"": 5,
                                        ""length"": 5
                                    },
                                    ""confidence"": 0.998,
                                    ""source"": ""D(1,200,234,333,234,333,282,200,283)""
                                },
                                {
                                    ""content"": ""Email:"",
                                    ""span"": {
                                        ""offset"": 11,
                                        ""length"": 6
                                    },
                                    ""confidence"": 0.995,
                                    ""source"": ""D(1,75,310,165,309,165,340,75,340)""
                                },
                                {
                                    ""content"": ""john@contoso.com"",
                                    ""span"": {
                                        ""offset"": 18,
                                        ""length"": 16
                                    },
                                    ""confidence"": 0.977,
                                    ""source"": ""D(1,179,309,458,311,458,340,179,341)""
                                }
                            ],
                            ""lines"": [
                                {
                                    ""content"": ""John Smith"",
                                    ""source"": ""D(1,69,234,333,233,333,282,69,282)"",
                                    ""span"": {
                                        ""offset"": 0,
                                        ""length"": 10
                                    }
                                },
                                {
                                    ""content"": ""Email: john@contoso.com"",
                                    ""source"": ""D(1,75,309,458,309,458,340,75,340)"",
                                    ""span"": {
                                        ""offset"": 11,
                                        ""length"": 23
                                    }
                                }
                            ]
                        }
                    ],
                    ""paragraphs"": [
                        {
                            ""content"": ""John Smith Email: john@contoso.com"",
                            ""source"": ""D(1,69,233,458,233,458,340,69,340)"",
                            ""span"": {
                                ""offset"": 0,
                                ""length"": 34
                            }
                        }
                    ],
                    ""sections"": [
                        {
                            ""span"": {
                                ""offset"": 0,
                                ""length"": 34
                            },
                            ""elements"": [
                                ""/paragraphs/0""
                            ]
                        }
                    ]
                }
            ]
        }
    }
    

Your application must typically parse the JSON to retrieve field values. For
example, the following python code extracts all of the _string_ values:

Python Copy

    
    
    # (continued from previous code example)
    
    # Iterate through the fields and extract the names and type-specific values
    contents = result_json[""result""][""contents""]
    for content in contents:
        if ""fields"" in content:
            fields = content[""fields""]
            for field_name, field_data in fields.items():
                if field_data['type'] == ""string"":
                    print(f""{field_name}: {field_data['valueString']}"")
    
    

The output from this code is shown here:

Copy

    
    
    ContactName: John Smith
    EmailAddress: john@contoso.com
    

* * *"
197,Develop AI information extraction solutions in Azure AI-3002 - Training,Create an Azure AI Content Understanding client application,Exercise - Develop a Content Understanding client application,"# Exercise - Develop a Content Understanding client application

Completed 100 XP

  * 40 minutes

Now it's your turn to build your own Content Understanding client application!

In this exercise, you use the Azure AI Content Understanding REST API to
extract information from content by submitting a file to an analyzer.

Note

To complete this lab, you need an **[Azure
subscription](https://azure.microsoft.com/free)** in which you have
administrative access.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/analyze-content-ai-
api/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2320464)

* * *"
198,Develop AI information extraction solutions in Azure AI-3002 - Training,Create an Azure AI Content Understanding client application,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

1.

What configuration values are needed to use the Azure AI Content Understanding
REST API?

The name of the resource group where the Azure AI service is deployed.

The Azure subscription ID and tenant ID.

The endpoint and key for the Azure AI service.

2.

What must be specified when calling the _analyze_ method to extract fields
from content?

The name of the Azure AI services resource.

The name of the analyzer.

The Operation-Location returned when the analyzer was created.

3.

How are the extracted fields returned?

As type-specific values.

As a list of strings.

As a single blob.

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
199,Develop AI information extraction solutions in Azure AI-3002 - Training,Create an Azure AI Content Understanding client application,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/analyze-content-ai-api/07-summary
200,Develop AI information extraction solutions in Azure AI-3002 - Training,Use prebuilt Document intelligence models,Introduction,"# Introduction

Completed 100 XP

  * 3 minutes

Many forms and documents that businesses handle are common across disparate
companies in different sectors. For example, most companies use invoices and
receipts. Microsoft Azure AI Document Intelligence includes prebuilt models so
you can handle common document types easily.

Imagine you conduct polls for private companies and political parties.
Participants submit their responses as paper forms or as online PDFs. You've
decided to deploy Azure AI Document Intelligence to streamline data entry. You
want to know if you can use the prebuilt models to generate meaningful data
from your forms.

In this module, you'll learn about the capabilities of the prebuilt models in
Azure AI Document Intelligence and how to use them.

## Learning objectives

At the end of this module, you'll be able to:

  * Identify business problems that you can solve by using prebuilt models in Azure AI Document Intelligence.
  * Analyze forms by using the General Document, Read, and Layout models.
  * Analyze forms by using financial, ID, and tax prebuilt models.

* * *"
201,Develop AI information extraction solutions in Azure AI-3002 - Training,Use prebuilt Document intelligence models,Understand prebuilt models,"# Understand prebuilt models

Completed 100 XP

  * 3 minutes

Choose your development language

C# Python

Prebuilt models in Azure AI Document Intelligence enable you to extract data
from common forms and documents without training your own models.

In your polling company, polling forms are unique to each survey project, but
you also use invoices and receipts to record financial transactions and you
have many unstructured documents. You want to know how much work is required
to extract names, addresses, amounts, and other information from these
documents.

Here, you learn how prebuilt models can help you analyze common document
types.

## What are prebuilt models?

The general approach used in AI solutions is to provide a large quantity of
sample data and then train an optimized model by trying different data
features, parameters, and statistical treatments. The combination that best
predicts the values that interest you constitute the trained model, and you
can use this model to predict values from new data.

Many of the forms that businesses use from day to day are of a few common
types. For example, most businesses issue or receive invoices and receipts.
Any business that has employees in the United States must use the W-2 tax
declaration form. Also you often have more general documents that you might
want to extract data from. For these cases, Microsoft helps you by providing
prebuilt models. Prebuilt models are already trained on large numbers of their
target form type.

If you want to use Document Intelligence to extract data from one of these
common forms or documents, you can choose to use a prebuilt model and you
don't have to train your own. Because Microsoft trains these models on a large
corpus of examples, you can expect them to provide accurate and reliable
results when dealing with their intended forms.

Several of the prebuilt models are trained on specific form types:

  * **Invoice model.** Extracts common fields and their values from invoices.
  * **Receipt model.** Extracts common fields and their values from receipts.
  * **US Tax model.** Unified US tax model that can extract from forms such as W-2, 1098, 1099, and 1040.
  * **ID document model.** Extracts common fields and their values from US drivers' licenses, European Union IDs and drivers license, and international passports.
  * **Business card model.** Extracts common fields and their values from business cards.
  * **Health insurance card model.** Extracts common fields and their values from health insurance cards.
  * **Marriage certificate.** Extracts information from marriage certificates.
  * **Credit/Debit card model.** Extracts common information from bank cards.
  * **Mortgage documents.** Extracts information from mortgage closing disclosure, Uniform Residential Loan Application (Form 1003), Appraisal (Form 1004), Validation of Employment (Form 1005), and Uniform Underwriting and Transmittal Summary (Form 1008).
  * **Bank statement model.** Extracts account information including beginning and ending balances, transaction details from bank statements.
  * **Pay Stub model.** Extracts wages, hours, deductions, net pay, and other common pay stub fields.
  * **Check model.** Extracts payee, amount, date, and other relevant information from checks.

The other models are designed to extract values from documents with less
specific structures:

  * **Read model.** Extracts text and languages from documents.
  * **General document model.** Extract text, keys, values, entities, and selection marks from documents.
  * **Layout model.** Extracts text and structure information from documents.

## Features of prebuilt models

The prebuilt models are designed to extract different types of data from the
documents and forms users submit. To select the right model for your
requirements, you must understand these features:

  * **Text extraction.** All the prebuilt models extract lines of text and words from hand-written and printed text.
  * **Key-value pairs.** Many models extract spans of text within a document that identify a label or key and its response or value as key-values pairs. For example, a typical key might be **Weight** and its value might be **31 kg**.
  * **Entities.** Text that includes common, more complex data structures can be extracted as entities. Entity types include people, locations, and dates.
  * **Selection marks.** Some models extract spans of text that indicate a choice as selection marks. These marks include radio buttons and check boxes.
  * **Tables.** Many models can extract tables in scanned forms included the data contained in cells, the numbers of columns and rows, and column and row headings. Tables with merged cells are supported.
  * **Fields.** Models trained for a specific form type identify the values of a fixed set of fields. For example, the Invoice model includes **CustomerName** and **InvoiceTotal** fields.

Also consider that prebuilt models are designed for and trained on generic
document and form types. If you have an industry-specific or unique form type
that you use often, you might be able to obtain more reliable and predictable
results by using a custom model. However, custom models take time to develop
because you must invest the time and resources to train them on example forms
before you can use it. The larger the number of example forms you provide for
training, the better the model is at predicting form content accurately.

## Input requirements

The prebuilt models are flexible but you can help them to return accurate and
helpful results by submitting one clear photo or high-quality scan for each
document.

You must also comply with these requirements when you submit a form for
analysis:

  * The file must be in JPEG, PNG, BMP, TIFF, or PDF format. Additionally, the Read model can accept Microsoft Office files.
  * The file must be smaller than 500 MB for the standard tier, and 4 MB for the free tier.
  * Images must have dimensions between 50 x 50 pixels and 10,000 x 10,000 pixels.
  * PDF documents must have dimensions less than 17 x 17 inches or A3 paper size.
  * PDF documents must not be protected with a password.

Note

If you can, submit text-embedded PDF files because they eliminate errors in
character recognition.

PDF and TIFF files can have any number of pages but, in the standard tier,
only the first 2,000 pages are analyzed. In the free tier, only the first two
pages are analyzed.

## Try out prebuilt models with Azure AI Document Intelligence Studio

Azure AI Document Intelligence is designed as a web service you can call using
code in your custom applications. However, it's often helpful to explore the
models and how they behave with your forms visually. You can perform such
experiments by using [Azure AI Document Intelligence
Studio](https://formrecognizer.appliedai.azure.com/studio) and use the
experience to help design and write your code.

You can choose any of the prebuilt models in Azure AI Document Intelligence
Studio. Microsoft provides some sample documents for use with each model or
you can add your own documents and analyze them.

[ ![Screenshot showing how to use Azure AI Document Intelligence Studio to
explore the business card prebuilt model.](../../wwl-data-ai/use-prebuilt-
form-recognizer-models/media/2-studio-business-card-example.png) ](../../wwl-
data-ai/use-prebuilt-form-recognizer-models/media/2-studio-business-card-
example.png#lightbox)

## Calling prebuilt models by using APIs

Because Azure AI Document Intelligence implements RESTful web services, you
can use web service calls from any language that supports them. However, when
you use Microsoft's Azure AI Document Intelligence APIs, security and session
management is simplified and you have to write less code.

Azure AI Document Intelligence is available for:

  * C# and other .NET languages.
  * Java.
  * Python.
  * JavaScript.

Whenever you want to call Azure AI Document Intelligence, you must start by
connecting and authenticating with the service in your Azure subscription. To
make that connection, you need:

  * **The service endpoint.** This value is the URL where the service is published.
  * **The API key.** This value is a unique key that grants access.

You obtain both of these values from the Azure portal.

Because the service can take a few seconds to respond, it's best to use
asynchronous calls to submit a form and then obtain results from the analysis:

C# Copy

    
    
    AnalyzeDocumentOperation operation = await client.AnalyzeDocumentFromUriAsync(WaitUntil.Completed, ""prebuilt-layout"", fileUri);
    
    AnalyzeResult result = operation.Value;
    

Python Copy

    
    
    poller = document_analysis_client.begin_analyze_document(
            ""prebuilt-layout"", AnalyzeDocumentRequest(url_source=docUrl
        ))
    result: AnalyzeResult = poller.result()
    

The details you can extract from these results depend on the model you used.

## Learn more

  * [What is Azure AI Document Intelligence?](/en-us/azure/ai-services/document-intelligence/overview)
  * [Azure AI Document Intelligence models](/en-us/azure/ai-services/document-intelligence/concept-model-overview)

* * *"
202,Develop AI information extraction solutions in Azure AI-3002 - Training,Use prebuilt Document intelligence models,"Use the General Document, Read, and Layout models","# Use the General Document, Read, and Layout models

Completed 100 XP

  * 3 minutes

If you want to extract text, languages, and other information from documents
with unpredictable structures, you can use the read, general document, or
layout models.

In your polling company, customers and partners often send specifications,
tenders, statements of work, and other documents with unpredictable
structures. You want to know if Azure AI Document Intelligence can analyze and
extract values from these documents.

Here, you'll learn about the prebuilt models that Microsoft provides for
general documents.

## Using the read model

The Azure AI Document Intelligence read model extracts printed and handwritten
text from documents and images. It's used to provide text extraction in all
the other prebuilt models.

The read model can also detect the language that a line of text is written in
and classify whether it's handwritten or printed text.

Note

The read model supports more languages for printed text than handwritten text.
Check the [documentation](/en-us/azure/ai-services/document-
intelligence/language-support-ocr#read-model) to see the current list of
supported languages.

For multi-page PDF or TIFF files, you can use the `pages` parameter in your
request to fix a page range for the analysis.

The read model is ideal if you want to extract words and lines from documents
with no fixed or predictable structure.

## Using the general document model

The general document model extends the functionality of the read model by
adding the detection of key-value pairs, entities, selection marks, and
tables. The model can extract these values from structured, semi-structured,
and unstructured documents.

The general document model is the only prebuilt model to support entity
extraction. It can recognize entities such as people, organizations, and dates
and it runs against the whole document, not just key-value pairs. This
approach ensures that, when structural complexity has prevented the model
extracting a key-value pair, an entity can be extracted instead. Remember,
however, that sometimes a single piece of text might return both a key-value
pair and an entity.

The types of entities you can detect include:

  * `Person`. The name of a person.
  * `PersonType`. A job title or role.
  * `Location`. Buildings, geographical features, geopolitical entities.
  * `Organization`. Companies, government bodies, sports clubs, musical bands, and other groups.
  * `Event`. Social gatherings, historical events, anniversaries.
  * `Product`. Objects bought and sold.
  * `Skill`. A capability belonging to a person.
  * `Address`. Mailing address for a physical location.
  * `Phone number`. Dialing codes and numbers for mobile phones and landlines.
  * `Email`. Email addresses.
  * `URL`. Webpage addresses.
  * `IP Address`. Network addresses for computer hardware.
  * `DateTime`. Calendar dates and times of day.
  * `Quantity`. Numerical measurements with their units.

## Using the layout model

As well as extracting text, the layout model returns selection marks and
tables from the input image or PDF file. It's a good model to use when you
need rich information about the structure of a document.

When you digitize a document, it can be at an odd angle. Tables can have
complicated structures with or without headers, cells that span columns or
rows, and incomplete columns or rows. The layout model can handle all of these
difficulties to extract the complete document structure.

For example, each table cell is extracted with:

  * Its content text.
  * The size and position of its bounding box.
  * If it's part of a header column.
  * Indexes to indicate its row and column position in the table.

Selection marks are extracted with their bounding box, a confidence indicator,
and whether they're selected or not.

## Learn more

  * [Language support for Azure AI Document Intelligence](/en-us/azure/ai-services/document-intelligence/language-support)
  * [Azure AI Document Intelligence read model](/en-us/azure/ai-services/document-intelligence/concept-read)
  * [Azure AI Document Intelligence general document model](/en-us/azure/ai-services/document-intelligence/concept-general-document)
  * [Azure AI Document Intelligence layout model](/en-us/azure/ai-services/document-intelligence/concept-layout)

* * *"
203,Develop AI information extraction solutions in Azure AI-3002 - Training,Use prebuilt Document intelligence models,"Use financial, ID, and tax models","# Use financial, ID, and tax models

Completed 100 XP

  * 3 minutes

Azure AI Document Intelligence includes some prebuilt models that are trained
on common form types. You can use these models to obtain the values of common
fields from invoices, receipts, business cards, and more.

In your polling company, invoices and receipts are often submitted as photos
or scans of the paper documents. Sometimes the scan is poor and the paper is
creased or damaged. You want to know if Azure AI Document Intelligence can get
this information into your databases more efficiently than manual data entry.

Here, you'll learn about the prebuilt models that handle financial, identity,
and tax documents.

## Using the invoice model

Your business both issues invoices and receives them from partner
organization. There might be many different formats on paper or in digitized
forms and some will have been scanned poorly at odd angles or from creased
paper.

The invoice model in Azure AI Document Intelligence can handle these
challenges and uses the features of the read model to extract text from
invoice scans. In addition, it extracts specific fields that are commonly used
on invoices including:

  * Customer name and reference ID
  * Purchase order number
  * Invoice and due dates
  * Details about the vendor, such as name, tax ID, physical address.
  * Similar details about the customer.
  * Billing and shipping addresses.
  * Amounts such as total tax, invoice total, and amount due.

Invoices also feature lines, usually in a table, each of which is one
purchased item. For each line, the invoice model identifies details including:

  * The description and product code of the product or service invoiced.
  * Amounts such as the unit price, the quantity of items, the tax incurred, and the line total.

## Using the receipt model

Receipts have similar fields and structures to invoices, but they record
amounts paid instead of amounts charged. Azure AI Document Intelligence faces
the same challenges of poor scanning or digitization but can reliably identify
fields including:

  * Merchant details such a name, phone number, and address.
  * Amounts such as receipt total, tax, and tip.
  * The date and time of the transaction.

As for invoices, receipts often include a table of items, each of which is a
product or service purchased. For each of these lines, the model recognizes:

  * The name of the item.
  * The quantity of the item purchased.
  * The unit price of the item.
  * The total price for that quantity.

Note

In Azure AI Document Intelligence v3.0 and later, the receipt model supports
single-page hotel receipt processing. If a receipt is classified as a hotel
receipt, the model extracts extra relevant fields such as arrival and
departure dates.

## Using the ID document model

The ID document model is trained to analyze two types of identity document:

  * United States drivers licenses.
  * International passports.

Note

Only the biographical pages of passports can be analyzed. Visas and other
travel documents are not supported.

The ID document model can extract fields including:

  * First and last names.
  * Personal information such as sex, date of birth, and nationality.
  * The country and region where the document was issued.
  * Unique numbers such as the document number and machine readable zone.
  * Endorsements, restrictions, and vehicle classifications.

Important

Since much of the data extracted by the ID document model is personal, it is
of a sensitive nature and covered by data protection laws in most
jurisdictions. Be sure that you have the permission of the individual to store
their data and comply with all legal requirements in the way you handle this
information.

## Using the business card model

Business cards are a popular way to exchange contact information quickly and
often include branding, unusual fonts, and graphic design elements. Fields
that the business card model can extract include:

  * First and last names.
  * Postal addresses.
  * Email and website addresses.
  * Various telephone numbers.

## Using other prebuilt models

Azure AI Document Intelligence offers several prebuilt models, with new models
being released regularly. Before training a custom model, it's worth verifying
if your use case can be analyzed accurately with one of these prebuilt models.
Using a prebuilt model will benefit from rigorous testing, updated model
versions, and reduced cost compared to a custom model.

## Learn more

  * [Azure AI Document Intelligence model overview](/en-us/azure/ai-services/document-intelligence/concept-model-overview)

* * *"
204,Develop AI information extraction solutions in Azure AI-3002 - Training,Use prebuilt Document intelligence models,Exercise - Analyze a document using Azure AI Document Intelligence,"# Exercise - Analyze a document using Azure AI Document Intelligence

Completed 100 XP

  * 3 minutes

In this exercise, you use Azure AI Document Intelligence to analyze forms
using a prebuilt model.

Note

To complete this lab, you need an **[Azure
subscription](https://azure.microsoft.com/free)** in which you have
administrative access.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/use-prebuilt-form-recognizer-
models/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2321470)

Tip

After completing the exercise, if you've finished exploring Azure AI services,
delete the Azure resources that you created during the exercise.

* * *"
205,Develop AI information extraction solutions in Azure AI-3002 - Training,Use prebuilt Document intelligence models,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

Choose the best response for each of the questions below.

## Check your knowledge

1.

You have a large set of documents with varying structures that contain
customer name and address information. You want to extract entities for each
customer. Which prebuilt model should you use?

Read model.

General document model.

ID document model.

2.

You are using the prebuilt layout model to analyze a document with many
checkboxes. You want to find out whether each box is checked or empty. What
object should you use in the returned JSON code?

Selection marks.

Bounding boxes.

Confidence indicators.

3.

You submit a Word document to the Azure AI Document Intelligence general
document model for analysis but you receive an error. The file is A4 size,
contains 1 MB of data, and is not password-protected. How should you resolve
the error?

Change from the free tier to the standard tier.

Submit the document one page at a time.

Convert the document to PDF format.

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
206,Develop AI information extraction solutions in Azure AI-3002 - Training,Use prebuilt Document intelligence models,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/use-prebuilt-form-recognizer-models/7-summary
207,Develop AI information extraction solutions in Azure AI-3002 - Training,Extract data from forms with Azure Document intelligence,Introduction,"# Introduction

Completed 100 XP

  * 3 minutes

Forms are used to communicate information in every industry, every day. Many
people still manually extract data from forms to exchange information.

![Screenshot of person handing a form to another person.](../../wwl-data-
ai/work-form-recognizer/media/introduction-image.png)

Consider some of the instances when a person needs to process form data:

  * When filing claims
  * When enrolling new patients in an online management system
  * When entering data from receipts to an expense report
  * When reviewing an operations report for anomalies
  * When selecting data from a report to give to a stakeholder

Without AI services, people need to manually sort through form documents to
identify important information and then manually reenter data to record it.
Some may also need to complete these tasks in real-time with a customer.

Azure Document Intelligence services provide the building blocks for
automation by using intelligent services to extract data at scale and with
accuracy. _Azure Document Intelligence_ is a Vision API that extracts key-
value pairs and table data from form documents.

**Uses of the Azure Document Intelligence service include** :

  * Process automation
  * Knowledge mining
  * Industry-specific applications

**In this module, you'll learn how to** :

  * Identify how Azure Document Intelligence's document analysis, prebuilt, and custom models can automate processes
  * Use Azure Document Intelligence's Optical Character Recognition (OCR) capabilities with SDKs and REST API
  * Develop and test a custom Azure Document Intelligence model

To complete this module, you'll need a Microsoft Azure subscription. If you
don't already have one, you can sign up for a free trial at
[https://azure.microsoft.com](https://azure.microsoft.com/).

* * *"
208,Develop AI information extraction solutions in Azure AI-3002 - Training,Extract data from forms with Azure Document intelligence,What is Azure Document Intelligence?,"# What is Azure Document Intelligence?

Completed 100 XP

  * 4 minutes

**Azure Document Intelligence** is one of many _Azure AI Services_ , cloud-
based artificial intelligence (AI) services with REST APIs and client library
SDKs that can be used to build intelligence into your applications.

Azure Document Intelligence uses Optical Character Recognition (OCR)
capabilities and deep learning models to extract text, key-value pairs,
selection marks, and tables from documents.

![Screenshot of how OCR works.](../../wwl-data-ai/work-form-
recognizer/media/how-optical-character-recognition-works.png)

OCR captures document structure by creating bounding boxes around detected
objects in an image. The locations of the bounding boxes are recorded as
coordinates in relation to the rest of the page. Azure Document Intelligence
services return bounding box data and other information in a structured form
with the relationships from the original file.

![Screenshot of JSON output sample.](../../wwl-data-ai/work-form-
recognizer/media/json-output-sample.png)

To build a high-accuracy model from scratch, people need to build deep
learning models, use a large amount of compute resources, and face long model
training times. These factors could make a project infeasible. Azure Document
Intelligence provides underlying models that have been trained on thousands of
form examples. The underlying models enable you to do high-accuracy data
extraction from your forms with little to no model training.

## Azure Document Intelligence service components

Azure Document Intelligence is composed of the following services:

  * **Document analysis models** : which take an input of JPEG, PNG, PDF, and TIFF files and return a JSON file with the location of text in bounding boxes, text content, tables, selection marks (also known as checkboxes or radio buttons), and document structure.

  * **Prebuilt models** : which detect and extract information from document images and return the extracted data in a structured JSON output. Azure Document Intelligence currently supports prebuilt models for several forms, including:

    * W-2 forms
    * Invoices
    * Receipts
    * ID documents
    * Business cards
  * **Custom models** : custom models extract data from forms specific to your business. Custom models can be trained through the [Azure Document Intelligence Studio](https://formrecognizer.appliedai.azure.com/studio).

Note

Some Azure Document Intelligence features are in preview, as of the time this
content was authored, and as a result, features and usage details might
change. You should refer to the [official
page](https://learn.microsoft.com/en-us/azure/ai-services/document-
intelligence/overview) for up-to-date information.

## Access services

You can access Azure Document Intelligence services in several ways. These
options include using:

  * A REST API
  * Client library SDKs
  * Azure Document Intelligence Studio
  * Azure AI Foundry

Tip

This module's exercise focuses on the Python and .NET SDKs. The underlying
REST services can be used by any language.

Check out the [documentation](/en-us/azure/ai-services/document-
intelligence/quickstarts/get-started-sdks-rest-api) for quick start guides on
all the available SDKs and the REST API.

* * *"
209,Develop AI information extraction solutions in Azure AI-3002 - Training,Extract data from forms with Azure Document intelligence,Get started with Azure Document Intelligence,"# Get started with Azure Document Intelligence

Completed 100 XP

  * 3 minutes

To start a project with Azure Document Intelligence services, you need an
Azure resource and selection of form files for data extraction.

## Subscribe to a resource

You can access Azure Document Intelligence services via:

  * An **Azure AI Service resource** : a multi-service subscription key (used across multiple Azure AI Services)

**OR**

  * An **Azure Document Intelligence resource** : a single-service subscription key (used only with a specific Azure AI Service)

Note

Create an Azure AI Services resource if you plan to access multiple Azure AI
services under a single endpoint/key. For Azure Document Intelligence access
only, create an Azure Document Intelligence resource. Note that you need a
single-service resource if you intend to use Microsoft Entra authentication.

You can subscribe to a service in the Azure portal or with the Azure Command
Line Interface (CLI). You can learn more about the CLI commands [here](/en-
us/cli/azure/cognitiveservices/account#commands).

## Understand Azure Document Intelligence file input requirements

Azure Document Intelligence works on input documents that meet these
requirements:

  * Format must be JPG, PNG, BMP, PDF (text or scanned), or TIFF.
  * The file size must be less than 500 MB for paid (S0) tier and 4 MB for free (F0) tier.
  * Image dimensions must be between 50 x 50 pixels and 10,000 x 10,000 pixels.
  * The total size of the training data set must be 500 pages or less.

More input requirements can be found in the [documentation](/en-
us/azure/cognitive-services/form-recognizer/overview) for specific models.

## Decide what component of Azure Document Intelligence to use

After you collect your files, decide what you need to accomplish.

Expand table

**Use case** | Recommended features to use  
---|---  
Use OCR capabilities to capture document analysis | Use the [Layout
model](/en-us/azure/applied-ai-services/form-recognizer/concept-model-
overview#layout), [Read model](/en-us/azure/applied-ai-services/form-
recognizer/concept-model-overview#read-preview), or [General Document
model](/en-us/azure/applied-ai-services/form-recognizer/concept-model-
overview#general-document-preview).  
Create an application that extracts data from W-2s, Invoices, Receipts, ID
documents, Health insurance, vaccination, and business cards | Use a _prebuilt
model_. These models don't need to be trained. Azure Document Intelligence
services analyze the documents and return a JSON output.  
Create an application to extract data from your industry-specific forms |
Create a custom model. This model needs to be trained on sample documents.
After you train the custom model, it can analyze new documents and return a
JSON output.  
  
* * *"
210,Develop AI information extraction solutions in Azure AI-3002 - Training,Extract data from forms with Azure Document intelligence,Train custom models,"# Train custom models

Completed 100 XP

  * 3 minutes

Azure's Azure Document Intelligence service supports supervised machine
learning. You can train custom models and create composite models with form
documents _and_ JSON documents that contain labeled fields.

![Screenshot of a sample form document needed for custom model
training.](../../wwl-data-ai/work-form-recognizer/media/labeled-form-
documents.png)

To train a custom model:

  1. Store sample forms in an Azure blob container, along with JSON files containing layout and label field information. 
    * You can generate an **ocr.json** file for each sample form using the Azure Document Intelligence's **Analyze document** function. Additionally, you need a single **fields.json** file describing the fields you want to extract, and a **labels.json** file for each sample form mapping the fields to their location in that form.
  2. Generate a shared access security (SAS) URL for the container.
  3. Use the **Build model** REST API function (or equivalent SDK method).
  4. Use the **Get model** REST API function (or equivalent SDK method) to get the trained **model ID**.

**OR**

  5. Use the Azure Document Intelligence Studio to label and train. There are two types of underlying models for custom forms _custom template models_ or _custom neural models_. 
    * **Custom template models** accurately extract labeled key-value pairs, selection marks, tables, regions, and signatures from documents. Training only takes a few minutes, and more than 100 languages are supported.
    * **Custom neural models** are deep learned models that combine layout and language features to accurately extract labeled fields from documents.This model is best for semi-structured or unstructured documents.

* * *"
211,Develop AI information extraction solutions in Azure AI-3002 - Training,Extract data from forms with Azure Document intelligence,Use Azure Document Intelligence models,"# Use Azure Document Intelligence models

Completed 100 XP

  * 4 minutes

## Using the API

To extract form data using a custom model, use the **analyze document**
function of either a supported SDK, or the REST API, while supplying model ID
(generated during model training). This function starts the form analysis.
which you can then request the result to get the analysis.

Example code to call your model:

**C#**

C# Copy

    
    
    string endpoint = ""<endpoint>"";
    string apiKey = ""<apiKey>"";
    AzureKeyCredential credential = new AzureKeyCredential(apiKey);
    DocumentAnalysisClient client = new DocumentAnalysisClient(new Uri(endpoint), credential);
    
    string modelId = ""<modelId>"";
    Uri fileUri = new Uri(""<fileUri>"");
    
    AnalyzeDocumentOperation operation = await client.AnalyzeDocumentFromUriAsync(WaitUntil.Completed, modelId, fileUri);
    AnalyzeResult result = operation.Value;
    

**Python**

Python Copy

    
    
    endpoint = ""YOUR_DOC_INTELLIGENCE_ENDPOINT""
    key = ""YOUR_DOC_INTELLIGENCE_KEY""
    
    model_id = ""YOUR_CUSTOM_BUILT_MODEL_ID""
    formUrl = ""YOUR_DOCUMENT""
    
    document_analysis_client = DocumentAnalysisClient(
        endpoint=endpoint, credential=AzureKeyCredential(key)
    )
    
    # Make sure your document's type is included in the list of document types the custom model can analyze
    task = document_analysis_client.begin_analyze_document_from_url(model_id, formUrl)
    result = task.result()
    

A successful JSON response contains **analyzeResult** that contains the
content extracted and an array of pages containing information about the
document content.

Example **analyze document** JSON response:

JSON Copy

    
    
    {
    	""status"": ""succeeded"",
    	""createdDateTime"": ""2023-10-18T23:39:50Z"",
    	""lastUpdatedDateTime"": ""2023-10-18T23:39:54Z"",
    	""analyzeResult"": {
    		""apiVersion"": ""2022-08-31"",
    		""modelId"": ""DocIntelModel"",
    		""stringIndexType"": ""utf16CodeUnit"",
    		""content"": ""Purchase Order\nHero Limited\nCompany Phone: 555-348-6512 Website: www.herolimited.com Email: accounts@herolimited.com\nPurchase Order\nDated As: 12/20/2020 Purchase Order #: 948284\nShipped To Vendor Name: Balozi Khamisi Company Name: Higgly Wiggly Books Address: 938 NE Burner Road Boulder City, CO 92848 Phone: 938-294-2949\nShipped From Name: Kidane Tsehaye Company Name: Jupiter Book Supply Address: 383 N Kinnick Road Seattle, WA 38383\nPhone: 932-299-0292\nDetails\nQuantity\nUnit Price\nTotal\nBindings\n20\n1.00\n20.00\nCovers Small\n20\n1.00\n20.00\nFeather Bookmark\n20\n5.00\n100.00\nCopper Swirl Marker\n20\n5.00\n100.00\nSUBTOTAL\n$140.00\nTAX\n$4.00\nTOTAL\n$144.00\nKidane Tsehaye\nManager\nKidane Tsehaye\nAdditional Notes: Do not Jostle Box. Unpack carefully. Enjoy. Jupiter Book Supply will refund you 50% per book if returned within 60 days of reading and offer you 25% off you next total purchase."",
    		""pages"": [
    			{
    				""pageNumber"": 1,
    				""angle"": 0,
    				""width"": 1159,
    				""height"": 1486,
    				""unit"": ""pixel"",
    				""words"": [
    					{
    						""content"": ""Purchase"",
    						""polygon"": [
    							89,
    							90,
    							174,
    							91,
    							174,
    							112,
    							88,
    							112
    						],
    						""confidence"": 0.996,
    						""span"": {
    							""offset"": 0,
    							""length"": 8
    						}
    					},
    					{
    						""content"": ""Order"",
    						""polygon"": [
    							178,
    							91,
    							237,
    							91,
    							236,
    							113,
    							178,
    							112
    						],
    						""confidence"": 0.997,
    						""span"": {
    							""offset"": 9,
    							""length"": 5
    						}
    					},
                        ...
    

Explore the documentation for [supported language
quickstarts](https://learn.microsoft.com/en-us/azure/ai-services/document-
intelligence/quickstarts/get-started-sdks-rest-api).

## Understanding confidence scores

If the confidence values of the **analyzeResult** are low, try to improve the
quality of your input documents.

You want to make sure that the form you're analyzing has a similar appearance
to forms in the training set if the confidence values are low. If the form
appearance varies, consider training more than one model, with each model
focused on one form format.

Depending on the use case, you might find that a confidence score of 80% or
higher is acceptable for a low-risk application. For more sensitive cases,
like reading medical records or billing statements, a score of 100% is
recommended.

* * *"
212,Develop AI information extraction solutions in Azure AI-3002 - Training,Extract data from forms with Azure Document intelligence,Use the Azure Document Intelligence Studio,"# Use the Azure Document Intelligence Studio

Completed 100 XP

  * 3 minutes

In addition to SDKs and the REST API, Azure Document Intelligence services can
be accessed through a user interface called the Azure Document Intelligence
Studio, an online tool for visually exploring, understanding, and integrating
features from the Azure Document Intelligence service. The Studio can be used
to analyze form layouts, extract data from prebuilt models, and train custom
models.

![Gif of Azure Document Intelligence Studio capabilities.](../../wwl-data-
ai/work-form-recognizer/media/doc-intelligence-studio.png)

The Azure Document Intelligence Studio currently supports the following
projects:

  * **Document analysis models**
    * Read: Extract printed and handwritten text lines, words, locations, and detected languages from documents and images.
    * Layout: Extract text, tables, selection marks, and structure information from documents (PDF and TIFF) and images (JPG, PNG, and BMP).
    * General Documents: Extract key-value pairs, selection marks, and entities from documents.
  * **Prebuilt models**
  * **Custom models**

### Build Document analysis model projects

To extract text, tables, structure, key-value pairs, and named entities with
document analysis models:

  * Create an Azure Document Intelligence or Azure AI Services resource
  * Select either ""Read"", ""Layout"", or ""General Documents"" under the Document analysis models category
  * Analyze your document. You'll need your Azure Document Intelligence or Azure AI service endpoint and key.

### Build prebuilt model projects

To extract data from common forms with prebuilt models:

  * Create an Azure Document Intelligence or Azure AI Services resource
  * Select one of the ""prebuilt models"" including W-2s, Invoices, Receipts, ID documents, Health insurance, vaccination, and business cards.
  * Analyze your document. You'll need your Azure Document Intelligence or Azure AI service endpoint and key.

### Build custom model projects

You can use Azure Document Intelligence Studio's custom service for the entire
process of training and testing custom models.

When you use Azure Document Intelligence Studio to build custom models, the
**ocr.json** files, **labels.json** files, and **fields.json** file needed for
training are automatically created and stored in your storage account.

To train a custom model and use it to extract data with custom models:

  * Create an Azure Document Intelligence or Azure AI Services resource
  * Collect at least 5-6 sample forms for training and upload them to your storage account container.
  * Configure cross-domain resource sharing (CORS). CORS enables Azure Document Intelligence Studio to store labeled files in your storage container.
  * Create a custom model project in Azure Document Intelligence Studio. You'll need to provide configurations linking your storage container and Azure Document Intelligence or Azure AI Service resource to the project.
  * Use Azure Document Intelligence Studio to apply labels to text.
  * Train your model. Once the model is trained, you'll receive a Model ID and Average Accuracy for tags.
  * Test your model by analyzing a new form that wasn't used in training.

* * *"
213,Develop AI information extraction solutions in Azure AI-3002 - Training,Extract data from forms with Azure Document intelligence,Exercise - Extract data from custom forms,"# Exercise - Extract data from custom forms

Completed 100 XP

  * 25 minutes

In this exercise, you'll use the Azure Document Intelligence service to train
and test a custom model.

Note

To complete this lab, you need an **[Azure
subscription](https://azure.microsoft.com/free)** in which you have
administrative access.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/work-form-
recognizer/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2322032)

Tip

After completing the exercise, if you've finished exploring Azure AI services,
delete the Azure resources that you created during the exercise.

* * *"
214,Develop AI information extraction solutions in Azure AI-3002 - Training,Extract data from forms with Azure Document intelligence,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

In this module, you have learned how to use the Azure Document Intelligence
service to extract data from forms.

Consider the following review questions to check your understanding of the
topics discussed in this module.

1.

A person plans to use an Azure Document Intelligence prebuilt invoice model.
To extract document data using the model and REST API language, what are two
calls they need to make to the API?

Train Model and Get Model Labels

Analyze Invoice and Get Analyze Invoice Result

Create Azure Document Intelligence and Get Analyze Invoice Result

2.

A person needs to build an application that submits expense claims and
extracts the merchant, date, and total from scanned receipts. What's the
**best** way to build the application?

Use the Read API of the Computer Vision service.

Use Azure Document Intelligence's prebuilt receipts model

Use Azure Document Intelligence's Layout service

3.

A person is building a custom model with Azure Document Intelligence services.
What is required to train a model?

Along with the form to analyze, JSON files need to be provided.

Training must be done through language-specific SDKs.

Nothing else is required.

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
215,Develop AI information extraction solutions in Azure AI-3002 - Training,Extract data from forms with Azure Document intelligence,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/work-form-recognizer/11-summary
216,Develop AI information extraction solutions in Azure AI-3002 - Training,Create a knowledge mining solution with Azure AI Search,Introduction,"# Introduction

Completed 100 XP

  * 1 minute

Azure AI Search is a powerful cloud-based service that enables you to extract,
enrich, and explore information from a wide variety of data sources. In this
module, you'll learn how to build intelligent search and knowledge mining
solutions using Azure AI Search.

We'll start by introducing the core concepts of Azure AI Search, including how
to connect to data sources and create indexes. You'll discover how the
indexing process works, and how AI skills can be used to enrich your data with
insights such as language detection, key phrase extraction, and image
analysis.

After learning how to implement an index, you'll explore how to query and
filter results using full-text search. Finally, you'll see how to use the
knowledge store to persist enriched data for further analysis and integration
with other systems.

* * *"
217,Develop AI information extraction solutions in Azure AI-3002 - Training,Create a knowledge mining solution with Azure AI Search,What is Azure AI Search?,"# What is Azure AI Search?

Completed 100 XP

  * 3 minutes

Azure AI Search provides a cloud-based solution for indexing and querying a
wide range of data sources, and creating comprehensive and high-scale search
solutions. It provides the infrastructure and tools to create search solutions
that extract data from structured, semi-structured, and non-structured
documents and other data sources.

With Azure AI Search, you can:

  * Index documents and data from a range of sources.
  * Use AI skills to enrich index data.
  * Store extracted insights in a knowledge store for analysis and integration.

Azure AI Search indexes contain insights extracted from your data; which can
include text inferred or read using OCR from images, entities and key phrases
detection through text analytics, and other derived information based on AI
skills that are integrated into the indexing process.

![Diagram of Azure AI Search supporting multiple applications.](../../wwl-
data-ai/ai-knowldge-mining/media/ai-search.png)

Azure AI search has many applications, including:

  * Implementing an _enterprise search_ solution to help employees or customers find information in websites or applications.
  * Supporting _retrieval augmented generation_ (RAG) in generative AI applications by using vector-based indexes for prompt grounding data.
  * Creating _knowledge mining_ solutions in which the indexing process is used to infer insights and extract granular data assets from documents to support data analytics.

In this module, we'll focus on Azure AI Search in knowledge mining scenarios.

* * *"
218,Develop AI information extraction solutions in Azure AI-3002 - Training,Create a knowledge mining solution with Azure AI Search,Extract data with an indexer,"# Extract data with an indexer

Completed 100 XP

  * 5 minutes

At the heart of Azure AI Search solutions is the creation of an _index_ An
index contains your searchable content and is created and updated,
unsurprisingly, by an _indexer_.

![Diagram of an indexer extracting documents from a source and creating an
index.](../../wwl-data-ai/ai-knowldge-mining/media/indexer.png)

The indexing process starts with a data source: the storage location of your
original data artifacts; for example, an Azure blob store container full of
documents, a database, or some other store.

The Indexer automates the extraction and indexing of data _fields_ through an
_enrichment pipeline_ , in which it applies _document cracking_ to extract the
contents of the source documents and applies incremental steps to create a
hierarchical (JSON-based) document with the required fields for the index
definition.

The result is a populated index, which can be queried to return specified
fields from documents that match the query criteria.

## How documents are constructed during indexing

The indexing process works by creating a document for each indexed entity.
During indexing, an _enrichment pipeline_ iteratively builds the documents
that combine metadata from the data source with enriched fields extracted or
generated by _skills_. You can think of each indexed document as a JSON
structure, which initially consists of a document with the index fields you
have mapped to fields extracted directly from the source data, like this:

  * **document**
    * **metadata_storage_name**
    * **metadata_author**
    * **content**

When the documents in the data source contain images, you can configure the
indexer to extract the image data and place each image in a
**normalized_images** collection, like this:

  * **document**
    * **metadata_storage_name**
    * **metadata_author**
    * **content**
    * **normalized_images**
      * **image0**
      * **image1**

Normalizing the image data in this way enables you to use the collection of
images as an input for skills that extract information from image data.

Each skill adds fields to the **document** , so for example a skill that
detects the _language_ in which a document is written might store its output
in a **language** field, like this:

  * **document**
    * **metadata_storage_name**
    * **metadata_author**
    * **content**
    * **normalized_images**
      * **image0**
      * **image1**
    * **language**

The document is structured hierarchically, and the skills are applied to a
specific _context_ within the hierarchy, enabling you to run the skill for
each item at a particular level of the document. For example, you could run an
optical character recognition ( _OCR_ ) skill for each image in the normalized
images collection to extract any text they contain:

  * **document**
    * **metadata_storage_name**
    * **metadata_author**
    * **content**
    * **normalized_images**
      * **image0**
        * **Text**
      * **image1**
        * **Text**
    * **language**

The output fields from each skill can be used as inputs for other skills later
in the pipeline, which in turn store _their_ outputs in the document
structure. For example, we could use a _merge_ skill to combine the original
text content with the text extracted from each image to create a new
**merged_content** field that contains all of the text in the document,
including image text.

  * **document**
    * **metadata_storage_name**
    * **metadata_author**
    * **content**
    * **normalized_images**
      * **image0**
        * **Text**
      * **image1**
        * **Text**
    * **language**
    * **merged_content**

The fields in the final document structure at the end of the pipeline are
mapped to index fields by the indexer in one of two ways:

  * Fields extracted directly from the source data are all mapped to index fields. These mappings can be _implicit_ (fields are automatically mapped to in fields with the same name in the index) or _explicit_ (a mapping is defined to match a source field to an index field, often to rename the field to something more useful or to apply a function to the data value as it is mapped).
  * Output fields from the skills in the skillset are explicitly mapped from their hierarchical location in the output to the target field in the index.

* * *"
219,Develop AI information extraction solutions in Azure AI-3002 - Training,Create a knowledge mining solution with Azure AI Search,Enrich extracted data with AI skills,"# Enrich extracted data with AI skills

Completed 100 XP

  * 5 minutes

The enrichment pipeline that is orchestrated by an indexer uses a _skillset_
of AI skills to create AI-enriched fields. The indexer applies each skill in
order, refining the index document at each step.

![Diagram of an indexer using a skillset to enrich data.](../../wwl-data-
ai/ai-knowldge-mining/media/skillset.png)

## Built-in skills

Azure AI Search provides a collection of _built-in_ skills that you can
include in a skillset for your indexer. Built-in skills include functionality
from Azure AI services such as Azure AI Vision and Azure AI Language, enabling
you to apply enrichments such as:

  * Detecting the language that text is written in.
  * Detecting and extracting places, locations, and other entities in the text.
  * Determining and extracting key phrases within a body of text.
  * Translating text.
  * Identifying and extracting (or removing) personally identifiable information (PII) within the text.
  * Extracting text from images.
  * Generating captions and tags to describe images.

To use the built-in skills, your indexer must have access to an Azure AI
services resource. You can use a restricted Azure AI search resource that is
included in Azure AI Search (and which is limited to indexing 20 or fewer
documents) or you can attach an Azure AI services resource in your Azure
subscription (which must be in the same region as your Azure AI Search
resource).

## Custom skills

You can further extend the enrichment capabilities of your index by creating
_custom_ skills. As the name suggests, custom skills perform custom logic on
input data from your index document to return new field values that can be
incorporated into the index. Often, custom skills are ""wrappers"" around
services that are specifically designed to extract data from documents. For
example, you could implement a custom skill as an Azure Function, and use it
to pass data from your index document to an Azure AI Document Intelligence
model, which can extract fields from a form.

Tip

To learn more about using custom skills with Azure AI Search, see **[Add a
custom skill to an Azure AI Search enrichment pipeline](/en-
us/azure/search/cognitive-search-custom-skill-interface)** in the Azure AI
Search documentation.

* * *"
220,Develop AI information extraction solutions in Azure AI-3002 - Training,Create a knowledge mining solution with Azure AI Search,Search an index,"# Search an index

Completed 100 XP

  * 5 minutes

The index is the searchable result of the indexing process. It consists of a
collection of JSON documents, with fields that contain the values extracted
during indexing. Client applications can query the index to retrieve, filter,
and sort information.

![Diagram of a client searching an Azure AI Search index.](../../wwl-data-
ai/ai-knowldge-mining/media/search-index.png)

Each index field can be configured with the following attributes:

  * **key** : Fields that define a unique key for index records.
  * **searchable** : Fields that can be queried using full-text search.
  * **filterable** : Fields that can be included in filter expressions to return only documents that match specified constraints.
  * **sortable** : Fields that can be used to order the results.
  * **facetable** : Fields that can be used to determine values for _facets_ (user interface elements used to filter the results based on a list of known field values).
  * **retrievable** : Fields that can be included in search results ( _by default, all fields are retrievable unless this attribute is explicitly removed_ ).

## Full-text search

While you could retrieve index entries based on simple field value matching,
most search solutions use _full-text search_ semantics to query an index.

Full-text search describes search solutions that parse text-based document
contents to find query terms. Full-text search queries in Azure AI Search are
based on the _Lucene_ query syntax, which provides a rich set of query
operations for searching, filtering, and sorting data in indexes. Azure AI
Search supports two variants of the Lucene syntax:

  * **Simple** \- An intuitive syntax that makes it easy to perform basic searches that match literal query terms submitted by a user.
  * **Full** \- An extended syntax that supports complex filtering, regular expressions, and other more sophisticated queries.

Client applications submit queries to Azure AI Search by specifying a search
expression along with other parameters that determine how the expression is
evaluated and the results returned. Some common parameters submitted with a
query include:

  * **search** \- A search expression that includes the terms to be found.
  * **queryType** \- The Lucene syntax to be evaluated ( _simple_ or _full_ ).
  * **searchFields** \- The index fields to be searched.
  * **select** \- The fields to be included in the results.
  * **searchMode** \- Criteria for including results based on multiple search terms. For example, suppose you search an index of travel-related documents for _comfortable hotel_. A searchMode value of _Any_ returns documents that contain ""comfortable"", ""hotel"", or both; while a searchMode value of _All_ restricts results to documents that contain both ""comfortable"" and ""hotel"".

Query processing consists of four stages:

  1. _Query parsing_. The search expression is evaluated and reconstructed as a tree of appropriate subqueries. Subqueries might include _term queries_ (finding specific individual words in the search expression - for example _hotel_ ), _phrase queries_ (finding multi-term phrases specified in quotation marks in the search expression - for example, _""free parking""_ ), and _prefix queries_ (finding terms with a specified prefix - for example _air*_ , which would match _airway_ , _air-conditioning_ , and _airport_ ).
  2. _Lexical analysis_ \- The query terms are analyzed and refined based on linguistic rules. For example, text is converted to lower case and nonessential _stopwords_ (such as ""the"", ""a"", ""is"", and so on) are removed. Then words are converted to their _root_ form (for example, ""comfortable"" might be simplified to ""comfort"") and composite words are split into their constituent terms.
  3. _Document retrieval_ \- The query terms are matched against the indexed terms, and the set of matching documents is identified.
  4. _Scoring_ \- A relevance score is assigned to each result based on a term frequency/inverse document frequency (TF/IDF) calculation.

Tip

For more information about querying an index, and details about simple and
full syntax, see **[Query types and composition in Azure AI Search](/en-
us/azure/search/search-query-overview)** in the Azure AI Search documentation.

It's common in a search solution for users to want to refine query results by
filtering and sorting based on field values. Azure AI Search supports both of
these capabilities through the search query API.

## Filtering results

You can apply filters to queries in two ways:

  * By including filter criteria in a _simple_ **search** expression.
  * By providing an OData filter expression as a **$filter** parameter with a _full_ syntax search expression.

You can apply a filter to any _filterable_ field in the index.

For example, suppose you want to find documents containing the text _London_
that have an **author** field value of _Reviewer_.

You can achieve this result by submitting the following _simple_ **search**
expression:

Copy

    
    
    search=London+author='Reviewer'
    queryType=Simple
    

Alternatively, you can use an OData filter in a **$filter** parameter with a
_full_ Lucene search expression like this:

Copy

    
    
    search=London
    $filter=author eq 'Reviewer'
    queryType=Full
    

Note

OData **$filter** expressions are case-sensitive!

### Filtering with facets

_Facets_ are a useful way to present users with filtering criteria based on
field values in a result set. They work best when a field has a small number
of discrete values that can be displayed as links or options in the user
interface.

To use facets, you must specify _facetable_ fields for which you want to
retrieve the possible values in an initial query. For example, you could use
the following parameters to return all of the possible values for the
**author** field:

Copy

    
    
    search=*
    facet=author
    

The results from this query include a collection of discrete facet values that
you can display in the user interface for the user to select. Then in a
subsequent query, you can use the selected facet value to filter the results:

Copy

    
    
    search=*
    $filter=author eq 'selected-facet-value-here'
    

## Sorting results

By default, results are sorted based on the relevancy score assigned by the
query process, with the highest scoring matches listed first. However, you can
override this sort order by including an OData **orderby** parameter that
specifies one or more _sortable_ fields and a sort order ( _asc_ or _desc_ ).

For example, to sort the results so that the most recently modified documents
are listed first, you could use the following parameter values:

Copy

    
    
    search=*
    $orderby=last_modified desc
    

Tip

For more information about using filters, see **[Filters in Azure AI
Search](/en-us/azure/search/search-filters)** in the Azure AI Search
documentation.

* * *"
221,Develop AI information extraction solutions in Azure AI-3002 - Training,Create a knowledge mining solution with Azure AI Search,Persist extracted information in a knowledge store,"# Persist extracted information in a knowledge store

Completed 100 XP

  * 5 minutes

While the index might be considered the primary output from an indexing
process, the enriched data it contains might also be useful in other ways. For
example:

  * Since the index is essentially a collection of JSON objects, each representing an indexed record, it might be useful to export the objects as JSON files for integration into a data orchestration process for extract, transform, and load (ETL) operations.
  * You may want to normalize the index records into a relational schema of tables for analysis and reporting.
  * Having extracted embedded images from documents during the indexing process, you might want to save those images as files.

Azure AI Search supports these scenarios by enabling you to define a
_knowledge store_ in the skillset that encapsulates your enrichment pipeline.
The knowledge store consists of _projections_ of the enriched data, which can
be JSON objects, tables, or image files. When an indexer runs the pipeline to
create or update an index, the projections are generated and persisted in the
knowledge store.

![Diagram of an indexing process loading projections into a knowledge
store.](../../wwl-data-ai/ai-knowldge-mining/media/knowledge-store.png)

Tip

To learn more about using a knowledge store, see **[Knowledge store in Azure
AI Search](/en-us/azure/search/knowledge-store-concept-intro)** in the Azure
AI Search documentation.

* * *"
222,Develop AI information extraction solutions in Azure AI-3002 - Training,Create a knowledge mining solution with Azure AI Search,Exercise - Create a knowledge mining solution,"# Exercise - Create a knowledge mining solution

Completed 100 XP

  * 40 minutes

It's time to out what you've learned into practice!

In this exercise, you use Azure AI Search to extract and enrich information
from documents into a searchable index and a knowledge store.

Note

To complete this lab, you need an **[Azure
subscription](https://azure.microsoft.com/free)** in which you have
administrative access.

Launch the exercise and follow the instructions.

[![Button to launch exercise.](../../wwl-data-ai/ai-knowldge-
mining/media/launch-
exercise.png)](https://go.microsoft.com/fwlink/?linkid=2320469)

* * *"
223,Develop AI information extraction solutions in Azure AI-3002 - Training,Create a knowledge mining solution with Azure AI Search,Module assessment,"# Module assessment

Completed 200 XP

  * 3 minutes

## Check your knowledge

1.

Which component of an Azure AI Search solution is scheduled to extract and
enrich data to populate an index?

Indexer

Projection

Query

2.

Which service supports built-in AI skills in Azure AI Search?

Azure Functions

Azure AI services

Azure Cosmos DB

3.

Which kind of projection results in a relational data schema for extracted
fields?

File

Object

Table

Submit answers

You must answer all questions before checking your work.

You must answer all questions before checking your work.

* * *"
224,Develop AI information extraction solutions in Azure AI-3002 - Training,Create a knowledge mining solution with Azure AI Search,Summary,Couldn't extract content for url: https://learn.microsoft.com/en-us/training/modules/ai-knowldge-mining/9-summary
