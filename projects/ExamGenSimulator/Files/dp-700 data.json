[
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Ingest Data with Dataflows Gen2 in Microsoft Fabric",
        "topic": "Introduction",
        "text": "Microsoft Fabric offers a unified solution for data engineering, integration, and analytics. A crucial step in end-to-end analytics is data ingestion. Dataflows Gen2 are used to ingest and transform data from multiple sources, and then land the cleansed data to another destination. They can be incorporated into data pipelines for more complex activity orchestration, and also used as a data source in Power BI. Imagine you work for a retail company with stores across the globe. As a data engineer, you need to prepare and transform data from various sources into a format that is suitable for data analysis and reporting. The business requests a semantic model that consolidates disparate data sources from the different stores. Dataflows Gen2 allow you to prepare the data to ensure consistency, and then stage the data in the preferred destination. They also enable reuse and make it easy to update the data. Without a dataflow, you'd have to manually extract and transform the data from every source, which is time-consuming and prone to errors. In this module, we explain how to use Dataflows Gen2 in Microsoft Fabric to meet your data ingestion needs.",
        "links": null,
        "images": null,
        "videos": [
            {
                "name": "video1",
                "video_src": "https://go.microsoft.com/fwlink/?linkid=2331876"
            }
        ],
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Ingest Data with Dataflows Gen2 in Microsoft Fabric",
        "topic": "Understand Dataflows Gen2 in Microsoft Fabric",
        "text": "In our scenario, you need to develop a semantic model that can standardize the data and provide access to the business. By using Dataflows Gen2, you can connect to the various data sources, and then prep and transform the data. You can land the data directly into your lakehouse or use a data pipeline for other destinations. What is a dataflow? Dataflows are a type of cloud-based ETL ( Extract, Transform, Load ) tool for building and executing scalable data transformation processes. Dataflows Gen2 allow you to extract data from various sources, transform it using a wide range of transformation operations, and load it into a destination. Using Power Query Online also allows for a visual interface to perform these tasks. Fundamentally, a dataflow includes all of the transformations to reduce data prep time and then can be loaded into a new table, included in a data pipeline, or used as a data source by data analysts. How to use Dataflows Gen2 Traditionally, data engineers spend significant time extracting, transforming, and loading data into a consumable format for downstream analytics. The goal of Dataflows Gen2 is to provide an easy, reusable way to perform ETL tasks using Power Query Online. If you only choose to use a data pipeline, you copy data, then use your preferred coding language to extract, transform, and load the data. Alternatively, you can create a Dataflow Gen2 first to extract and transform the data. You can also load the data into a lakehouse, and other destinations. Now the business can easily consume the curated semantic model. Adding a data destination to your dataflow is optional, and the dataflow preserves all transformation steps. To perform other tasks or load data to a different destination after transformation, create a data pipeline and add the Dataflow Gen2 activity to your orchestration. Another option might be to use a data pipeline and Dataflow Gen2 for ELT (Extract, Load, Transform) process. For this order, you'd use a Pipeline to extract and load the data into your preferred destination, such as the lakehouse. Then you'd create a Dataflow Gen2 to connect to Lakehouse data to cleanse and transform data. In this case, you'd offer the Dataflow as a curated semantic model for data analysts to develop reports. Dataflows can be horizontally partitioned as well. Once you create a global dataflow, data analysts can use dataflows to create specialized semantic models for specific needs. Dataflows allow you to promote reusable ETL logic that prevents the need to create more connections to your data source. Dataflows offer a wide variety of transformations, and can be run manually, on a refresh schedule, or as part of a data pipeline orchestration. Tip Make your dataflow discoverable so data analysts can also connect to the dataflow through Power BI Desktop. This reduces the data preparation for report development. Benefits and limitations There's more than one way to ETL or ELT data in Microsoft Fabric. Consider the benefits and limitations for using Dataflows Gen2. Benefits: Extend data with consistent data, such as a standard date dimension table. Allow self-service users access to a subset of data warehouse separately. Optimize performance with dataflows, which enable extracting data once for reuse, reducing data refresh time for slower sources. Simplify data source complexity by only exposing dataflows to larger analyst groups. Ensure consistency and quality of data by enabling users to clean and transform data before loading it to a destination. Simplify data integration by providing a low-code interface that ingests data from various sources. Limitations: Dataflows aren't a replacement for a data warehouse. Row-level security isn't supported. Fabric capacity workspace is required.",
        "links": null,
        "images": null,
        "videos": [
            {
                "name": "video1",
                "video_src": "https://go.microsoft.com/fwlink/?linkid=2331976"
            }
        ],
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Ingest Data with Dataflows Gen2 in Microsoft Fabric",
        "topic": "Explore Dataflows Gen2 in Microsoft Fabric",
        "text": "In Microsoft Fabric, you can create a Dataflow Gen2 in the Data Factory workload or Power BI workspace, or directly in the lakehouse. Since our scenario is focused on data ingestion, let's look at the Data Factory workload experience. Dataflows Gen2 use Power Query Online to visualize transformations. See an overview of the interface: 1. Power Query ribbon Dataflows Gen2 support a wide variety of data source connectors. Common sources include cloud and on-premises relational databases, Excel or flat files, SharePoint, SalesForce, Spark, and Fabric lakehouses. Then there are numerous data transformations possible, such as: Filter and Sort rows Pivot and Unpivot Merge and Append queries Split and Conditional split Replace values and Remove duplicates Add, Rename, Reorder, or Delete columns Rank and Percentage calculator Choose Top N and Bottom N You can also create and manage data source connections, manage parameters, and configure the default data destination in this ribbon. 2. Queries pane The Queries pane shows you the different data sources - now called queries . These queries are called tables when loaded to your data store. You can duplicate or reference a query if you need multiple copies of the same data, such as creating a star schema and splitting data into separate, smaller tables. You can also disable the load of a query, in case you only need the one-time import. 3. Diagram view The Diagram View allows you to visually see how the data sources are connected and the different applied transformations. For example, your dataflow connects to a data source, duplicates the query, removes columns from the source query, then unpivots the duplicate query. Each query is represented as a shape with all of the applied transformations and connected by a line for the duplicate query. You can turn this view on or off. 4. Data Preview pane The Data Preview pane only shows a subset of data to allow you to see which transformations you should make and how they affect the data. You can also interact with the preview pane by dragging and dropping columns to change order or right-clicking on columns to filter or make changes. The data preview shows all of your transformations for the selected query. 5. Query Settings pane The Query Settings pane includes the Applied Steps . Each transformation is represented as a step, some of which are automatically applied when you connect the data source. Depending on the complexity of the transformations, you might have several applied steps for each query. Most steps have a gear icon that allows you to modify the step, otherwise you must delete and repeat the transformation. Each step also has a contextual menu when you right-click so you can rename, reorder, or delete the steps. You can also view the data source query when connecting to a data source that supports query folding. While this visual interface is helpful, you can also view the M code through Advanced editor . In the Query settings pane, you can see a Data Destination option to land your data in one of the following locations in your Fabric environment: Lakehouse Warehouse SQL database You can also load your dataflow to Azure SQL database, Azure Data Explorer, or Azure Synapse Analytics. Dataflows Gen2 provide a low-to-no-code solution to ingest, transform, and load data into your Fabric data stores. Power BI developers are familiar and can quickly begin to perform transformations upstream to improve performance for their reports. Note For more information, see the Power Query documentation to optimize your dataflows.",
        "links": [
            "https://learn.microsoft.com/en-us/power-query/"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-dataflow-gen-2-fabric/media/power-query-online-overview.png",
                "image_alt": "Screenshot of the Power Query Online interface."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-dataflow-gen-2-fabric/media/power-query-advanced-editor.png",
                "image_alt": "Screenshot of the advanced editor with sample code"
            }
        ],
        "videos": [
            {
                "name": "video1",
                "video_src": "https://go.microsoft.com/fwlink/?linkid=2331875"
            }
        ],
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Ingest Data with Dataflows Gen2 in Microsoft Fabric",
        "topic": "Integrate Dataflows Gen2 and Pipelines in Microsoft Fabric",
        "text": "Dataflows Gen2 provide an excellent option for data transformations in Microsoft Fabric. The combination of dataflows and pipelines is useful when you need to perform additional operations on the transformed data. Data pipelines are a common concept in data engineering and offer a wide variety of activities to orchestrate. Some common activities include: Copy data Incorporate Dataflow Add Notebook Get metadata Execute a script or stored procedure Pipelines provide a visual way to complete activities in a specific order. You can use a dataflow for data ingestion, transformation, and landing into a Fabric data store. Then incorporate the dataflow into a pipeline to orchestrate extra activities, like execute scripts or stored procedures after the dataflow has completed. Pipelines can also be scheduled or activated by a trigger to run your dataflow. By using a pipeline to run your dataflow, you can have the data refreshed when you need it instead of having to manually run the dataflow. When you're dealing with enterprise or frequently changing data, automation allows you to focus on other responsibilities.",
        "links": null,
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-dataflow-gen-2-fabric/media/dataflow-schedule-pipeline.png",
                "image_alt": "Screenshot of the pipeline schedule window for a dataflow."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Ingest Data with Dataflows Gen2 in Microsoft Fabric",
        "topic": "Exercise - Create and use a Dataflow Gen2 in Microsoft Fabric",
        "text": "In this exercise, you'll use a Dataflow Gen2 to load transformed data into a lakehouse and add a dataflow to a pipeline. Note You need a Microsoft Fabric trial license with the Fabric preview enabled in your tenant. See Getting started with Fabric to enable your Fabric trial license. Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://go.microsoft.com/fwlink/?linkid=2259607"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Ingest Data with Dataflows Gen2 in Microsoft Fabric",
        "topic": "Summary",
        "text": "In this module, we walked through a scenario where both data engineers need to ingest, transform, and load data into a Fabric data store such as a lakehouse. We also identified data analyst needs to perform transformations closer to the data source to support Power BI report development. With Microsoft Fabric, you can create Dataflows Gen2 to perform data integration for your lakehouse, and optionally include the dataflow in a Data Pipeline as well. You learned about Dataflows Gen2 and how to use them as part of your data integration process. Power Query Online offers a visual interface to perform complex data transformations without writing any code. To learn more about data integration, see the Data Factory in Microsoft Fabric documentation.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-factory/"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Orchestrate processes and data movement with Microsoft Fabric",
        "topic": "Introduction",
        "text": "Data pipelines define a sequence of activities that orchestrate an overall process, usually by extracting data from one or more sources and loading it into a destination; often transforming it along the way. Pipelines are commonly used to automate extract , transform , and load (ETL) processes that ingest transactional data from operational data stores into an analytical data store, such as a lakehouse, data warehouse, or SQL database. If you're already familiar with Azure Data Factory, then data pipelines in Microsoft Fabric will be immediately familiar. They use the same architecture of connected activities to define a process that can include multiple kinds of data processing tasks and control flow logic. You can run pipelines interactively in the Microsoft Fabric user interface, or schedule them to run automatically.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Orchestrate processes and data movement with Microsoft Fabric",
        "topic": "Understand pipelines",
        "text": "Pipelines in Microsoft Fabric encapsulate a sequence of activities that perform data movement and processing tasks. You can use a pipeline to define data transfer and transformation activities, and orchestrate these activities through control flow activities that manage branching, looping, and other typical processing logic. The graphical pipeline canvas in the Fabric user interface enables you to build complex pipelines with minimal or no coding required. Core pipeline concepts Before building pipelines in Microsoft Fabric, you should understand a few core concepts. Activities Activities are the executable tasks in a pipeline. You can define a flow of activities by connecting them in a sequence. The outcome of a particular activity (success, failure, or completion) can be used to direct the flow to the next activity in the sequence. There are two broad categories of activity in a pipeline. Data transformation activities - activities that encapsulate data transfer operations, including simple Copy Data activities that extract data from a source and load it to a destination, and more complex Data Flow activities that encapsulate dataflows (Gen2) that apply transformations to the data as it is transferred. Other data transformation activities include Notebook activities to run a Spark notebook, Stored procedure activities to run SQL code, Delete data activities to delete existing data, and others. In OneLake, you can configure the destination to a lakehouse, warehouse, SQL database, or other options. Control flow activities - activities that you can use to implement loops, conditional branching, or manage variable and parameter values. The wide range of control flow activities enables you to implement complex pipeline logic to orchestrate data ingestion and transformation flow. Tip For details about the complete set of pipeline activities available in Microsoft Fabric, see Activity overview in the Microsoft Fabric documentation. Parameters Pipelines can be parameterized, enabling you to provide specific values to be used each time a pipeline is run. For example, you might want to use a pipeline to save ingested data in a folder, but have the flexibility to specify a folder name each time the pipeline is run. Using parameters increases the reusability of your pipelines, enabling you to create flexible data ingestion and transformation processes. Pipeline runs Each time a pipeline is executed, a data pipeline run is initiated. Runs can be initiated on-demand in the Fabric user interface or scheduled to start at a specific frequency. Use the unique run ID to review run details to confirm they completed successfully and investigate the specific settings used for each execution.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-factory/activity-overview"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-data-factory-pipelines-fabric/media/pipeline.png",
                "image_alt": "Screenshot of a pipeline in Microsoft Fabric."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Orchestrate processes and data movement with Microsoft Fabric",
        "topic": "Use the Copy Data activity",
        "text": "The Copy Data activity is one of the most common uses of a data pipeline. Many pipelines consist of a single Copy Data activity that is used to ingest data from an external source into a lakehouse file or table. You can also combine the Copy Data activity with other activities to create a repeatable data ingestion process - for example by using a Delete data activity to remove existing data, a Copy Data activity to replace the deleted data with a file containing data from an external source, and a Notebook activity to run Spark code that transforms the data in the file and loads it into a table. The Copy Data tool When you add a Copy Data activity to a pipeline, a graphical tool takes you through the steps required to configure the data source and destination for the copy operation. A wide range of source connections is supported, making it possible to ingest data from most common sources. In OneLake, this includes support for lakehouse, warehouse, SQL Database, and others. Copy Data activity settings After you've added a Copy Data activity to a pipeline, you can select it in the pipeline canvas and edit its settings in the pane underneath. When to use the Copy Data activity Use the Copy Data activity when you need to copy data directly between a supported source and destination without applying any transformations, or when you want to import the raw data and apply transformations in later pipeline activities. If you need to apply transformations to the data as it is ingested, or merge data from multiple sources, consider using a Data Flow activity to run a dataflow (Gen2). You can use the Power Query user interface to define a dataflow (Gen2) that includes multiple transformation steps, and include it in a pipeline. Tip To learn more about Dataflow (Gen2) in Microsoft Fabric to ingest data, consider completing the Ingest Data with Dataflows Gen2 in Microsoft Fabric module.",
        "links": [
            "https://learn.microsoft.com/en-us/training/modules/use-dataflow-gen-2-fabric"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-data-factory-pipelines-fabric/media/copy-data.png",
                "image_alt": "Screenshot of the Copy Data tool in Microsoft Fabric."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-data-factory-pipelines-fabric/media/copy-sql-database.png",
                "image_alt": "Screenshot of the Copy Data tool showing the SQL Database support in Microsoft Fabric."
            },
            {
                "image_name": "image3",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-data-factory-pipelines-fabric/media/copy-data-activity.png",
                "image_alt": "Screenshot of a Copy Data activity in Microsoft Fabric."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Orchestrate processes and data movement with Microsoft Fabric",
        "topic": "Use pipeline templates",
        "text": "You can define pipelines from any combination of activities you choose, enabling to create custom data ingestion and transformation processes to meet your specific needs. However, there are many common pipeline scenarios for which Microsoft Fabric includes predefined pipeline templates that you can use and customize as required. To create a pipeline based on a template, select the Templates tile in a new pipeline as shown here. Selecting this option displays a selection of pipeline templates, as shown here. You can select the most appropriate template for your needs, and then edit the pipeline in the pipeline canvas to customize it to your needs.",
        "links": null,
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-data-factory-pipelines-fabric/media/start-pipeline.png",
                "image_alt": "Screenshot of the Choose a task to start tile."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-data-factory-pipelines-fabric/media/pipeline-templates.png",
                "image_alt": "Screenshot of pipeline templates."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Orchestrate processes and data movement with Microsoft Fabric",
        "topic": "Run and monitor pipelines",
        "text": "When you have completed a pipeline, you can use the Validate option to check that the configuration is valid, and then either run it interactively or specify a schedule. View run history You can view the run history for a pipeline to see details of each run, either from the pipeline canvas or from the pipeline item listed in the page for the workspace.",
        "links": null,
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-data-factory-pipelines-fabric/media/run-pipeline.png",
                "image_alt": "Screenshot of a the Run menu for a pipeline in Microsoft Fabric."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-data-factory-pipelines-fabric/media/pipeline-runs.png",
                "image_alt": "Screenshot of a pipeline run history in Microsoft Fabric."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Orchestrate processes and data movement with Microsoft Fabric",
        "topic": "Exercise - Ingest data with a pipeline",
        "text": "Now it's your chance to implement a pipeline in Microsoft Fabric. In this exercise, you create a pipeline that copies data from an external source into a lakehouse. Then enhance the pipeline by adding activities to transform the ingested data. Note You need a Microsoft Fabric trial license with the Fabric preview enabled in your tenant. See Getting started with Fabric to enable your Fabric trial license. Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/fundamentals/fabric-trial"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://go.microsoft.com/fwlink/?linkid=2259606"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Orchestrate processes and data movement with Microsoft Fabric",
        "topic": "Summary",
        "text": "With Microsoft Fabric, you can create pipelines that encapsulate complex data ingestion and transformation processes. Pipelines provide an effective way to orchestrate data processing tasks that can be run on-demand or at scheduled intervals. Tip For more information about pipelines in Microsoft Fabric, see Data pipelines in the Microsoft Fabric documentation.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-factory/data-factory-overview"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Use Apache Spark in Microsoft Fabric",
        "topic": "Introduction",
        "text": "Apache Spark is an open source parallel processing framework for large-scale data processing and analytics. Spark has become popular in \"big data\" processing scenarios, and is available in multiple platform implementations; including Azure HDInsight, Azure Synapse Analytics, and Microsoft Fabric. This module explores how you can use Spark in Microsoft Fabric to ingest, process, and analyze data in a lakehouse. While the core techniques and code described in this module are common to all Spark implementations, the integrated tools and ability to work with Spark in the same environment as other data services in Microsoft Fabric makes it easier to incorporate Spark-based data processing into your overall data analytics solution.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Use Apache Spark in Microsoft Fabric",
        "topic": "Prepare to use Apache Spark",
        "text": "Apache Spark is a distributed data processing framework that enables large-scale data analytics by coordinating work across multiple processing nodes in a cluster, known in Microsoft Fabric as a Spark pool . Put more simply, Spark uses a \"divide and conquer\" approach to processing large volumes of data quickly by distributing the work across multiple computers. The process of distributing tasks and collating results is handled for you by Spark. Spark can run code written in a wide range of languages, including Java, Scala (a Java-based scripting language), Spark R, Spark SQL, and PySpark (a Spark-specific variant of Python). In practice, most data engineering and analytics workloads are accomplished using a combination of PySpark and Spark SQL. Spark pools A Spark pool consists of compute nodes that distribute data processing tasks. The general architecture is shown in the following diagram. As shown in the diagram, a Spark pool contains two kinds of node: A head node in a Spark pool coordinates distributed processes through a driver program. The pool includes multiple worker nodes on which executor processes perform the actual data processing tasks. The Spark pool uses this distributed compute architecture to access and process data in a compatible data store - such as a data lakehouse based in OneLake. Spark pools in Microsoft Fabric Microsoft Fabric provides a starter pool in each workspace, enabling Spark jobs to be started and run quickly with minimal setup and configuration. You can configure the starter pool to optimize the nodes it contains in accordance with your specific workload needs or cost constraints. Additionally, you can create custom Spark pools with specific node configurations that support your particular data processing needs. Note The ability to customize Spark pool settings can be disabled by Fabric administrators at the Fabric Capacity level. For more information, see Capacity administration settings for Data Engineering and Data Science in the Fabric documentation. You can manage settings for the starter pool and create new Spark pools in the Admin portal section of the workspace settings, under Capacity settings , then Data Engineering/Science Settings. Specific configuration settings for Spark pools include: Node Family : The type of virtual machines used for the Spark cluster nodes. In most cases, memory optimized nodes provide optimal performance. Autoscale : Whether or not to automatically provision nodes as needed, and if so, the initial and maximum number of nodes to be allocated to the pool. Dynamic allocation : Whether or not to dynamically allocate executor processes on the worker nodes based on data volumes. If you create one or more custom Spark pools in a workspace, you can set one of them (or the starter pool) as the default pool to be used if a specific pool is not specified for a given Spark job. Tip For more information about managing Spark pools in Microsoft Fabric, see Configuring starter pools in Microsoft Fabric and How to create custom Spark pools in Microsoft Fabric in the Microsoft Fabric documentation. Runtimes and environments The Spark open source ecosystem includes multiple versions of the Spark runtime , which determines the version of Apache Spark, Delta Lake, Python, and other core software components that are installed. Additionally, within a runtime you can install and use a wide selection of code libraries for common (and sometimes very specialized) tasks. Since a great deal of Spark processing is performed using PySpark, the huge range of Python libraries ensures that whatever the task you need to perform, there's probably a library to help. In some cases, organizations may need to define multiple environments to support a diverse range of data processing tasks. Each environment defines a specific runtime version as well as the libraries that must be installed to perform specific operations. Data engineers and scientists can then select which environment they want to use with a Spark pool for a particular task. Spark runtimes in Microsoft Fabric Microsoft Fabric supports multiple Spark runtimes, and will continue to add support for new runtimes as they are released. You can use the workspace settings interface to specify the Spark runtime that is used by default environment when a Spark pool is started. Tip For more information about Spark runtimes in Microsoft Fabric, see Apache Spark Runtimes in Fabric in the Microsoft Fabric documentation. Environments in Microsoft Fabric You can create custom environments in a Fabric workspace, enabling you to use specific Spark runtimes, libraries, and configuration settings for different data processing operations. When creating an environment, you can: Specify the Spark runtime it should use. View the built-in libraries that are installed in every environment. Install specific public libraries from the Python Package Index (PyPI). Install custom libraries by uploading a package file. Specify the Spark pool that the environment should use. Specify Spark configuration properties to override default behavior. Upload resource files that need to be available in the environment. After creating at least one custom environment, you can specify it as the default environment in the workspace settings. Tip For more information about using custom environments in Microsoft Fabric, see Create, configure, and use an environment in Microsoft Fabric in the Microsoft Fabric documentation. Additional Spark configuration options Managing Spark pools and environments are the primary ways in which you can manage Spark processing in a Fabric workspace. However, there are some additional options that you can use to make further optimizations. Native execution engine The native execution engine in Microsoft Fabric is a vectorized processing engine that runs Spark operations directly on lakehouse infrastructure. Using the native execution engine can significantly improve the performance of queries when working with large data sets in Parquet or Delta file formats. To use the native execution engine, you can enable it at the environment level or within an individual notebook. To enable the native execution engine at the environment level, set the following Spark properties in the environment configuration: spark.native.enabled : true spark.shuffle.manager : org.apache.spark.shuffle.sort.ColumnarShuffleManager To enable the native execution engine for a specific script or notebook, you can set these configuration properties at the beginning of your code, like this: %%configure \n{ \n   \"conf\": {\n       \"spark.native.enabled\": \"true\", \n       \"spark.shuffle.manager\": \"org.apache.spark.shuffle.sort.ColumnarShuffleManager\" \n   } \n} Tip For more information about the native execution engine, see Native execution engine for Fabric Spark in the Microsoft Fabric documentation. High concurrency mode When you run Spark code in Microsoft Fabric, a Spark session is initiated. You can optimize the efficiency of Spark resource usage by using high concurrency mode to share Spark sessions across multiple concurrent users or processes. A notebook uses a Spark session for its execution. When high concurrency mode is enabled, multiple users can, for example, run code in notebooks that use the same Spark session, while ensuring isolation of code to avoid variables in one notebook being affected by code in another notebook. You can also enable high concurrency mode for Spark jobs, enabling similar efficiencies for concurrent non-interactive Spark script execution. To enable high concurrency mode, use the Data Engineering/Science section of the workspace settings interface. Tip For more information about high concurrency mode, see High concurrency mode in Apache Spark for Fabric in the Microsoft Fabric documentation. Automatic MLFlow logging MLFlow is an open source library that is used in data science workloads to manage machine learning training and model deployment. A key capability of MLFlow is the ability to log model training and management operations. By default, Microsoft Fabric uses MLFlow to implicitly log machine learning experiment activity without requiring the data scientist to include explicit code to do so. You can disable this functionality in the workspace settings. Spark administration for a Fabric capacity Administrators can manage Spark settings at a Fabric capacity level, enabling them to restrict and override Spark settings in workspaces within an organization. Tip For more information about managing Spark configuration at the Fabric capacity level, see Configure and manage data engineering and data science settings for Fabric capacities in the Microsoft Fabric documentation.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-engineering/capacity-settings-overview?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-engineering/configure-starter-pools?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-engineering/create-custom-spark-pools?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-engineering/runtime?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-engineering/create-and-use-environment?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-engineering/native-execution-engine-overview?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-engineering/high-concurrency-overview?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-engineering/capacity-settings-management?azure-portal-true"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-apache-spark-work-files-lakehouse/media/spark-pool.png",
                "image_alt": "Diagram of a Spark pool."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-apache-spark-work-files-lakehouse/media/spark-settings.png",
                "image_alt": "Screenshot of the Spark settings page in Microsoft Fabric."
            },
            {
                "image_name": "image3",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-apache-spark-work-files-lakehouse/media/spark-environment.png",
                "image_alt": "Screenshot of the Environment page in Microsoft Fabric."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Use Apache Spark in Microsoft Fabric",
        "topic": "Run Spark code",
        "text": "To edit and run Spark code in Microsoft Fabric, you can use notebooks , or you can define a Spark job . Notebooks When you want to use Spark to explore and analyze data interactively, use a notebook. Notebooks enable you to combine text, images, and code written in multiple languages to create an interactive item that you can share with others and collaborate on. Notebooks consist of one or more cells , each of which can contain markdown-formatted content or executable code. You can run the code interactively in the notebook and see the results immediately. Spark job definition If you want to use Spark to ingest and transform data as part of an automated process, you can define a Spark job to run a script on-demand or based on a schedule. To configure a Spark job, create a Spark Job Definition in your workspace and specify the script it should run. You can also specify a reference file (for example, a Python code file containing definitions of functions that are used in your script) and a reference to a specific lakehouse containing data that the script processes.",
        "links": null,
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-apache-spark-work-files-lakehouse/media/notebook.png",
                "image_alt": "Screenshot of a notebook in Microsoft Fabric."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-apache-spark-work-files-lakehouse/media/spark-job.png",
                "image_alt": "Screenshot of a Spark job definition in Microsoft Fabric."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Use Apache Spark in Microsoft Fabric",
        "topic": "Work with data in a Spark dataframe",
        "text": "Natively, Spark uses a data structure called a resilient distributed dataset (RDD); but while you can write code that works directly with RDDs, the most commonly used data structure for working with structured data in Spark is the dataframe , which is provided as part of the Spark SQL library. Dataframes in Spark are similar to those in the ubiquitous Pandas Python library, but optimized to work in Spark's distributed processing environment. Note In addition to the Dataframe API, Spark SQL provides a strongly-typed Dataset API that is supported in Java and Scala. We'll focus on the Dataframe API in this module. Loading data into a dataframe Let's explore a hypothetical example to see how you can use a dataframe to work with data. Suppose you have the following data in a comma-delimited text file named products.csv in the Files/data folder in your lakehouse: ProductID,ProductName,Category,ListPrice\n771,\"Mountain-100 Silver, 38\",Mountain Bikes,3399.9900\n772,\"Mountain-100 Silver, 42\",Mountain Bikes,3399.9900\n773,\"Mountain-100 Silver, 44\",Mountain Bikes,3399.9900\n... Inferring a schema In a Spark notebook, you could use the following PySpark code to load the file data into a dataframe and display the first 10 rows: %%pyspark\ndf = spark.read.load('Files/data/products.csv',\n    format='csv',\n    header=True\n)\ndisplay(df.limit(10)) The %%pyspark line at the beginning is called a magic , and tells Spark that the language used in this cell is PySpark. You can select the language you want to use as a default in the toolbar of the Notebook interface, and then use a magic to override that choice for a specific cell. For example, here's the equivalent Scala code for the products data example: %%spark\nval df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"Files/data/products.csv\")\ndisplay(df.limit(10)) The magic %%spark is used to specify Scala. Both of these code samples would produce output like this: ProductID ProductName Category ListPrice 771 Mountain-100 Silver, 38 Mountain Bikes 3399.9900 772 Mountain-100 Silver, 42 Mountain Bikes 3399.9900 773 Mountain-100 Silver, 44 Mountain Bikes 3399.9900 ... ... ... ... Specifying an explicit schema In the previous example, the first row of the CSV file contained the column names, and Spark was able to infer the data type of each column from the data it contains. You can also specify an explicit schema for the data, which is useful when the column names aren't included in the data file, like this CSV example: 771,\"Mountain-100 Silver, 38\",Mountain Bikes,3399.9900\n772,\"Mountain-100 Silver, 42\",Mountain Bikes,3399.9900\n773,\"Mountain-100 Silver, 44\",Mountain Bikes,3399.9900\n... The following PySpark example shows how to specify a schema for the dataframe to be loaded from a file named product-data.csv in this format: from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nproductSchema = StructType([\n    StructField(\"ProductID\", IntegerType()),\n    StructField(\"ProductName\", StringType()),\n    StructField(\"Category\", StringType()),\n    StructField(\"ListPrice\", FloatType())\n    ])\n\ndf = spark.read.load('Files/data/product-data.csv',\n    format='csv',\n    schema=productSchema,\n    header=False)\ndisplay(df.limit(10)) The results would once again be similar to: ProductID ProductName Category ListPrice 771 Mountain-100 Silver, 38 Mountain Bikes 3399.9900 772 Mountain-100 Silver, 42 Mountain Bikes 3399.9900 773 Mountain-100 Silver, 44 Mountain Bikes 3399.9900 ... ... ... ... Tip Specifying an explicit schema also improves performance! Filtering and grouping dataframes You can use the methods of the Dataframe class to filter, sort, group, and otherwise manipulate the data it contains. For example, the following code example uses the select method to retrieve the ProductID and ListPrice columns from the df dataframe containing product data in the previous example: pricelist_df = df.select(\"ProductID\", \"ListPrice\") The results from this code example would look something like this: ProductID ListPrice 771 3399.9900 772 3399.9900 773 3399.9900 ... ... In common with most data manipulation methods, select returns a new dataframe object. Tip Selecting a subset of columns from a dataframe is a common operation, which can also be achieved by using the following shorter syntax: pricelist_df = df[\"ProductID\", \"ListPrice\"] You can \"chain\" methods together to perform a series of manipulations that results in a transformed dataframe. For example, this example code chains the select and where methods to create a new dataframe containing the ProductName and ListPrice columns for products with a category of Mountain Bikes or Road Bikes : bikes_df = df.select(\"ProductName\", \"Category\", \"ListPrice\").where((df[\"Category\"]==\"Mountain Bikes\") | (df[\"Category\"]==\"Road Bikes\"))\ndisplay(bikes_df) The results from this code example would look something like this: ProductName Category ListPrice Mountain-100 Silver, 38 Mountain Bikes 3399.9900 Road-750 Black, 52 Road Bikes 539.9900 ... ... ... To group and aggregate data, you can use the groupBy method and aggregate functions. For example, the following PySpark code counts the number of products for each category: counts_df = df.select(\"ProductID\", \"Category\").groupBy(\"Category\").count()\ndisplay(counts_df) The results from this code example would look something like this: Category count Headsets 3 Wheels 14 Mountain Bikes 32 ... ... Saving a dataframe You'll often want to use Spark to transform raw data and save the results for further analysis or downstream processing. The following code example saves the dataFrame into a parquet file in the data lake, replacing any existing file of the same name. bikes_df.write.mode(\"overwrite\").parquet('Files/product_data/bikes.parquet') Note The Parquet format is typically preferred for data files that you will use for further analysis or ingestion into an analytical store. Parquet is a very efficient format that is supported by most large scale data analytics systems. In fact, sometimes your data transformation requirement may simply be to convert data from another format (such as CSV) to Parquet! Partitioning the output file Partitioning is an optimization technique that enables Spark to maximize performance across the worker nodes. More performance gains can be achieved when filtering data in queries by eliminating unnecessary disk IO. To save a dataframe as a partitioned set of files, use the partitionBy method when writing the data. The following example saves the bikes_df dataframe (which contains the product data for the mountain bikes and road bikes categories), and partitions the data by category: bikes_df.write.partitionBy(\"Category\").mode(\"overwrite\").parquet(\"Files/bike_data\") The folder names generated when partitioning a dataframe include the partitioning column name and value in a column=value format, so the code example creates a folder named bike_data that contains the following subfolders: Category=Mountain Bikes Category=Road Bikes Each subfolder contains one or more parquet files with the product data for the appropriate category. Note You can partition the data by multiple columns, which results in a hierarchy of folders for each partitioning key. For example, you might partition sales order data by year and month, so that the folder hierarchy includes a folder for each year value, which in turn contains a subfolder for each month value. Load partitioned data When reading partitioned data into a dataframe, you can load data from any folder within the hierarchy by specifying explicit values or wildcards for the partitioned fields. The following example loads data for products in the Road Bikes category: road_bikes_df = spark.read.parquet('Files/bike_data/Category=Road Bikes')\ndisplay(road_bikes_df.limit(5)) Note The partitioning columns specified in the file path are omitted in the resulting dataframe. The results produced by the example query would not include a Category column - the category for all rows would be Road Bikes .",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Use Apache Spark in Microsoft Fabric",
        "topic": "Work with data using Spark SQL",
        "text": "The Dataframe API is part of a Spark library named Spark SQL, which enables data analysts to use SQL expressions to query and manipulate data. Creating database objects in the Spark catalog The Spark catalog is a metastore for relational data objects such as views and tables. The Spark runtime can use the catalog to seamlessly integrate code written in any Spark-supported language with SQL expressions that may be more natural to some data analysts or developers. One of the simplest ways to make data in a dataframe available for querying in the Spark catalog is to create a temporary view, as shown in the following code example: df.createOrReplaceTempView(\"products_view\") A view is temporary, meaning that it's automatically deleted at the end of the current session. You can also create tables that are persisted in the catalog to define a database that can be queried using Spark SQL. Tables are metadata structures that store their underlying data in the storage location associated with the catalog. In Microsoft Fabric, data for managed tables is stored in the Tables storage location shown in your data lake, and any tables created using Spark are listed there. You can create an empty table by using the spark.catalog.createTable method, or you can save a dataframe as a table by using its saveAsTable method. Deleting a managed table also deletes its underlying data. For example, the following code saves a dataframe as a new table named products : df.write.format(\"delta\").saveAsTable(\"products\") Note The Spark catalog supports tables based on files in various formats. The preferred format in Microsoft Fabric is delta , which is the format for a relational data technology on Spark named Delta Lake . Delta tables support features commonly found in relational database systems, including transactions, versioning, and support for streaming data. Additionally, you can create external tables by using the spark.catalog.createExternalTable method. External tables define metadata in the catalog but get their underlying data from an external storage location; typically a folder in the Files storage area of a lakehouse. Deleting an external table doesn't delete the underlying data. Tip You can apply the same partitioning technique to delta lake tables as discussed for parquet files in the previous unit. Partitioning tables can result in better performance when querying them. Using the Spark SQL API to query data You can use the Spark SQL API in code written in any language to query data in the catalog. For example, the following PySpark code uses a SQL query to return data from the products table as a dataframe. bikes_df = spark.sql(\"SELECT ProductID, ProductName, ListPrice \\\n                      FROM products \\\n                      WHERE Category IN ('Mountain Bikes', 'Road Bikes')\")\ndisplay(bikes_df) The results from the code example would look similar to the following table: ProductID ProductName ListPrice 771 Mountain-100 Silver, 38 3399.9900 839 Road-750 Black, 52 539.9900 ... ... ... Using SQL code The previous example demonstrated how to use the Spark SQL API to embed SQL expressions in Spark code. In a notebook, you can also use the %%sql magic to run SQL code that queries objects in the catalog, like this: %%sql\n\nSELECT Category, COUNT(ProductID) AS ProductCount\nFROM products\nGROUP BY Category\nORDER BY Category The SQL code example returns a resultset that is automatically displayed in the notebook as a table: Category ProductCount Bib-Shorts 3 Bike Racks 1 Bike Stands 1 ... ...",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Use Apache Spark in Microsoft Fabric",
        "topic": "Visualize data in a Spark notebook",
        "text": "One of the most intuitive ways to analyze the results of data queries is to visualize them as charts. Notebooks in Microsoft Fabric provide some basic charting capabilities in the user interface, and when that functionality doesn't provide what you need, you can use one of the many Python graphics libraries to create and display data visualizations in the notebook. Using built-in notebook charts When you display a dataframe or run a SQL query in a Spark notebook, the results are displayed under the code cell. By default, results are rendered as a table, but you can also change the results view to a chart and use the chart properties to customize how the chart visualizes the data, as shown here: The built-in charting functionality in notebooks is useful when you want to quickly summarize the data visually. When you want to have more control over how the data is formatted, you should consider using a graphics package to create your own visualizations. Using graphics packages in code There are many graphics packages that you can use to create data visualizations in code. In particular, Python supports a large selection of packages; most of them built on the base Matplotlib library. The output from a graphics library can be rendered in a notebook, making it easy to combine code to ingest and manipulate data with inline data visualizations and markdown cells to provide commentary. For example, you could use the following PySpark code to aggregate data from the hypothetical products data explored previously in this module, and use Matplotlib to create a chart from the aggregated data. from matplotlib import pyplot as plt\n\n# Get the data as a Pandas dataframe\ndata = spark.sql(\"SELECT Category, COUNT(ProductID) AS ProductCount \\\n                  FROM products \\\n                  GROUP BY Category \\\n                  ORDER BY Category\").toPandas()\n\n# Clear the plot area\nplt.clf()\n\n# Create a Figure\nfig = plt.figure(figsize=(12,8))\n\n# Create a bar plot of product counts by category\nplt.bar(x=data['Category'], height=data['ProductCount'], color='orange')\n\n# Customize the chart\nplt.title('Product Counts by Category')\nplt.xlabel('Category')\nplt.ylabel('Products')\nplt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)\nplt.xticks(rotation=70)\n\n# Show the plot area\nplt.show() The Matplotlib library requires data to be in a Pandas dataframe rather than a Spark dataframe, so the toPandas method is used to convert it. The code then creates a figure with a specified size and plots a bar chart with some custom property configuration before showing the resulting plot. The chart produced by the code would look similar to the following image: You can use the Matplotlib library to create many kinds of chart; or if preferred, you can use other libraries such as Seaborn to create highly customized charts.",
        "links": null,
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-apache-spark-work-files-lakehouse/media/notebook-chart.png",
                "image_alt": "Screenshot of notebook chart of product counts by category."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-apache-spark-work-files-lakehouse/media/chart.png",
                "image_alt": "Screenshot of a bar chart showing product counts by category."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Use Apache Spark in Microsoft Fabric",
        "topic": "Exercise - Analyze data with Apache Spark",
        "text": "Now it's your opportunity to work with Apache Spark in Microsoft Fabric. In this exercise, you'll use a Spark notebook to analyze and visualize data from files in lakehouse. Note You need a Microsoft Fabric trial license with the Fabric preview enabled in your tenant. See Getting started with Fabric to enable your Fabric trial license. Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://go.microsoft.com/fwlink/?linkid=2259707"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Use Apache Spark in Microsoft Fabric",
        "topic": "Summary",
        "text": "Apache Spark is a key technology used in big data analytics. Spark support in Microsoft Fabric enables you to integrate big data processing in Spark with the other data analytics and visualization capabilities of the platform. Tip For more information about working with data in Spark, see the Spark SQL, DataFrames and Datasets Guide in the Apache Spark documentation.",
        "links": [
            "https://spark.apache.org/docs/latest/sql-programming-guide.html"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Work with real-time data in an Eventhouse in Microsoft Fabric",
        "topic": "Introduction",
        "text": "An Eventhouse in Microsoft Fabric provides a data store for large volumes of data. It's a container that houses one or more KQL databases, each optimized for storing and analyzing real-time data that arrives continuously from various sources. You can load data into a KQL database in an Eventhouse using an Eventstream or you can directly ingest data into a KQL database. Once you have ingested data, you can then: Query the data using Kusto Query Language (KQL) or T-SQL in a KQL queryset. Use Real-Time Dashboards to visualize the data. Use Fabric Activator to automate actions based on the data. Understanding how KQL databases work helps you write effective queries to analyze real-time data. In this module, you'll learn about the characteristics that make KQL databases ideal for real-time data, then apply this knowledge by exploring KQL querying techniques and database objects like materialized views and stored functions. How do KQL databases work with real-time data? KQL databases automatically partition data by ingestion time, making recent data quickly accessible while storing historical data for trend analysis. Partitioning means the database organizes data into separate storage locations based on when it arrived, so when you query for recent data, the database knows exactly where to search rather than scanning all the data. Think of it like a digital conveyor belt - events flow in continuously, get organized automatically by when they arrive, and are immediately available for analysis while the stream keeps flowing. This automatic time-based organization works because real-time data has a unique characteristic: it represents immutable events that happened at specific moments in time. Immutable means these events can't be changed once they've occurred - a temperature reading at 3:15 PM will always be that reading because it represents what actually happened at that moment. Since each event is permanently tied to when it happened, this creates what we call time-series data - data where the timestamp is often as important as the event itself. Time-series data follows an append-only pattern where new events are continuously added, and data is rarely updated or deleted because events represent what actually happened at specific moments. This is fundamentally different from traditional relational databases, where you typically update existing records and maintain relationships between different data tables.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Work with real-time data in an Eventhouse in Microsoft Fabric",
        "topic": "Get started with an Eventhouse",
        "text": "When you create an Eventhouse, a default KQL database is automatically created with the same name. An Eventhouse contains one or more KQL databases, where you can create tables, stored procedures, materialized views, functions, data streams, and shortcuts to manage your data. You can use the default KQL database or create other KQL databases as needed. Work with data in your Eventhouse There are several ways to access and work with data in a KQL database within an Eventhouse: Data ingestion You can ingest data directly into your KQL database from various sources: Local files, Azure storage, Amazon S3 Azure Event Hubs, Fabric Eventstream, Real-Time hub OneLake, Data Factory copy, Dataflows Connectors to sources such as Apache Kafka, Confluent Cloud Kafka, Apache Flink, MQTT (Message Queuing Telemetry Transport), Amazon Kinesis, Google Cloud Pub/Sub Database shortcuts You can create database shortcuts to existing KQL databases in other eventhouses or Azure Data Explorer databases. These shortcuts let you query data from external KQL databases as if the data were stored locally in your eventhouse, without actually copying the data. OneLake availability You can enable OneLake availability for individual KQL databases or tables, making your data accessible throughout the Fabric ecosystem for cross-workload integration with Power BI, Warehouse, Lakehouse, and other Fabric services. Query data in a KQL database To query data in a KQL database, you can use KQL or T-SQL in KQL querysets . When you create a KQL database, an attached KQL queryset is automatically created for running and saving queries. Basic KQL syntax KQL uses a pipeline approach where data flows from one operation to the next using the pipe ( | ) character. Think of it like a funnel - you start with an entire data table, and each operator filters, rearranges, or summarizes the data before passing it to the next step. The order of operators matters because each step works on the results from the previous step. Important KQL is case-sensitive for everything including table names, column names, function names, operators, keywords, and string values. All identifiers must match exactly. For example, TaxiTrips is different from taxitrips or TAXITRIPS . Here's an example that shows the funnel concept: TaxiTrips\n| where fare_amount > 20\n| project trip_id, pickup_datetime, fare_amount\n| take 10 This query starts with all data in the TaxiTrips table, filters it to show only trips with fares over $20, selects specific columns using the project operator, and uses the take operator to return the first 10 rows that match the criteria in the where clause. The simplest KQL query consists of a table name: TaxiTrips This returns all columns from the TaxiTrips table, but the number of rows displayed is limited by your query tool's default settings. To retrieve a sample of data from potentially large tables, use the take operator: TaxiTrips\n| take 100 This returns the first 100 rows from the TaxiTrips table, which is useful for exploring data structure without processing the entire table. You can also aggregate data: TaxiTrips\n| summarize trip_count = count() by taxi_id This returns a summary table showing the total number of trips ( trip_count ) for each unique taxi_id , effectively counting how many trips each taxi has made. Analyze data with KQL queryset KQL queryset provides a workspace for running and managing queries against KQL databases. The KQL queryset allows you to save queries for future use, organize multiple query tabs, and share queries with others for collaboration. The KQL queryset also supports T-SQL queries, allowing you to use T-SQL syntax alongside KQL for data analysis. You can also create data visualizations while exploring your data, rendering query results as charts, tables, and other visual formats. Use Copilot to assist with queries For AI-based assistance with KQL querying, you can use Copilot for Real-Time Intelligence When your administrator enables Copilot, you see the option in the queryset menu bar. Copilot opens as a pane to the side of the main query interface. When you ask a question about your data, Copilot generates the KQL code to answer your question.",
        "links": [
            "https://learn.microsoft.com/wwl/query-data-kql-database-microsoft-fabric/media/eventhouse.png#lightbox",
            "https://learn.microsoft.com/wwl/query-data-kql-database-microsoft-fabric/media/get-data.png#lightbox",
            "https://learn.microsoft.com/wwl/query-data-kql-database-microsoft-fabric/media/queryset-visual.png#lightbox",
            "https://learn.microsoft.com/wwl/query-data-kql-database-microsoft-fabric/media/kql-copilot.png#lightbox"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/query-data-kql-database-microsoft-fabric/media/eventhouse.png",
                "image_alt": "Screenshot of an Eventhouse in Microsoft Fabric."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/query-data-kql-database-microsoft-fabric/media/get-data.png",
                "image_alt": "Screenshot of the Get Data menu for an eventhouse in Microsoft Fabric."
            },
            {
                "image_name": "image3",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/query-data-kql-database-microsoft-fabric/media/queryset-visual.png",
                "image_alt": "Screenshot of a visualization in a queryset."
            },
            {
                "image_name": "image4",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/query-data-kql-database-microsoft-fabric/media/kql-copilot.png",
                "image_alt": "Screenshot of Copilot for Real-Time Intelligence."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Work with real-time data in an Eventhouse in Microsoft Fabric",
        "topic": "Use KQL effectively",
        "text": "Understanding how to write efficient KQL queries is essential for getting good performance when working with Eventhouses. This unit covers key optimization techniques and explains why they matter for your queries. Why query optimization matters Query performance in KQL databases depends on the amount of data processed. When you understand how KQL processes data, you can write queries that: Run faster by reducing the data scanned - For example, instead of scanning millions of rows, filter early to process only thousands Use fewer resources - For example, selecting only 3 columns instead of all 50 columns reduces processing overhead Work reliably with growing data - For example, a query that works on 1 million rows today will still perform well when your data grows to 10 million rows The key principle is: the less data your query needs to process, the faster it runs. Understand key optimization techniques Filter data early and effectively Filtering reduces the amount of data that subsequent operations need to process, and KQL databases use indexes and data organization techniques that make early filtering especially efficient. Time-based filtering is effective because Eventhouses typically contain time-series data: TaxiTrips\n| where pickup_datetime > ago(30min)  // Filter first - uses time index\n| project trip_id, vendor_id, pickup_datetime, fare_amount\n| summarize avg_fare = avg(fare_amount) by vendor_id Order your filters by how much data they eliminate - put filters that eliminate the most data first. Think of it like a funnel: start with the filter that removes the most rows, then apply more specific filters to the remaining data: TaxiTrips\n| where pickup_datetime > ago(1d)    // Time filter first - eliminates most data\n| where vendor_id == \"VTS\"           // Specific vendor - eliminates some data  \n| where fare_amount > 0              // Value filter - eliminates least data\n| summarize trip_count = count() Reduce columns early Projecting or selecting only the columns you need reduces resource usage. This is especially important when working with wide tables that have many columns. TaxiTrips\n| project trip_id, pickup_datetime, fare_amount  // Select columns early\n| where pickup_datetime > ago(1d)                // Then filter\n| summarize avg_fare = avg(fare_amount) Optimize aggregations and joins Aggregations and joins are resource-intensive operations because they need to process and combine large amounts of data. How you structure them can significantly affect query performance. For aggregations, limit results when exploring data : TaxiTrips\n| where pickup_datetime > ago(1d)\n| summarize trip_count = count() by trip_id, vendor_id\n| limit 1000  // Limit results for exploration For joins, put the smaller table first . When joining tables, KQL processes the first table to match with the second table. Starting with a smaller table means fewer rows to process, making the join more efficient. // Good: Small vendor table first\nVendorInfo        \n| join kind=inner TaxiTrips on vendor_id\n\n// Avoid: Large taxi table first\nTaxiTrips         \n| join kind=inner VendorInfo on vendor_id",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Work with real-time data in an Eventhouse in Microsoft Fabric",
        "topic": "Materialized views and stored functions",
        "text": "Now that you understand basic KQL querying and optimization techniques, let's explore materialized views and stored functions in eventhouses. Understand materialized views Materialized views are precomputed aggregations that solve a common performance challenge in KQL databases. KQL databases in eventhouses often contain millions or billions of rows from streaming data sources like IoT sensors, application logs, and other events. Running aggregation queries across these large datasets can take significant time and computing resources. Materialized views store precomputed aggregation results and automatically update them as new data arrives. Instead of recalculating metrics from all historical data every time you query, the materialized view maintains the results and only processes the new data to update the aggregations. This provides instant results for dashboards and reports, even when working with massive datasets. How automatic updates work A materialized view consists of two parts that work together to provide always-current results: A materialized part : Precomputed aggregation results from data that has already been processed A delta : New data that has arrived since the last background update When you query a materialized view, the system automatically combines both parts at query time to give you fresh, up-to-date results. This means materialized views always return current data, regardless of when the background materialization process last ran. Meanwhile, a background process periodically moves data from the delta part into the materialized part, keeping the precomputed results current. This approach provides the speed of precomputed results with the freshness of real-time data. Create materialized views A materialized view encapsulates a KQL summarize statement that automatically updates as new data arrives. Here's an example that tracks trip metrics by vendor and day: .create materialized-view TripsByVendor on table TaxiTrips\n{\n    TaxiTrips\n    | summarize trips = count(), avg_fare = avg(fare_amount), total_revenue = sum(fare_amount)\n    by vendor_id, pickup_date = format_datetime(pickup_datetime, \"yyyy-MM-dd\")\n} Query materialized views Once created, materialized views can be queried like regular tables: TripsByVendor\n| where pickup_date >= ago(7d)\n| project pickup_date, vendor_id, trips, avg_fare, total_revenue\n| sort by pickup_date desc, total_revenue desc Understand stored functions KQL includes the ability to encapsulate a query as a function, making it easier to repeat common queries. You can also specify parameters for a function, so you can repeat the same query with variable values. Stored functions are useful in eventhouses where you have streaming data and multiple people writing queries. Instead of writing the same filtering or transformation logic repeatedly, you can define it once as a function and reuse it across different queries. Functions also help ensure that calculations are performed consistently when different team members need to apply the same logic to the data. Create a function .create-or-alter function trips_by_min_passenger_count(num_passengers:long)\n{\n    TaxiTrips\n    | where passenger_count >= num_passengers \n    | project trip_id, pickup_datetime\n} To call the function, use it like a table. In this example, the trips_by_min_passenger_count function is used to find 10 trips with at least three passengers: trips_by_min_passenger_count(3)\n| take 10",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Work with real-time data in an Eventhouse in Microsoft Fabric",
        "topic": "Exercise - Work with data in an Eventhouse",
        "text": "Now it's your chance to work with real-time data. In this exercise, you use KQL and T-SQL to query bike-sharing data in a KQL database. This lab takes approximately 30 minutes to complete. Important You need a Microsoft Fabric license to complete this exercise. See Getting started with Fabric for details of how to enable a free Fabric trial license. Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://go.microsoft.com/fwlink/?linkid=2335175&azure-portal=true"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Ingest data with Microsoft Fabric",
        "unit": "Work with real-time data in an Eventhouse in Microsoft Fabric",
        "topic": "Summary",
        "text": "In this module, you explored how to work with real-time data in eventhouses using KQL. You learned how to write effective KQL queries and use advanced features like materialized views and stored functions. To learn more about KQL, explore these resources: Kusto Query Language (KQL) overview Kusto Query Language best practices Copilot for Real-Time Intelligence",
        "links": [
            "https://learn.microsoft.com/en-us/kusto/query/?azure-portal=true",
            "https://learn.microsoft.com/en-us/kusto/query/best-practices?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/get-started/copilot-real-time-intelligence?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Introduction to end-to-end analytics using Microsoft Fabric",
        "topic": "Introduction",
        "text": "Microsoft Fabric is an end-to-end analytics platform that provides a single, integrated environment for data professionals and the business to collaborate on data projects. Fabric provides a set of integrated services that enable you to ingest, store, process, and analyze data in a single environment. Microsoft Fabric provides tools for both citizen and professional data practitioners, and integrates with tools the business needs to make decisions. Fabric includes the following services: Data engineering Data integration Data warehousing Real-time intelligence Data science Business intelligence Additionally, Microsoft Fabric integrates Copilot, a generative AI assistant that enhances productivity across all workloads by providing intelligent code completion, natural language to SQL conversion, automated insights, and contextual assistance for data professionals and business users alike. This module introduces the Fabric platform, discusses who Fabric is for, explores Fabric services, and examines how Copilot enhances the analytics experience.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Introduction to end-to-end analytics using Microsoft Fabric",
        "topic": "Explore end-to-end analytics with Microsoft Fabric",
        "text": "Scalable analytics can be complex, fragmented, and expensive. Microsoft Fabric simplifies analytics solutions by providing a single, easy-to-use product that integrates various tools and services into one platform. Fabric is a unified software-as-a-service (SaaS) platform where all data is stored in a single open format in OneLake. OneLake is accessible by all analytics engines in the platform, ensuring scalability, cost-effectiveness, and accessibility from anywhere with an internet connection. OneLake OneLake is Fabric's centralized data storage architecture that enables collaboration by eliminating the need to move or copy data between systems. OneLake unifies your data across regions and clouds into a single logical lake without moving or duplicating data. OneLake is built on Azure Data Lake Storage (ADLS) and supports various formats, including Delta, Parquet, CSV, and JSON. All compute engines in Fabric automatically store their data in OneLake, making it directly accessible without the need for movement or duplication. For tabular data, the analytical engines in Fabric write data in delta-parquet format and all engines interact with the format seamlessly. Shortcuts are references to files or storage locations external to OneLake, allowing you to access existing cloud data without copying it. Shortcuts ensure data consistency and enable Fabric to stay in sync with the source. Workspaces In Microsoft Fabric, workspaces serve as logical containers that help you organize and manage your data, reports, and other assets. They provide a clear separation of resources, making it easier to control access and maintain security. Each workspace has its own set of permissions, ensuring that only authorized users can view or modify its contents. This structure supports team collaboration while maintaining strict access control for both business and IT users. Workspaces allow you to manage compute resources and integrate with Git for version control. You can optimize performance and cost by configuring compute settings, while Git integration helps track changes, collaborate on code, and maintain a history of your work. Administration and governance Fabric's OneLake is centrally governed and open for collaboration. Data is secured and governed in one place, which allows users to easily find and access the data they need. Fabric administration is centralized in the Admin portal . In the admin portal you can manage groups and permissions, configure data sources and gateways, and monitor usage and performance. You can also access the Fabric admin APIs and SDKs in the admin portal, which can automate common tasks and integrate Fabric with other systems. The OneLake catalog helps you analyze, monitor, and maintain data governance. It provides guidance on sensitivity labels, item metadata, and data refresh status, offering insights into the governance status and actions for improvement. Note Review the Microsoft Fabric administration documentation for more information.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/admin"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/introduction-end-analytics-use-microsoft-fabric/media/onelake-architecture.png",
                "image_alt": "Diagram of the OneLake architecture with workloads accessing the same OneLake data storage and Delta-Parquet storage format as the foundation for serverless compute."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Introduction to end-to-end analytics using Microsoft Fabric",
        "topic": "Explore data teams and Microsoft Fabric",
        "text": "Microsoft Fabric's unified data analytics platform makes it easier for data professionals to collaborate on projects. Fabric increases collaboration between data professionals by removing data silos and the need for multiple systems. Traditional roles and challenges In a traditional analytics development process, data teams often face several challenges due to the division of data tasks and workflows. Data engineers process and curate data for analysts, who then use it to create business reports. This process requires extensive coordination, often leading to delays and misinterpretations. Data analysts often need to perform downstream data transformations before creating Power BI reports. This process is time-consuming and can lack the necessary context, making it harder for analysts to connect directly with the data. Data scientists face difficulties integrating native data science techniques with existing systems, which are often complex, and makes it challenging to efficiently provide data-driven insights. Evolution of collaborative workflows Microsoft Fabric simplifies the analytics development process by unifying tools into a SaaS platform. Fabric allows different roles to collaborate effectively without duplicating efforts. Data engineers can ingest, transform, and load data directly into OneLake using Pipelines, which automate workflows and support scheduling. They can store data in lakehouses, using the Delta-Parquet format for efficient storage and versioning. Notebooks provide advanced scripting capabilities for complex transformations. Analytics engineers bridge the gap between data engineering and analysis by curating data assets in lakehouses, ensuring data quality, and enabling self-service analytics. They can create semantic models in Power BI to organize and present data effectively. Data analysts can transform data upstream using dataflows and connect directly to OneLake with Direct Lake mode, reducing the need for downstream transformations. They can create interactive reports more efficiently using Power BI. Data scientists can use integrated notebooks with support for Python and Spark to build and test machine learning models. They can store and access data in lakehouses and integrate with Azure Machine Learning to operationalize and deploy models. Low-to-no-code users and citizen developers can discover curated datasets through the OneLake catalog and use Power BI templates to quickly create reports and dashboards. They can also use dataflows to perform simple ETL tasks without relying on data engineers.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Introduction to end-to-end analytics using Microsoft Fabric",
        "topic": "Enable and use Microsoft Fabric",
        "text": "Before you can explore the end-to-end capabilities of Microsoft Fabric, it must be enabled for your organization. You might need to work with your IT department to enable Fabric for your organization, including one of the following roles: Fabric admin (formerly Power BI admin) : Manages Fabric settings and configurations. Power Platform admin : Oversees Power Platform services, including Fabric. Microsoft 365 admin : Manages organization-wide Microsoft services, including Fabric. Enable Microsoft Fabric Admins can enable Fabric in the Admin portal > Tenant settings in the Power BI service. Fabric can be enabled for the entire organization or for specific Microsoft 365 or Microsoft Entra security groups. Admins can also delegate this ability to other users at the capacity level. Note If your organization isn't using Fabric or Power BI today, you can sign up for a free Fabric trial to explore its features. Create workspaces Workspaces are collaborative environments where you can create and manage items like lakehouses, warehouses, and reports. All data is stored in OneLake and accessed through workspaces. Workspaces also support data lineage view, providing a visual view of data flow and dependencies to enhance transparency and decision-making. In Workspace settings , you can configure: License type to use Fabric features. OneDrive access for the workspace. Azure Data Lake Gen2 Storage connection. Git integration for version control. Spark workload settings for performance optimization. You can manage workspace access through four roles: admin , contributor , member , and viewer . These roles apply to all items in a workspace and should be reserved for collaboration. For more granular access control, use item-level permissions based on business needs. Note Learn more about workspaces in the Fabric documentation . Discover data with OneLake catalog The OneLake catalog in Microsoft Fabric helps users easily find and access various data sources within their organization. Users explore and connect to data sources, ensuring they have the right data for their needs. Users only see items shared with them. Here are some considerations when using OneLake catalog: Narrow results by workspaces or domains (if implemented). Explore default categories to quickly locate relevant data. Filter by keyword or item type. Create items with Fabric workloads After you create your Fabric enabled workspace, you can start creating items in Fabric. Each workload in Fabric offers different item types for storing, processing, and analyzing data. Fabric workloads include: Data Engineering : Create lakehouses and operationalize workflows to build, transform, and share your data estate. Data Factory : Ingest, transform, and orchestrate data. Data Science : Detect trends, identify outliers, and predict values using machine learning. Data Warehouse : Combine multiple sources in a traditional warehouse for analytics. Databases : Create and manage databases with tools to insert, query, and extract data. Industry Solutions : Use out-of-the-box industry data solutions. Real-Time Intelligence : Process, monitor, and analyze streaming data. Power BI : Create reports and dashboards to make data-driven decisions. Fabric integrates capabilities from existing Microsoft tools like Power BI, Azure Synapse Analytics, and Azure Data Factory into a unified platform. Fabric also supports a data mesh architecture, allowing decentralized data ownership while maintaining centralized governance. This design eliminates the need for direct Azure resource access, simplifying data workflows. Enhance productivity with Copilot in Fabric Microsoft Copilot in Fabric is a generative AI assistant that enhances productivity and accelerates insights across all Fabric workloads. Copilot uses large language models (LLMs) to provide intelligent assistance for data professionals, self-service users, and business users. Copilot capabilities across workloads Copilot provides tailored assistance for each Fabric workload: Data Engineering and Data Science : Intelligent code completion, automated routine tasks, industry-standard code templates, and contextual code suggestions that adapt to specific tasks. Copilot helps with data preparation, pipeline building, and insight generation. Data Factory : AI-enhanced toolset supporting both citizen and professional data wranglers with intelligent code generation for data transformation and code explanations for complex tasks. Data Warehouse and SQL Database : Natural language to SQL conversion, code completion, quick actions, and intelligent insights. Users can describe what they want in plain language, and Copilot generates the appropriate SQL queries. Power BI : Automatic report generation, summary creation for report pages, synonym generation for better Q&A capabilities, and natural language querying of data. Business users can also use Copilot to extract more insights and chat with the report data. Real-Time Intelligence : Advanced AI tool that translates natural language questions into Kusto Query Language (KQL) queries, streamlining data analysis for both experienced users and citizen data scientists. Enabling Copilot Administrators must enable Copilot in the Admin portal > Tenant settings in the Power BI service. Copilot helps users work more efficiently by providing AI assistance for common tasks like writing code, generating queries, and creating reports. This support is available to both technical and business users while maintaining organizational security policies. Tip For more information, see the Copilot for Microsoft Fabric and Power BI documentation.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial",
            "https://learn.microsoft.com/en-us/fabric/get-started/workspaces",
            "https://learn.microsoft.com/wwl/introduction-end-analytics-use-microsoft-fabric/media/onelake-catalog.png#lightbox",
            "https://learn.microsoft.com/en-us/fabric/fundamentals/copilot-faq-fabric"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/introduction-end-analytics-use-microsoft-fabric/media/enable-fabric.png",
                "image_alt": "Screenshot of the Fabric admin portal where you can enable Fabric items."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/introduction-end-analytics-use-microsoft-fabric/media/onelake-catalog.png",
                "image_alt": "Screenshot of the OneLake catalog."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Introduction to end-to-end analytics using Microsoft Fabric",
        "topic": "Summary",
        "text": "Data professionals are increasingly expected to be able to work with data at scale, and to be able to do so in a way that is secure, compliant, and cost-effective. At the same time, the business wants to use that data more effectively and quickly to make better decisions. Microsoft Fabric is a collection of tools and services that enables organizations to do just that. In this module, you learned about Fabric's OneLake storage, the workloads that are included in Fabric, how to enable and use Fabric in your organization, and how Copilot enhances productivity across all Fabric workloads with intelligent AI assistance.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Get started with lakehouses in Microsoft Fabric",
        "topic": "Introduction",
        "text": "The foundation of Microsoft Fabric is a lakehouse , which is built on top of the OneLake scalable storage layer and uses Apache Spark and SQL compute engines for big data processing. A lakehouse is a unified platform that combines: The flexible and scalable storage of a data lake The ability to query and analyze data of a data ware house Imagine your company has been using a data warehouse to store structured data from its transactional systems, such as order history, inventory levels, and customer information. You collect unstructured data from social media, website logs, and external sources that are difficult to manage and analyze using the existing data warehouse infrastructure. Your company's new directive is to improve its decision-making capabilities by analyzing data in various formats across multiple sources, so the company chooses Microsoft Fabric. In this module, we explore how a lakehouse in Microsoft Fabric provides a scalable and flexible data store for files and tables that you can query using SQL.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Get started with lakehouses in Microsoft Fabric",
        "topic": "Explore the Microsoft Fabric lakehouse",
        "text": "A lakehouse presents as a database and is built on top of a data lake using Delta format tables. Lakehouses combine the SQL-based analytical capabilities of a relational data warehouse and the flexibility and scalability of a data lake. Lakehouses store all data formats and can be used with various analytics tools and programming languages. As cloud-based solutions, lakehouses can scale automatically and provide high availability and disaster recovery. Some benefits of a lakehouse include: Lakehouses use Spark and SQL engines to process large-scale data and support machine learning or predictive modeling analytics. Lakehouse data is organized in a schema-on-read format , which means you define the schema as needed rather than having a predefined schema. Lakehouses support ACID (Atomicity, Consistency, Isolation, Durability) transactions through Delta Lake formatted tables for data consistency and integrity. Lakehouses are a single location for data engineers, data scientists, and data analysts to access and use data. A lakehouse is a great option if you want a scalable analytics solution that maintains data consistency. It's important to evaluate your specific requirements to determine which solution is the best fit. Load data into a lakehouse Fabric lakehouses are a central element for your analytics solution. You can follow the ETL (Extract, Transform, Load) process to ingest and transform data before loading to the lakehouse. You can ingest data in many common formats from various sources, including local files, databases, or APIs. You can also create Fabric shortcuts to data in external sources, such as Azure Data Lake Store Gen2 or OneLake. Use the Lakehouse explorer to browse files, folders, shortcuts, and tables and view their contents within the Fabric platform. Ingested data can be transformed and then loaded using either Apache Spark with notebooks or Dataflows Gen2. Use Data Factory pipelines to orchestrate your different ETL activities and land the prepared data into your lakehouse. Note Dataflows Gen2 are based on Power Query - a familiar tool to data analysts using Excel or Power BI that provides visual representation of transformations as an alternative to traditional programming. You can use your lakehouse for many reasons, including: Analyze using SQL. Train machine learning models. Perform analytics on real-time data. Develop reports in Power BI. Secure a lakehouse Lakehouse access is managed either through the workspace or item-level sharing. Workspaces roles should be used for collaborators because these roles grant access to all items within the workspace. Item-level sharing is best used for granting access for read-only needs, such as analytics or Power BI report development. Fabric lakehouses also support data governance features including sensitivity labels, and can be extended by using Microsoft Purview with your Fabric tenant. Note For more information, see the Security in Microsoft Fabric documentation.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/security/security-overview"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-lakehouses/media/lakehouse-components.png",
                "image_alt": "Diagram of a lakehouse, displaying the folder structure of a data lake and the relational capabilities of a data warehouse."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Get started with lakehouses in Microsoft Fabric",
        "topic": "Work with Microsoft Fabric lakehouses",
        "text": "Now that you understand the core capabilities of a Microsoft Fabric lakehouse, let's explore how to work with one. Create and explore a lakehouse When you create a new lakehouse, you have three different data items automatically created in your workspace. The lakehouse contains shortcuts, folders, files, and tables. The Semantic model (default) provides an easy data source for Power BI report developers. The SQL analytics endpoint allows read-only access to query data with SQL. You can work with the data in the lakehouse in two modes: lakehouse enables you to add and interact with tables, files, and folders in the lakehouse. SQL analytics endpoint enables you to use SQL to query the tables in the lakehouse and manage its relational semantic model. Ingest data into a lakehouse Ingesting data into your lakehouse is the first step in your ETL process. Use any of the following methods to bring data into your lakehouse. Upload : Upload local files. Dataflows Gen2 : Import and transform data using Power Query. Notebooks : Use Apache Spark to ingest, transform, and load data. Data Factory pipelines : Use the Copy data activity. This data can then be loaded directly into files or tables. Consider your data loading pattern when ingesting data to determine if you should load all raw data as files before processing or use staging tables. Spark job definitions can also be used to submit batch/streaming jobs to Spark clusters. By uploading the binary files from the compilation output of different languages (for example, .jar from Java), you can apply different transformation logic to the data hosted on a lakehouse. Besides the binary file, you can further customize the behavior of the job by uploading more libraries and command line arguments. Note For more information, see the Create an Apache Spark job definition documentation. Access data using shortcuts Another way to access and use data in Fabric is to use shortcuts . Shortcuts enable you to integrate data into your lakehouse while keeping it stored in external storage. Shortcuts are useful when you need to source data that's in a different storage account or even a different cloud provider. Within your lakehouse you can create shortcuts that point to different storage accounts and other Fabric items like data warehouses, KQL databases, and other lakehouses. Source data permissions and credentials are all managed by OneLake. When accessing data through a shortcut to another OneLake location, the identity of the calling user will be utilized to authorize access to the data in the target path of the shortcut. The user must have permissions in the target location to read the data. Shortcuts can be created in both lakehouses and KQL databases, and appear as a folder in the lake. This allows Spark, SQL, Real-Time intelligence and Analysis Services to all utilize shortcuts when querying data. Note For more information on how to use shortcuts, see OneLake shortcuts documentation in the Microsoft Fabric documentation.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-engineering/create-spark-job-definition",
            "https://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-lakehouses/media/lakehouse-items.png",
                "image_alt": "Screenshot of the three Lakehouse items as described."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-lakehouses/media/explorer-modes.png",
                "image_alt": "Screenshot of the two lakehouse Explorer modes."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Get started with lakehouses in Microsoft Fabric",
        "topic": "Explore and transform data in a lakehouse",
        "text": "Transform and load data Most data requires transformations before loading into tables. You might ingest raw data directly into a lakehouse and then further transform and load into tables. Regardless of your ETL design, you can transform and load data simply using the same tools to ingest data. Transformed data can then be loaded as a file or a Delta table. Notebooks are favored by data engineers familiar with different programming languages including PySpark, SQL, and Scala. Dataflows Gen2 are excellent for developers familiar with Power BI or Excel since they use the PowerQuery interface. Pipelines provide a visual interface to perform and orchestrate ETL processes. Pipelines can be as simple or as complex as you need. Analyze and visualize data in a lakehouse After data is ingested, transformed, and loaded, it's ready for others to use. Fabric items provide the flexibility needed for every organization so you can use the tools that work for you. Data scientists can use notebooks or Data wrangler to explore and train machine learning models for AI. Report developers can use the semantic model to create Power BI reports. Analysts can use the SQL analytics endpoint to query, filter, aggregate, and otherwise explore data in lakehouse tables. By combining the data visualization capabilities of Power BI with the centralized storage and tabular schema of a data lakehouse, you can implement an end-to-end analytics solution on a single platform.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Get started with lakehouses in Microsoft Fabric",
        "topic": "Exercise - Create a Microsoft Fabric lakehouse",
        "text": "In this exercise, explore Microsoft Fabric lakehouse tasks like creating a lakehouse, importing data, querying tables with SQL, and generating reports. The exercise emphasizes the importance of the lakehouse as a central component in data engineering, warehousing, and analytics, enabling users to effectively manage and analyze their data within the lakehouse environment. Note You need a Microsoft Fabric trial license with the Fabric preview enabled in your tenant. See Getting started with Fabric to enable your Fabric trial license. Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://aka.ms/mslearn-fabric-lakehouse"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Get started with lakehouses in Microsoft Fabric",
        "topic": "Summary",
        "text": "In this module, we explored how lakehouses fit into a data analytics solution using Microsoft Fabric. Lakehouses provide data engineers and analysts with the combined benefits of data lake storage and a relational data warehouse. You can use a lakehouse as the basis of an end-to-end data analytics solution that includes data ingestion, transformation, modeling, and visualization. Fabric lakehouses provide value as a Software-as-a-Service data store that provides all of the benefits with less administration. For more information, see the Data Engineering in Microsoft Fabric documentation.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-engineering/"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Use Apache Spark in Microsoft Fabric",
        "topic": "Introduction",
        "text": "Apache Spark is an open source parallel processing framework for large-scale data processing and analytics. Spark has become popular in \"big data\" processing scenarios, and is available in multiple platform implementations; including Azure HDInsight, Azure Synapse Analytics, and Microsoft Fabric. This module explores how you can use Spark in Microsoft Fabric to ingest, process, and analyze data in a lakehouse. While the core techniques and code described in this module are common to all Spark implementations, the integrated tools and ability to work with Spark in the same environment as other data services in Microsoft Fabric makes it easier to incorporate Spark-based data processing into your overall data analytics solution.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Use Apache Spark in Microsoft Fabric",
        "topic": "Prepare to use Apache Spark",
        "text": "Apache Spark is a distributed data processing framework that enables large-scale data analytics by coordinating work across multiple processing nodes in a cluster, known in Microsoft Fabric as a Spark pool . Put more simply, Spark uses a \"divide and conquer\" approach to processing large volumes of data quickly by distributing the work across multiple computers. The process of distributing tasks and collating results is handled for you by Spark. Spark can run code written in a wide range of languages, including Java, Scala (a Java-based scripting language), Spark R, Spark SQL, and PySpark (a Spark-specific variant of Python). In practice, most data engineering and analytics workloads are accomplished using a combination of PySpark and Spark SQL. Spark pools A Spark pool consists of compute nodes that distribute data processing tasks. The general architecture is shown in the following diagram. As shown in the diagram, a Spark pool contains two kinds of node: A head node in a Spark pool coordinates distributed processes through a driver program. The pool includes multiple worker nodes on which executor processes perform the actual data processing tasks. The Spark pool uses this distributed compute architecture to access and process data in a compatible data store - such as a data lakehouse based in OneLake. Spark pools in Microsoft Fabric Microsoft Fabric provides a starter pool in each workspace, enabling Spark jobs to be started and run quickly with minimal setup and configuration. You can configure the starter pool to optimize the nodes it contains in accordance with your specific workload needs or cost constraints. Additionally, you can create custom Spark pools with specific node configurations that support your particular data processing needs. Note The ability to customize Spark pool settings can be disabled by Fabric administrators at the Fabric Capacity level. For more information, see Capacity administration settings for Data Engineering and Data Science in the Fabric documentation. You can manage settings for the starter pool and create new Spark pools in the Admin portal section of the workspace settings, under Capacity settings , then Data Engineering/Science Settings. Specific configuration settings for Spark pools include: Node Family : The type of virtual machines used for the Spark cluster nodes. In most cases, memory optimized nodes provide optimal performance. Autoscale : Whether or not to automatically provision nodes as needed, and if so, the initial and maximum number of nodes to be allocated to the pool. Dynamic allocation : Whether or not to dynamically allocate executor processes on the worker nodes based on data volumes. If you create one or more custom Spark pools in a workspace, you can set one of them (or the starter pool) as the default pool to be used if a specific pool is not specified for a given Spark job. Tip For more information about managing Spark pools in Microsoft Fabric, see Configuring starter pools in Microsoft Fabric and How to create custom Spark pools in Microsoft Fabric in the Microsoft Fabric documentation. Runtimes and environments The Spark open source ecosystem includes multiple versions of the Spark runtime , which determines the version of Apache Spark, Delta Lake, Python, and other core software components that are installed. Additionally, within a runtime you can install and use a wide selection of code libraries for common (and sometimes very specialized) tasks. Since a great deal of Spark processing is performed using PySpark, the huge range of Python libraries ensures that whatever the task you need to perform, there's probably a library to help. In some cases, organizations may need to define multiple environments to support a diverse range of data processing tasks. Each environment defines a specific runtime version as well as the libraries that must be installed to perform specific operations. Data engineers and scientists can then select which environment they want to use with a Spark pool for a particular task. Spark runtimes in Microsoft Fabric Microsoft Fabric supports multiple Spark runtimes, and will continue to add support for new runtimes as they are released. You can use the workspace settings interface to specify the Spark runtime that is used by default environment when a Spark pool is started. Tip For more information about Spark runtimes in Microsoft Fabric, see Apache Spark Runtimes in Fabric in the Microsoft Fabric documentation. Environments in Microsoft Fabric You can create custom environments in a Fabric workspace, enabling you to use specific Spark runtimes, libraries, and configuration settings for different data processing operations. When creating an environment, you can: Specify the Spark runtime it should use. View the built-in libraries that are installed in every environment. Install specific public libraries from the Python Package Index (PyPI). Install custom libraries by uploading a package file. Specify the Spark pool that the environment should use. Specify Spark configuration properties to override default behavior. Upload resource files that need to be available in the environment. After creating at least one custom environment, you can specify it as the default environment in the workspace settings. Tip For more information about using custom environments in Microsoft Fabric, see Create, configure, and use an environment in Microsoft Fabric in the Microsoft Fabric documentation. Additional Spark configuration options Managing Spark pools and environments are the primary ways in which you can manage Spark processing in a Fabric workspace. However, there are some additional options that you can use to make further optimizations. Native execution engine The native execution engine in Microsoft Fabric is a vectorized processing engine that runs Spark operations directly on lakehouse infrastructure. Using the native execution engine can significantly improve the performance of queries when working with large data sets in Parquet or Delta file formats. To use the native execution engine, you can enable it at the environment level or within an individual notebook. To enable the native execution engine at the environment level, set the following Spark properties in the environment configuration: spark.native.enabled : true spark.shuffle.manager : org.apache.spark.shuffle.sort.ColumnarShuffleManager To enable the native execution engine for a specific script or notebook, you can set these configuration properties at the beginning of your code, like this: %%configure \n{ \n   \"conf\": {\n       \"spark.native.enabled\": \"true\", \n       \"spark.shuffle.manager\": \"org.apache.spark.shuffle.sort.ColumnarShuffleManager\" \n   } \n} Tip For more information about the native execution engine, see Native execution engine for Fabric Spark in the Microsoft Fabric documentation. High concurrency mode When you run Spark code in Microsoft Fabric, a Spark session is initiated. You can optimize the efficiency of Spark resource usage by using high concurrency mode to share Spark sessions across multiple concurrent users or processes. A notebook uses a Spark session for its execution. When high concurrency mode is enabled, multiple users can, for example, run code in notebooks that use the same Spark session, while ensuring isolation of code to avoid variables in one notebook being affected by code in another notebook. You can also enable high concurrency mode for Spark jobs, enabling similar efficiencies for concurrent non-interactive Spark script execution. To enable high concurrency mode, use the Data Engineering/Science section of the workspace settings interface. Tip For more information about high concurrency mode, see High concurrency mode in Apache Spark for Fabric in the Microsoft Fabric documentation. Automatic MLFlow logging MLFlow is an open source library that is used in data science workloads to manage machine learning training and model deployment. A key capability of MLFlow is the ability to log model training and management operations. By default, Microsoft Fabric uses MLFlow to implicitly log machine learning experiment activity without requiring the data scientist to include explicit code to do so. You can disable this functionality in the workspace settings. Spark administration for a Fabric capacity Administrators can manage Spark settings at a Fabric capacity level, enabling them to restrict and override Spark settings in workspaces within an organization. Tip For more information about managing Spark configuration at the Fabric capacity level, see Configure and manage data engineering and data science settings for Fabric capacities in the Microsoft Fabric documentation.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-engineering/capacity-settings-overview?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-engineering/configure-starter-pools?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-engineering/create-custom-spark-pools?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-engineering/runtime?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-engineering/create-and-use-environment?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-engineering/native-execution-engine-overview?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-engineering/high-concurrency-overview?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-engineering/capacity-settings-management?azure-portal-true"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-apache-spark-work-files-lakehouse/media/spark-pool.png",
                "image_alt": "Diagram of a Spark pool."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-apache-spark-work-files-lakehouse/media/spark-settings.png",
                "image_alt": "Screenshot of the Spark settings page in Microsoft Fabric."
            },
            {
                "image_name": "image3",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-apache-spark-work-files-lakehouse/media/spark-environment.png",
                "image_alt": "Screenshot of the Environment page in Microsoft Fabric."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Use Apache Spark in Microsoft Fabric",
        "topic": "Run Spark code",
        "text": "To edit and run Spark code in Microsoft Fabric, you can use notebooks , or you can define a Spark job . Notebooks When you want to use Spark to explore and analyze data interactively, use a notebook. Notebooks enable you to combine text, images, and code written in multiple languages to create an interactive item that you can share with others and collaborate on. Notebooks consist of one or more cells , each of which can contain markdown-formatted content or executable code. You can run the code interactively in the notebook and see the results immediately. Spark job definition If you want to use Spark to ingest and transform data as part of an automated process, you can define a Spark job to run a script on-demand or based on a schedule. To configure a Spark job, create a Spark Job Definition in your workspace and specify the script it should run. You can also specify a reference file (for example, a Python code file containing definitions of functions that are used in your script) and a reference to a specific lakehouse containing data that the script processes.",
        "links": null,
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-apache-spark-work-files-lakehouse/media/notebook.png",
                "image_alt": "Screenshot of a notebook in Microsoft Fabric."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-apache-spark-work-files-lakehouse/media/spark-job.png",
                "image_alt": "Screenshot of a Spark job definition in Microsoft Fabric."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Use Apache Spark in Microsoft Fabric",
        "topic": "Work with data in a Spark dataframe",
        "text": "Natively, Spark uses a data structure called a resilient distributed dataset (RDD); but while you can write code that works directly with RDDs, the most commonly used data structure for working with structured data in Spark is the dataframe , which is provided as part of the Spark SQL library. Dataframes in Spark are similar to those in the ubiquitous Pandas Python library, but optimized to work in Spark's distributed processing environment. Note In addition to the Dataframe API, Spark SQL provides a strongly-typed Dataset API that is supported in Java and Scala. We'll focus on the Dataframe API in this module. Loading data into a dataframe Let's explore a hypothetical example to see how you can use a dataframe to work with data. Suppose you have the following data in a comma-delimited text file named products.csv in the Files/data folder in your lakehouse: ProductID,ProductName,Category,ListPrice\n771,\"Mountain-100 Silver, 38\",Mountain Bikes,3399.9900\n772,\"Mountain-100 Silver, 42\",Mountain Bikes,3399.9900\n773,\"Mountain-100 Silver, 44\",Mountain Bikes,3399.9900\n... Inferring a schema In a Spark notebook, you could use the following PySpark code to load the file data into a dataframe and display the first 10 rows: %%pyspark\ndf = spark.read.load('Files/data/products.csv',\n    format='csv',\n    header=True\n)\ndisplay(df.limit(10)) The %%pyspark line at the beginning is called a magic , and tells Spark that the language used in this cell is PySpark. You can select the language you want to use as a default in the toolbar of the Notebook interface, and then use a magic to override that choice for a specific cell. For example, here's the equivalent Scala code for the products data example: %%spark\nval df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"Files/data/products.csv\")\ndisplay(df.limit(10)) The magic %%spark is used to specify Scala. Both of these code samples would produce output like this: ProductID ProductName Category ListPrice 771 Mountain-100 Silver, 38 Mountain Bikes 3399.9900 772 Mountain-100 Silver, 42 Mountain Bikes 3399.9900 773 Mountain-100 Silver, 44 Mountain Bikes 3399.9900 ... ... ... ... Specifying an explicit schema In the previous example, the first row of the CSV file contained the column names, and Spark was able to infer the data type of each column from the data it contains. You can also specify an explicit schema for the data, which is useful when the column names aren't included in the data file, like this CSV example: 771,\"Mountain-100 Silver, 38\",Mountain Bikes,3399.9900\n772,\"Mountain-100 Silver, 42\",Mountain Bikes,3399.9900\n773,\"Mountain-100 Silver, 44\",Mountain Bikes,3399.9900\n... The following PySpark example shows how to specify a schema for the dataframe to be loaded from a file named product-data.csv in this format: from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nproductSchema = StructType([\n    StructField(\"ProductID\", IntegerType()),\n    StructField(\"ProductName\", StringType()),\n    StructField(\"Category\", StringType()),\n    StructField(\"ListPrice\", FloatType())\n    ])\n\ndf = spark.read.load('Files/data/product-data.csv',\n    format='csv',\n    schema=productSchema,\n    header=False)\ndisplay(df.limit(10)) The results would once again be similar to: ProductID ProductName Category ListPrice 771 Mountain-100 Silver, 38 Mountain Bikes 3399.9900 772 Mountain-100 Silver, 42 Mountain Bikes 3399.9900 773 Mountain-100 Silver, 44 Mountain Bikes 3399.9900 ... ... ... ... Tip Specifying an explicit schema also improves performance! Filtering and grouping dataframes You can use the methods of the Dataframe class to filter, sort, group, and otherwise manipulate the data it contains. For example, the following code example uses the select method to retrieve the ProductID and ListPrice columns from the df dataframe containing product data in the previous example: pricelist_df = df.select(\"ProductID\", \"ListPrice\") The results from this code example would look something like this: ProductID ListPrice 771 3399.9900 772 3399.9900 773 3399.9900 ... ... In common with most data manipulation methods, select returns a new dataframe object. Tip Selecting a subset of columns from a dataframe is a common operation, which can also be achieved by using the following shorter syntax: pricelist_df = df[\"ProductID\", \"ListPrice\"] You can \"chain\" methods together to perform a series of manipulations that results in a transformed dataframe. For example, this example code chains the select and where methods to create a new dataframe containing the ProductName and ListPrice columns for products with a category of Mountain Bikes or Road Bikes : bikes_df = df.select(\"ProductName\", \"Category\", \"ListPrice\").where((df[\"Category\"]==\"Mountain Bikes\") | (df[\"Category\"]==\"Road Bikes\"))\ndisplay(bikes_df) The results from this code example would look something like this: ProductName Category ListPrice Mountain-100 Silver, 38 Mountain Bikes 3399.9900 Road-750 Black, 52 Road Bikes 539.9900 ... ... ... To group and aggregate data, you can use the groupBy method and aggregate functions. For example, the following PySpark code counts the number of products for each category: counts_df = df.select(\"ProductID\", \"Category\").groupBy(\"Category\").count()\ndisplay(counts_df) The results from this code example would look something like this: Category count Headsets 3 Wheels 14 Mountain Bikes 32 ... ... Saving a dataframe You'll often want to use Spark to transform raw data and save the results for further analysis or downstream processing. The following code example saves the dataFrame into a parquet file in the data lake, replacing any existing file of the same name. bikes_df.write.mode(\"overwrite\").parquet('Files/product_data/bikes.parquet') Note The Parquet format is typically preferred for data files that you will use for further analysis or ingestion into an analytical store. Parquet is a very efficient format that is supported by most large scale data analytics systems. In fact, sometimes your data transformation requirement may simply be to convert data from another format (such as CSV) to Parquet! Partitioning the output file Partitioning is an optimization technique that enables Spark to maximize performance across the worker nodes. More performance gains can be achieved when filtering data in queries by eliminating unnecessary disk IO. To save a dataframe as a partitioned set of files, use the partitionBy method when writing the data. The following example saves the bikes_df dataframe (which contains the product data for the mountain bikes and road bikes categories), and partitions the data by category: bikes_df.write.partitionBy(\"Category\").mode(\"overwrite\").parquet(\"Files/bike_data\") The folder names generated when partitioning a dataframe include the partitioning column name and value in a column=value format, so the code example creates a folder named bike_data that contains the following subfolders: Category=Mountain Bikes Category=Road Bikes Each subfolder contains one or more parquet files with the product data for the appropriate category. Note You can partition the data by multiple columns, which results in a hierarchy of folders for each partitioning key. For example, you might partition sales order data by year and month, so that the folder hierarchy includes a folder for each year value, which in turn contains a subfolder for each month value. Load partitioned data When reading partitioned data into a dataframe, you can load data from any folder within the hierarchy by specifying explicit values or wildcards for the partitioned fields. The following example loads data for products in the Road Bikes category: road_bikes_df = spark.read.parquet('Files/bike_data/Category=Road Bikes')\ndisplay(road_bikes_df.limit(5)) Note The partitioning columns specified in the file path are omitted in the resulting dataframe. The results produced by the example query would not include a Category column - the category for all rows would be Road Bikes .",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Use Apache Spark in Microsoft Fabric",
        "topic": "Work with data using Spark SQL",
        "text": "The Dataframe API is part of a Spark library named Spark SQL, which enables data analysts to use SQL expressions to query and manipulate data. Creating database objects in the Spark catalog The Spark catalog is a metastore for relational data objects such as views and tables. The Spark runtime can use the catalog to seamlessly integrate code written in any Spark-supported language with SQL expressions that may be more natural to some data analysts or developers. One of the simplest ways to make data in a dataframe available for querying in the Spark catalog is to create a temporary view, as shown in the following code example: df.createOrReplaceTempView(\"products_view\") A view is temporary, meaning that it's automatically deleted at the end of the current session. You can also create tables that are persisted in the catalog to define a database that can be queried using Spark SQL. Tables are metadata structures that store their underlying data in the storage location associated with the catalog. In Microsoft Fabric, data for managed tables is stored in the Tables storage location shown in your data lake, and any tables created using Spark are listed there. You can create an empty table by using the spark.catalog.createTable method, or you can save a dataframe as a table by using its saveAsTable method. Deleting a managed table also deletes its underlying data. For example, the following code saves a dataframe as a new table named products : df.write.format(\"delta\").saveAsTable(\"products\") Note The Spark catalog supports tables based on files in various formats. The preferred format in Microsoft Fabric is delta , which is the format for a relational data technology on Spark named Delta Lake . Delta tables support features commonly found in relational database systems, including transactions, versioning, and support for streaming data. Additionally, you can create external tables by using the spark.catalog.createExternalTable method. External tables define metadata in the catalog but get their underlying data from an external storage location; typically a folder in the Files storage area of a lakehouse. Deleting an external table doesn't delete the underlying data. Tip You can apply the same partitioning technique to delta lake tables as discussed for parquet files in the previous unit. Partitioning tables can result in better performance when querying them. Using the Spark SQL API to query data You can use the Spark SQL API in code written in any language to query data in the catalog. For example, the following PySpark code uses a SQL query to return data from the products table as a dataframe. bikes_df = spark.sql(\"SELECT ProductID, ProductName, ListPrice \\\n                      FROM products \\\n                      WHERE Category IN ('Mountain Bikes', 'Road Bikes')\")\ndisplay(bikes_df) The results from the code example would look similar to the following table: ProductID ProductName ListPrice 771 Mountain-100 Silver, 38 3399.9900 839 Road-750 Black, 52 539.9900 ... ... ... Using SQL code The previous example demonstrated how to use the Spark SQL API to embed SQL expressions in Spark code. In a notebook, you can also use the %%sql magic to run SQL code that queries objects in the catalog, like this: %%sql\n\nSELECT Category, COUNT(ProductID) AS ProductCount\nFROM products\nGROUP BY Category\nORDER BY Category The SQL code example returns a resultset that is automatically displayed in the notebook as a table: Category ProductCount Bib-Shorts 3 Bike Racks 1 Bike Stands 1 ... ...",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Use Apache Spark in Microsoft Fabric",
        "topic": "Visualize data in a Spark notebook",
        "text": "One of the most intuitive ways to analyze the results of data queries is to visualize them as charts. Notebooks in Microsoft Fabric provide some basic charting capabilities in the user interface, and when that functionality doesn't provide what you need, you can use one of the many Python graphics libraries to create and display data visualizations in the notebook. Using built-in notebook charts When you display a dataframe or run a SQL query in a Spark notebook, the results are displayed under the code cell. By default, results are rendered as a table, but you can also change the results view to a chart and use the chart properties to customize how the chart visualizes the data, as shown here: The built-in charting functionality in notebooks is useful when you want to quickly summarize the data visually. When you want to have more control over how the data is formatted, you should consider using a graphics package to create your own visualizations. Using graphics packages in code There are many graphics packages that you can use to create data visualizations in code. In particular, Python supports a large selection of packages; most of them built on the base Matplotlib library. The output from a graphics library can be rendered in a notebook, making it easy to combine code to ingest and manipulate data with inline data visualizations and markdown cells to provide commentary. For example, you could use the following PySpark code to aggregate data from the hypothetical products data explored previously in this module, and use Matplotlib to create a chart from the aggregated data. from matplotlib import pyplot as plt\n\n# Get the data as a Pandas dataframe\ndata = spark.sql(\"SELECT Category, COUNT(ProductID) AS ProductCount \\\n                  FROM products \\\n                  GROUP BY Category \\\n                  ORDER BY Category\").toPandas()\n\n# Clear the plot area\nplt.clf()\n\n# Create a Figure\nfig = plt.figure(figsize=(12,8))\n\n# Create a bar plot of product counts by category\nplt.bar(x=data['Category'], height=data['ProductCount'], color='orange')\n\n# Customize the chart\nplt.title('Product Counts by Category')\nplt.xlabel('Category')\nplt.ylabel('Products')\nplt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)\nplt.xticks(rotation=70)\n\n# Show the plot area\nplt.show() The Matplotlib library requires data to be in a Pandas dataframe rather than a Spark dataframe, so the toPandas method is used to convert it. The code then creates a figure with a specified size and plots a bar chart with some custom property configuration before showing the resulting plot. The chart produced by the code would look similar to the following image: You can use the Matplotlib library to create many kinds of chart; or if preferred, you can use other libraries such as Seaborn to create highly customized charts.",
        "links": null,
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-apache-spark-work-files-lakehouse/media/notebook-chart.png",
                "image_alt": "Screenshot of notebook chart of product counts by category."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-apache-spark-work-files-lakehouse/media/chart.png",
                "image_alt": "Screenshot of a bar chart showing product counts by category."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Use Apache Spark in Microsoft Fabric",
        "topic": "Exercise - Analyze data with Apache Spark",
        "text": "Now it's your opportunity to work with Apache Spark in Microsoft Fabric. In this exercise, you'll use a Spark notebook to analyze and visualize data from files in lakehouse. Note You need a Microsoft Fabric trial license with the Fabric preview enabled in your tenant. See Getting started with Fabric to enable your Fabric trial license. Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://go.microsoft.com/fwlink/?linkid=2259707"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Use Apache Spark in Microsoft Fabric",
        "topic": "Summary",
        "text": "Apache Spark is a key technology used in big data analytics. Spark support in Microsoft Fabric enables you to integrate big data processing in Spark with the other data analytics and visualization capabilities of the platform. Tip For more information about working with data in Spark, see the Spark SQL, DataFrames and Datasets Guide in the Apache Spark documentation.",
        "links": [
            "https://spark.apache.org/docs/latest/sql-programming-guide.html"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Work with Delta Lake tables in Microsoft Fabric",
        "topic": "Introduction",
        "text": "Tables in a Microsoft Fabric lakehouse are based on the\nLinux foundation Delta Lake table format, commonly used in Apache Spark. Delta Lake is an open-source storage layer for Spark that enables relational database capabilities for batch and streaming data. By using Delta Lake, you can implement a lakehouse architecture to support SQL-based data manipulation semantics in Spark with support for transactions and schema enforcement. The result is an analytical data store that offers many of the advantages of a relational database system with the flexibility of data file storage in a data lake. While you don't need to work directly with Delta Lake APIs in order to use tables in a Fabric lakehouse, an understanding of the Delta Lake metastore architecture and familiarity with some of the more specialized Delta table operations can greatly expand your ability to build advanced analytics solutions on Microsoft Fabric.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Work with Delta Lake tables in Microsoft Fabric",
        "topic": "Understand Delta Lake",
        "text": "Delta Lake is an open-source storage layer that adds relational database semantics to Spark-based data lake processing. Tables in Microsoft Fabric lakehouses are Delta tables, which is signified by the triangular Delta (  ) icon on tables in the lakehouse user interface. Delta tables are schema abstractions over data files that are stored in Delta format. For each table, the lakehouse stores a folder containing Parquet data files and a _delta_Log folder in which transaction details are logged in JSON format. The benefits of using Delta tables include: Relational tables that support querying and data modification . With Apache Spark, you can store data in Delta tables that support CRUD (create, read, update, and delete) operations. In other words, you can select , insert , update , and delete rows of data in the same way you would in a relational database system. Support for ACID transactions . Relational databases are designed to support transactional data modifications that provide atomicity (transactions complete as a single unit of work), consistency (transactions leave the database in a consistent state), isolation (in-process transactions can't interfere with one another), and durability (when a transaction completes, the changes it made are persisted). Delta Lake brings this same transactional support to Spark by implementing a transaction log and enforcing serializable isolation for concurrent operations. Data versioning and time travel . Because all transactions are logged in the transaction log, you can track multiple versions of each table row and even use the time travel feature to retrieve a previous version of a row in a query. Support for batch and streaming data . While most relational databases include tables that store static data, Spark includes native support for streaming data through the Spark Structured Streaming API. Delta Lake tables can be used as both sinks (destinations) and sources for streaming data. Standard formats and interoperability . The underlying data for Delta tables is stored in Parquet format, which is commonly used in data lake ingestion pipelines. Additionally, you can use the SQL analytics endpoint for the Microsoft Fabric lakehouse to query Delta tables in SQL.",
        "links": null,
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/work-delta-lake-tables-fabric/media/delta-table.png",
                "image_alt": "Screenshot of the salesorders table viewed in the Lakehouse explorer in Microsoft Fabric."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/work-delta-lake-tables-fabric/media/delta-files.png",
                "image_alt": "Screenshot of the files view of the parquet files in the salesorders table viewed through Lakehouse explorer."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Work with Delta Lake tables in Microsoft Fabric",
        "topic": "Create delta tables",
        "text": "When you create a table in a Microsoft Fabric lakehouse, a delta table is defined in the metastore for the lakehouse and the data for the table is stored in the underlying Parquet files for the table. With most interactive tools in the Microsoft Fabric environment, the details of mapping the table definition in the metastore to the underlying files are abstracted. However, when working with Apache Spark in a lakehouse, you have greater control of the creation and management of delta tables. Creating a delta table from a dataframe One of the easiest ways to create a delta table in Spark is to save a dataframe in the delta format. For example, the following PySpark code loads a dataframe with data from an existing file, and then saves that dataframe as a delta table: # Load a file into a dataframe\ndf = spark.read.load('Files/mydata.csv', format='csv', header=True)\n\n# Save the dataframe as a delta table\ndf.write.format(\"delta\").saveAsTable(\"mytable\") The code specifies that the table should be saved in delta format with a specified table name. The data for the table is saved in Parquet files (regardless of the format of the source file you loaded into the dataframe) in the Tables storage area in the lakehouse, along with a _delta_log folder containing the transaction logs for the table. The table is listed in the Tables folder for the lakehouse in the Data explorer pane. Managed vs external tables In the previous example, the dataframe was saved as a managed table; meaning that the table definition in the metastore and the underlying data files are both managed by the Spark runtime for the Fabric lakehouse. Deleting the table will also delete the underlying files from the Tables storage location for the lakehouse. You can also create tables as external tables, in which the relational table definition in the metastore is mapped to an alternative file storage location. For example, the following code creates an external table for which the data is stored in the folder in the Files storage location for the lakehouse: df.write.format(\"delta\").saveAsTable(\"myexternaltable\", path=\"Files/myexternaltable\") In this example, the table definition is created in the metastore (so the table is listed in the Tables user interface for the lakehouse), but the Parquet data files and JSON log files for the table are stored in the Files storage location (and will be shown in the Files node in the Lakehouse explorer pane). You can also specify a fully qualified path for a storage location, like this: df.write.format(\"delta\").saveAsTable(\"myexternaltable\", path=\"abfss://my_store_url..../myexternaltable\") Deleting an external table from the lakehouse metastore doesn't delete the associated data files. Creating table metadata While it's common to create a table from existing data in a dataframe, there are often scenarios where you want to create a table definition in the metastore that will be populated with data in other ways. There are multiple ways you can accomplish this goal. Use the DeltaTableBuilder API The DeltaTableBuilder API enables you to write Spark code to create a table based on your specifications. For example, the following code creates a table with a specified name and columns. from delta.tables import *\n\nDeltaTable.create(spark) \\\n  .tableName(\"products\") \\\n  .addColumn(\"Productid\", \"INT\") \\\n  .addColumn(\"ProductName\", \"STRING\") \\\n  .addColumn(\"Category\", \"STRING\") \\\n  .addColumn(\"Price\", \"FLOAT\") \\\n  .execute() Use Spark SQL You can also create delta tables by using the Spark SQL CREATE TABLE statement, as shown in this example: %%sql\n\nCREATE TABLE salesorders\n(\n    Orderid INT NOT NULL,\n    OrderDate TIMESTAMP NOT NULL,\n    CustomerName STRING,\n    SalesTotal FLOAT NOT NULL\n)\nUSING DELTA The previous example creates a managed table. You can also create an external table by specifying a LOCATION parameter, as shown here: %%sql\n\nCREATE TABLE MyExternalTable\nUSING DELTA\nLOCATION 'Files/mydata' When creating an external table, the schema of the table is determined by the Parquet files containing the data in the specified location. This approach can be useful when you want to create a table definition that references data that has already been saved in delta format, or based on a folder where you expect to ingest data in delta format. Saving data in delta format So far you've seen how to save a dataframe as a delta table (creating both the table schema definition in the metastore and the data files in delta format) and how to create the table definition (which creates the table schema in the metastore without saving any data files). A third possibility is to save data in delta format without creating a table definition in the metastore. This approach can be useful when you want to persist the results of data transformations performed in Spark in a file format over which you can later \"overlay\" a table definition or process directly by using the delta lake API. For example, the following PySpark code saves a dataframe to a new folder location in delta format: delta_path = \"Files/mydatatable\"\ndf.write.format(\"delta\").save(delta_path) Delta files are saved in Parquet format in the specified path, and include a _delta_log folder containing transaction log files. Transaction logs record any changes in the data, such as updates made to external tables or through the delta lake API. You can replace the contents of an existing folder with the data in a dataframe by using the overwrite mode, as shown here: new_df.write.format(\"delta\").mode(\"overwrite\").save(delta_path) You can also add rows from a dataframe to an existing folder by using the append mode: new_rows_df.write.format(\"delta\").mode(\"append\").save(delta_path) Tip If you use the technique described here to save a dataframe to the Tables location in the lakehouse, Microsoft Fabric uses an automatic table discovery capability to create the corresponding table metadata in the metastore.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Work with Delta Lake tables in Microsoft Fabric",
        "topic": "Optimize delta tables",
        "text": "Spark is a parallel-processing framework, with data stored on one or more worker nodes. In addition, Parquet files are immutable, with new files written for every update or delete. This process can result in Spark storing data in a large number of small files, known as the small file problem. It means that queries over large amounts of data can run slowly, or even fail to complete. OptimizeWrite function OptimizeWrite is a feature of Delta Lake which reduces the number of files as they're written. Instead of writing many small files, it writes fewer larger files. This helps to prevent the small files problem and ensure that performance isn't degraded. In Microsoft Fabric, OptimizeWrite is enabled by default. You can enable or disable it at the Spark session level: # Disable Optimize Write at the Spark session level\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", False)\n\n# Enable Optimize Write at the Spark session level\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", True)\n\nprint(spark.conf.get(\"spark.microsoft.delta.optimizeWrite.enabled\")) Note OptimizeWrite can also be set in Table Properties and for individual write commands. Optimize Optimize is a table maintenance feature that consolidates small Parquet files into fewer large files. You might run Optimize after loading large tables, resulting in: fewer larger files better compression efficient data distribution across nodes To run Optimize: In Lakehouse Explorer , select the ... menu beside a table name and select Maintenance . Select Run OPTIMIZE command . Optionally, select Apply V-order to maximize reading speeds in Fabric . Select Run now . V-Order function When you run Optimize, you can optionally run V-Order, which is designed for the Parquet file format in Fabric. V-Order enables lightning-fast reads, with in-memory-like data access times. It also improves cost efficiency as it reduces network, disk, and CPU resources during reads. V-Order is enabled by default in Microsoft Fabric and is applied as data is being written. It incurs a small overhead of about 15% making writes a little slower. However, V-Order enables faster reads from the Microsoft Fabric compute engines, such as Power BI, SQL, Spark, and others. In Microsoft Fabric, the Power BI and SQL engines use Microsoft Verti-Scan technology which takes full advantage of V-Order optimization to speed up reads. Spark and other engines don't use VertiScan technology but still benefit from V-Order optimization by about 10% faster reads, sometimes up to 50%. V-Order works by applying special sorting, row group distribution, dictionary encoding, and compression on Parquet files. It's 100% compliant to the open-source Parquet format and all Parquet engines can read it. V-Order might not be beneficial for write-intensive scenarios such as staging data stores where data is only read once or twice. In these situations, disabling V-Order might reduce the overall processing time for data ingestion. Apply V-Order to individual tables by using the Table Maintenance feature by running the OPTIMIZE command. Vacuum The VACUUM command enables you to remove old data files. Every time an update or delete is done, a new Parquet file is created and an entry is made in the transaction log. Old Parquet files are retained to enable time travel, which means that Parquet files accumulate over time. The VACUUM command removes old Parquet data files, but not the transaction logs. When you run VACUUM, you can't time travel back earlier than the retention period. Data files that aren't currently referenced in a transaction log and that are older than the specified retention period are permanently deleted by running VACUUM. Choose your retention period based on factors such as: Data retention requirements Data size and storage costs Data change frequency Regulatory requirements The default retention period is 7 days (168 hours), and the system prevents you from using a shorter retention period. You can run VACUUM on an ad-hoc basis or scheduled using Fabric notebooks. Run VACUUM on individual tables by using the Table maintenance feature: In Lakehouse Explorer , select the ... menu beside a table name and select Maintenance . Select Run VACUUM command using retention threshold and set the retention threshold. Select Run now . You can also run VACUUM as a SQL command in a notebook: %%sql\nVACUUM lakehouse2.products RETAIN 168 HOURS; VACUUM commits to the Delta transaction log, so you can view previous runs in DESCRIBE HISTORY. %%sql\nDESCRIBE HISTORY lakehouse2.products; Partitioning Delta tables Delta Lake allows you to organize data into partitions. This might improve performance by enabling data skipping , which boosts performance by skipping over irrelevant data objects based on an object's metadata. Consider a situation where large amounts of sales data are being stored. You could partition sales data by year. The partitions are stored in subfolders named \"year=2021\", \"year=2022\", etc. If you only want to report on sales data for 2024, then the partitions for other years can be skipped, which improves read performance. Partitioning of small amounts of data can degrade performance, however, because it increases the number of files and can exacerbate the \"small files problem.\" Use partitioning when: You have very large amounts of data. Tables can be split into a few large partitions. Don't use partitioning when: Data volumes are small. A partitioning column has high cardinality, as this creates a large number of partitions. A partitioning column would result in multiple levels. Partitions are a fixed data layout and don't adapt to different query patterns. When considering how to use partitioning, think about how your data is used, and its granularity. In this example, a DataFrame containing product data is partitioned by Category: df.write.format(\"delta\").partitionBy(\"Category\").saveAsTable(\"partitioned_products\", path=\"abfs_path/partitioned_products\") In the Lakehouse Explorer, you can see the data is a partitioned table. There's one folder for the table, called \"partitioned_products.\" There are subfolders for each category, for example \"Category=Bike Racks\", etc. We can create a similar partitioned table using SQL: %%sql\nCREATE TABLE partitioned_products (\n    ProductID INTEGER,\n    ProductName STRING,\n    Category STRING,\n    ListPrice DOUBLE\n)\nPARTITIONED BY (Category);",
        "links": null,
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/work-delta-lake-tables-fabric/media/optimize-write.png",
                "image_alt": "Diagram showing how Optimize Write writes fewer large files."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/work-delta-lake-tables-fabric/media/optimize-command.png",
                "image_alt": "Diagram showing how Optimize consolidates Parquet files."
            },
            {
                "image_name": "image3",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/work-delta-lake-tables-fabric/media/table-maintenance-v-order.png",
                "image_alt": "Screen picture of table maintenance with V-order selected"
            },
            {
                "image_name": "image4",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/work-delta-lake-tables-fabric/media/how-vacuum-works.png",
                "image_alt": "Diagram showing how vacuum works."
            },
            {
                "image_name": "image5",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/work-delta-lake-tables-fabric/media/table-maintenance-vacuum.png",
                "image_alt": "Screen picture showing the table maintenance options."
            },
            {
                "image_name": "image6",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/work-delta-lake-tables-fabric/media/partitioning.png",
                "image_alt": "Diagram showing partitioning by one or more columns."
            },
            {
                "image_name": "image7",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/work-delta-lake-tables-fabric/media/explorer-partitioned-table.png",
                "image_alt": "Screen picture of the lakehouse explorer and the product file partitioned by category."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Work with Delta Lake tables in Microsoft Fabric",
        "topic": "Work with delta tables in Spark",
        "text": "You can work with delta tables (or delta format files) to retrieve and modify data in multiple ways. Using Spark SQL The most common way to work with data in delta tables in Spark is to use Spark SQL. You can embed SQL statements in other languages (such as PySpark or Scala) by using the spark.sql library. For example, the following code inserts a row into the products table. spark.sql(\"INSERT INTO products VALUES (1, 'Widget', 'Accessories', 2.99)\") Alternatively, you can use the %%sql magic in a notebook to run SQL statements. %%sql\n\nUPDATE products\nSET ListPrice = 2.49 WHERE ProductId = 1; Use the Delta API When you want to work with delta files rather than catalog tables, it may be simpler to use the Delta Lake API. You can create an instance of a DeltaTable from a folder location containing files in delta format, and then use the API to modify the data in the table. from delta.tables import *\nfrom pyspark.sql.functions import *\n\n# Create a DeltaTable object\ndelta_path = \"Files/mytable\"\ndeltaTable = DeltaTable.forPath(spark, delta_path)\n\n# Update the table (reduce price of accessories by 10%)\ndeltaTable.update(\n    condition = \"Category == 'Accessories'\",\n    set = { \"Price\": \"Price * 0.9\" }) Use time travel to work with table versioning Modifications made to delta tables are logged in the transaction log for the table. You can use the logged transactions to view the history of changes made to the table and to retrieve older versions of the data (known as time travel ) To see the history of a table, you can use the DESCRIBE SQL command as shown here. %%sql\n\nDESCRIBE HISTORY products The results of this statement show the transactions that have been applied to the table, as shown here (some columns have been omitted): version timestamp operation operationParameters 2 2023-04-04T21:46:43Z UPDATE {\"predicate\":\"(ProductId = 1)\"} 1 2023-04-04T21:42:48Z WRITE {\"mode\":\"Append\",\"partitionBy\":\"[]\"} 0 2023-04-04T20:04:23Z CREATE TABLE {\"isManaged\":\"true\",\"description\":null,\"partitionBy\":\"[]\",\"properties\":\"{}\"} To see the history of an external table, you can specify the folder location instead of the table name. %%sql\n\nDESCRIBE HISTORY 'Files/mytable' You can retrieve data from a specific version of the data by reading the delta file location into a dataframe, specifying the version required as a versionAsOf option: df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path) Alternatively, you can specify a timestamp by using the timestampAsOf option: df = spark.read.format(\"delta\").option(\"timestampAsOf\", '2022-01-01').load(delta_path)",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Work with Delta Lake tables in Microsoft Fabric",
        "topic": "Use delta tables with streaming data",
        "text": "All of the data we explored up to this point has been static data in files. However, many data analytics scenarios involve streaming data that must be processed in near real time. For example, you might need to capture readings emitted by internet-of-things (IoT) devices and store them in a table as they occur. Spark processes batch data and streaming data in the same way, enabling streaming data to be processed in real-time using the same API. Spark Structured Streaming A typical stream processing solution involves: Constantly reading a stream of data from a source . Optionally, processing the data to select specific fields, aggregate and group values, or otherwise manipulating the data. Writing the results to a sink . Spark includes native support for streaming data through Spark Structured Streaming , an API that is based on a boundless dataframe in which streaming data is captured for processing. A Spark Structured Streaming dataframe can read data from many different kinds of streaming source, including: Network ports Real time message brokering services such as Azure Event Hubs or Kafka File system locations. Tip For more information about Spark Structured Streaming, see Structured Streaming Programming Guide in the Spark documentation. Streaming with Delta tables You can use a Delta table as a source or a sink for Spark Structured Streaming. For example, you could capture a stream of real time data from an IoT device and write the stream directly to a Delta table as a sink. You can then query the table to see the latest streamed data. Or you could read a Delta as a streaming source, enabling near real-time reporting as new data is added to the table. Using a Delta table as a streaming source In the following PySpark example, a Delta table is created to store details of Internet sales orders: %%sql\nCREATE TABLE orders_in\n(\n        OrderID INT,\n        OrderDate DATE,\n        Customer STRING,\n        Product STRING,\n        Quantity INT,\n        Price DECIMAL\n)\nUSING DELTA; A hypothetical data stream of internet orders is inserted into the orders_in table: %%sql\nINSERT INTO orders_in (OrderID, OrderDate, Customer, Product, Quantity, Price)\nVALUES\n    (3001, '2024-09-01', 'Yang', 'Road Bike Red', 1, 1200),\n    (3002, '2024-09-01', 'Carlson', 'Mountain Bike Silver', 1, 1500),\n    (3003, '2024-09-02', 'Wilson', 'Road Bike Yellow', 2, 1350),\n    (3004, '2024-09-02', 'Yang', 'Road Front Wheel', 1, 115),\n    (3005, '2024-09-02', 'Rai', 'Mountain Bike Black', 1, NULL); To verify, you can read and display data from the input table: # Read and display the input table\ndf = spark.read.format(\"delta\").table(\"orders_in\")\n\ndisplay(df) The data is then loaded into a streaming DataFrame from the Delta table: # Load a streaming DataFrame from the Delta table\nstream_df = spark.readStream.format(\"delta\") \\\n    .option(\"ignoreChanges\", \"true\") \\\n    .table(\"orders_in\") Note When using a Delta table as a streaming source, only append operations can be included in the stream. Data modifications can cause an error unless you specify the ignoreChanges or ignoreDeletes option. You can check that the stream is streaming by using the isStreaming property which should return True: # Verify that the stream is streaming\nstream_df.isStreaming Transform the data stream After reading the data from the Delta table into a streaming DataFrame, you can use the Spark Structured Streaming API to process it. For example, you could count the number of orders placed every minute and send the aggregated results to a downstream process for near-real-time visualization. In this example, any rows with NULL in the Price column are filtered and new columns are added for IsBike and Total. from pyspark.sql.functions import col, expr\n\ntransformed_df = stream_df.filter(col(\"Price\").isNotNull()) \\\n    .withColumn('IsBike', expr(\"INSTR(Product, 'Bike') > 0\").cast('int')) \\\n    .withColumn('Total', expr(\"Quantity * Price\").cast('decimal')) Using a Delta table as a streaming sink The data stream is then written to a Delta table: # Write the stream to a delta table\noutput_table_path = 'Tables/orders_processed'\ncheckpointpath = 'Files/delta/checkpoint'\ndeltastream = transformed_df.writeStream.format(\"delta\").option(\"checkpointLocation\", checkpointpath).start(output_table_path)\n\nprint(\"Streaming to orders_processed...\") Note The checkpointLocation option is used to write a checkpoint file that tracks the state of the stream processing. This file enables you to recover from failure at the point where stream processing left off. After the streaming process starts, you can query the Delta Lake table to see what is in the output table. There might be a short delay before you can query the table. %%sql\nSELECT *\n    FROM orders_processed\n    ORDER BY OrderID; In the results of this query, order 3005 is excluded because it had NULL in the Price column. And the two columns that were added during the transformation are displayed - IsBike and Total. OrderID OrderDate Customer Product Quantity Price IsBike Total 3001 2023-09-01 Yang Road Bike Red 1 1200 1 1200 3002 2023-09-01 Carlson Mountain Bike Silver 1 1500 1 1500 3003 2023-09-02 Wilson Road Bike Yellow 2 1350 1 2700 3004 2023-09-02 Yang Road Front Wheel 1 115 0 115 When finished, stop the streaming data to avoid unnecessary processing costs using the stop method: # Stop the streaming data to avoid excessive processing costs\ndeltastream.stop() Tip For more information about using Delta tables for streaming data, see Table streaming reads and writes in the Delta Lake documentation.",
        "links": [
            "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html",
            "https://docs.delta.io/latest/delta-streaming.html?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Work with Delta Lake tables in Microsoft Fabric",
        "topic": "Exercise - Use delta tables in Apache Spark",
        "text": "Now it's your turn to use Apache Spark to work with delta tables in a Microsoft Fabric lakehouse. Note To complete this exercise, you need a Microsoft Fabric tenant. See Getting started with Fabric to find out how to enable a Fabric trial license. Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://go.microsoft.com/fwlink/?linkid=2259245&azure-portal=true"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Work with Delta Lake tables in Microsoft Fabric",
        "topic": "Summary",
        "text": "Delta Lake is a technology that adds relational database semantics to Apache Spark. Tables in a Microsoft Fabric lakehouse are based on Delta Lake, enabling you to take advantage of many advanced features and techniques through the Delta Lake API. Tip For more information about Delta Lake, see the Delta Lake documentation .",
        "links": [
            "https://docs.delta.io/latest/index.html?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Ingest Data with Dataflows Gen2 in Microsoft Fabric",
        "topic": "Introduction",
        "text": "Microsoft Fabric offers a unified solution for data engineering, integration, and analytics. A crucial step in end-to-end analytics is data ingestion. Dataflows Gen2 are used to ingest and transform data from multiple sources, and then land the cleansed data to another destination. They can be incorporated into data pipelines for more complex activity orchestration, and also used as a data source in Power BI. Imagine you work for a retail company with stores across the globe. As a data engineer, you need to prepare and transform data from various sources into a format that is suitable for data analysis and reporting. The business requests a semantic model that consolidates disparate data sources from the different stores. Dataflows Gen2 allow you to prepare the data to ensure consistency, and then stage the data in the preferred destination. They also enable reuse and make it easy to update the data. Without a dataflow, you'd have to manually extract and transform the data from every source, which is time-consuming and prone to errors. In this module, we explain how to use Dataflows Gen2 in Microsoft Fabric to meet your data ingestion needs.",
        "links": null,
        "images": null,
        "videos": [
            {
                "name": "video1",
                "video_src": "https://go.microsoft.com/fwlink/?linkid=2331876"
            }
        ],
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Ingest Data with Dataflows Gen2 in Microsoft Fabric",
        "topic": "Understand Dataflows Gen2 in Microsoft Fabric",
        "text": "In our scenario, you need to develop a semantic model that can standardize the data and provide access to the business. By using Dataflows Gen2, you can connect to the various data sources, and then prep and transform the data. You can land the data directly into your lakehouse or use a data pipeline for other destinations. What is a dataflow? Dataflows are a type of cloud-based ETL ( Extract, Transform, Load ) tool for building and executing scalable data transformation processes. Dataflows Gen2 allow you to extract data from various sources, transform it using a wide range of transformation operations, and load it into a destination. Using Power Query Online also allows for a visual interface to perform these tasks. Fundamentally, a dataflow includes all of the transformations to reduce data prep time and then can be loaded into a new table, included in a data pipeline, or used as a data source by data analysts. How to use Dataflows Gen2 Traditionally, data engineers spend significant time extracting, transforming, and loading data into a consumable format for downstream analytics. The goal of Dataflows Gen2 is to provide an easy, reusable way to perform ETL tasks using Power Query Online. If you only choose to use a data pipeline, you copy data, then use your preferred coding language to extract, transform, and load the data. Alternatively, you can create a Dataflow Gen2 first to extract and transform the data. You can also load the data into a lakehouse, and other destinations. Now the business can easily consume the curated semantic model. Adding a data destination to your dataflow is optional, and the dataflow preserves all transformation steps. To perform other tasks or load data to a different destination after transformation, create a data pipeline and add the Dataflow Gen2 activity to your orchestration. Another option might be to use a data pipeline and Dataflow Gen2 for ELT (Extract, Load, Transform) process. For this order, you'd use a Pipeline to extract and load the data into your preferred destination, such as the lakehouse. Then you'd create a Dataflow Gen2 to connect to Lakehouse data to cleanse and transform data. In this case, you'd offer the Dataflow as a curated semantic model for data analysts to develop reports. Dataflows can be horizontally partitioned as well. Once you create a global dataflow, data analysts can use dataflows to create specialized semantic models for specific needs. Dataflows allow you to promote reusable ETL logic that prevents the need to create more connections to your data source. Dataflows offer a wide variety of transformations, and can be run manually, on a refresh schedule, or as part of a data pipeline orchestration. Tip Make your dataflow discoverable so data analysts can also connect to the dataflow through Power BI Desktop. This reduces the data preparation for report development. Benefits and limitations There's more than one way to ETL or ELT data in Microsoft Fabric. Consider the benefits and limitations for using Dataflows Gen2. Benefits: Extend data with consistent data, such as a standard date dimension table. Allow self-service users access to a subset of data warehouse separately. Optimize performance with dataflows, which enable extracting data once for reuse, reducing data refresh time for slower sources. Simplify data source complexity by only exposing dataflows to larger analyst groups. Ensure consistency and quality of data by enabling users to clean and transform data before loading it to a destination. Simplify data integration by providing a low-code interface that ingests data from various sources. Limitations: Dataflows aren't a replacement for a data warehouse. Row-level security isn't supported. Fabric capacity workspace is required.",
        "links": null,
        "images": null,
        "videos": [
            {
                "name": "video1",
                "video_src": "https://go.microsoft.com/fwlink/?linkid=2331976"
            }
        ],
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Ingest Data with Dataflows Gen2 in Microsoft Fabric",
        "topic": "Explore Dataflows Gen2 in Microsoft Fabric",
        "text": "In Microsoft Fabric, you can create a Dataflow Gen2 in the Data Factory workload or Power BI workspace, or directly in the lakehouse. Since our scenario is focused on data ingestion, let's look at the Data Factory workload experience. Dataflows Gen2 use Power Query Online to visualize transformations. See an overview of the interface: 1. Power Query ribbon Dataflows Gen2 support a wide variety of data source connectors. Common sources include cloud and on-premises relational databases, Excel or flat files, SharePoint, SalesForce, Spark, and Fabric lakehouses. Then there are numerous data transformations possible, such as: Filter and Sort rows Pivot and Unpivot Merge and Append queries Split and Conditional split Replace values and Remove duplicates Add, Rename, Reorder, or Delete columns Rank and Percentage calculator Choose Top N and Bottom N You can also create and manage data source connections, manage parameters, and configure the default data destination in this ribbon. 2. Queries pane The Queries pane shows you the different data sources - now called queries . These queries are called tables when loaded to your data store. You can duplicate or reference a query if you need multiple copies of the same data, such as creating a star schema and splitting data into separate, smaller tables. You can also disable the load of a query, in case you only need the one-time import. 3. Diagram view The Diagram View allows you to visually see how the data sources are connected and the different applied transformations. For example, your dataflow connects to a data source, duplicates the query, removes columns from the source query, then unpivots the duplicate query. Each query is represented as a shape with all of the applied transformations and connected by a line for the duplicate query. You can turn this view on or off. 4. Data Preview pane The Data Preview pane only shows a subset of data to allow you to see which transformations you should make and how they affect the data. You can also interact with the preview pane by dragging and dropping columns to change order or right-clicking on columns to filter or make changes. The data preview shows all of your transformations for the selected query. 5. Query Settings pane The Query Settings pane includes the Applied Steps . Each transformation is represented as a step, some of which are automatically applied when you connect the data source. Depending on the complexity of the transformations, you might have several applied steps for each query. Most steps have a gear icon that allows you to modify the step, otherwise you must delete and repeat the transformation. Each step also has a contextual menu when you right-click so you can rename, reorder, or delete the steps. You can also view the data source query when connecting to a data source that supports query folding. While this visual interface is helpful, you can also view the M code through Advanced editor . In the Query settings pane, you can see a Data Destination option to land your data in one of the following locations in your Fabric environment: Lakehouse Warehouse SQL database You can also load your dataflow to Azure SQL database, Azure Data Explorer, or Azure Synapse Analytics. Dataflows Gen2 provide a low-to-no-code solution to ingest, transform, and load data into your Fabric data stores. Power BI developers are familiar and can quickly begin to perform transformations upstream to improve performance for their reports. Note For more information, see the Power Query documentation to optimize your dataflows.",
        "links": [
            "https://learn.microsoft.com/en-us/power-query/"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-dataflow-gen-2-fabric/media/power-query-online-overview.png",
                "image_alt": "Screenshot of the Power Query Online interface."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-dataflow-gen-2-fabric/media/power-query-advanced-editor.png",
                "image_alt": "Screenshot of the advanced editor with sample code"
            }
        ],
        "videos": [
            {
                "name": "video1",
                "video_src": "https://go.microsoft.com/fwlink/?linkid=2331875"
            }
        ],
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Ingest Data with Dataflows Gen2 in Microsoft Fabric",
        "topic": "Integrate Dataflows Gen2 and Pipelines in Microsoft Fabric",
        "text": "Dataflows Gen2 provide an excellent option for data transformations in Microsoft Fabric. The combination of dataflows and pipelines is useful when you need to perform additional operations on the transformed data. Data pipelines are a common concept in data engineering and offer a wide variety of activities to orchestrate. Some common activities include: Copy data Incorporate Dataflow Add Notebook Get metadata Execute a script or stored procedure Pipelines provide a visual way to complete activities in a specific order. You can use a dataflow for data ingestion, transformation, and landing into a Fabric data store. Then incorporate the dataflow into a pipeline to orchestrate extra activities, like execute scripts or stored procedures after the dataflow has completed. Pipelines can also be scheduled or activated by a trigger to run your dataflow. By using a pipeline to run your dataflow, you can have the data refreshed when you need it instead of having to manually run the dataflow. When you're dealing with enterprise or frequently changing data, automation allows you to focus on other responsibilities.",
        "links": null,
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-dataflow-gen-2-fabric/media/dataflow-schedule-pipeline.png",
                "image_alt": "Screenshot of the pipeline schedule window for a dataflow."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Ingest Data with Dataflows Gen2 in Microsoft Fabric",
        "topic": "Exercise - Create and use a Dataflow Gen2 in Microsoft Fabric",
        "text": "In this exercise, you'll use a Dataflow Gen2 to load transformed data into a lakehouse and add a dataflow to a pipeline. Note You need a Microsoft Fabric trial license with the Fabric preview enabled in your tenant. See Getting started with Fabric to enable your Fabric trial license. Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://go.microsoft.com/fwlink/?linkid=2259607"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Ingest Data with Dataflows Gen2 in Microsoft Fabric",
        "topic": "Summary",
        "text": "In this module, we walked through a scenario where both data engineers need to ingest, transform, and load data into a Fabric data store such as a lakehouse. We also identified data analyst needs to perform transformations closer to the data source to support Power BI report development. With Microsoft Fabric, you can create Dataflows Gen2 to perform data integration for your lakehouse, and optionally include the dataflow in a Data Pipeline as well. You learned about Dataflows Gen2 and how to use them as part of your data integration process. Power Query Online offers a visual interface to perform complex data transformations without writing any code. To learn more about data integration, see the Data Factory in Microsoft Fabric documentation.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-factory/"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Orchestrate processes and data movement with Microsoft Fabric",
        "topic": "Introduction",
        "text": "Data pipelines define a sequence of activities that orchestrate an overall process, usually by extracting data from one or more sources and loading it into a destination; often transforming it along the way. Pipelines are commonly used to automate extract , transform , and load (ETL) processes that ingest transactional data from operational data stores into an analytical data store, such as a lakehouse, data warehouse, or SQL database. If you're already familiar with Azure Data Factory, then data pipelines in Microsoft Fabric will be immediately familiar. They use the same architecture of connected activities to define a process that can include multiple kinds of data processing tasks and control flow logic. You can run pipelines interactively in the Microsoft Fabric user interface, or schedule them to run automatically.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Orchestrate processes and data movement with Microsoft Fabric",
        "topic": "Understand pipelines",
        "text": "Pipelines in Microsoft Fabric encapsulate a sequence of activities that perform data movement and processing tasks. You can use a pipeline to define data transfer and transformation activities, and orchestrate these activities through control flow activities that manage branching, looping, and other typical processing logic. The graphical pipeline canvas in the Fabric user interface enables you to build complex pipelines with minimal or no coding required. Core pipeline concepts Before building pipelines in Microsoft Fabric, you should understand a few core concepts. Activities Activities are the executable tasks in a pipeline. You can define a flow of activities by connecting them in a sequence. The outcome of a particular activity (success, failure, or completion) can be used to direct the flow to the next activity in the sequence. There are two broad categories of activity in a pipeline. Data transformation activities - activities that encapsulate data transfer operations, including simple Copy Data activities that extract data from a source and load it to a destination, and more complex Data Flow activities that encapsulate dataflows (Gen2) that apply transformations to the data as it is transferred. Other data transformation activities include Notebook activities to run a Spark notebook, Stored procedure activities to run SQL code, Delete data activities to delete existing data, and others. In OneLake, you can configure the destination to a lakehouse, warehouse, SQL database, or other options. Control flow activities - activities that you can use to implement loops, conditional branching, or manage variable and parameter values. The wide range of control flow activities enables you to implement complex pipeline logic to orchestrate data ingestion and transformation flow. Tip For details about the complete set of pipeline activities available in Microsoft Fabric, see Activity overview in the Microsoft Fabric documentation. Parameters Pipelines can be parameterized, enabling you to provide specific values to be used each time a pipeline is run. For example, you might want to use a pipeline to save ingested data in a folder, but have the flexibility to specify a folder name each time the pipeline is run. Using parameters increases the reusability of your pipelines, enabling you to create flexible data ingestion and transformation processes. Pipeline runs Each time a pipeline is executed, a data pipeline run is initiated. Runs can be initiated on-demand in the Fabric user interface or scheduled to start at a specific frequency. Use the unique run ID to review run details to confirm they completed successfully and investigate the specific settings used for each execution.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-factory/activity-overview"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-data-factory-pipelines-fabric/media/pipeline.png",
                "image_alt": "Screenshot of a pipeline in Microsoft Fabric."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Orchestrate processes and data movement with Microsoft Fabric",
        "topic": "Use the Copy Data activity",
        "text": "The Copy Data activity is one of the most common uses of a data pipeline. Many pipelines consist of a single Copy Data activity that is used to ingest data from an external source into a lakehouse file or table. You can also combine the Copy Data activity with other activities to create a repeatable data ingestion process - for example by using a Delete data activity to remove existing data, a Copy Data activity to replace the deleted data with a file containing data from an external source, and a Notebook activity to run Spark code that transforms the data in the file and loads it into a table. The Copy Data tool When you add a Copy Data activity to a pipeline, a graphical tool takes you through the steps required to configure the data source and destination for the copy operation. A wide range of source connections is supported, making it possible to ingest data from most common sources. In OneLake, this includes support for lakehouse, warehouse, SQL Database, and others. Copy Data activity settings After you've added a Copy Data activity to a pipeline, you can select it in the pipeline canvas and edit its settings in the pane underneath. When to use the Copy Data activity Use the Copy Data activity when you need to copy data directly between a supported source and destination without applying any transformations, or when you want to import the raw data and apply transformations in later pipeline activities. If you need to apply transformations to the data as it is ingested, or merge data from multiple sources, consider using a Data Flow activity to run a dataflow (Gen2). You can use the Power Query user interface to define a dataflow (Gen2) that includes multiple transformation steps, and include it in a pipeline. Tip To learn more about Dataflow (Gen2) in Microsoft Fabric to ingest data, consider completing the Ingest Data with Dataflows Gen2 in Microsoft Fabric module.",
        "links": [
            "https://learn.microsoft.com/en-us/training/modules/use-dataflow-gen-2-fabric"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-data-factory-pipelines-fabric/media/copy-data.png",
                "image_alt": "Screenshot of the Copy Data tool in Microsoft Fabric."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-data-factory-pipelines-fabric/media/copy-sql-database.png",
                "image_alt": "Screenshot of the Copy Data tool showing the SQL Database support in Microsoft Fabric."
            },
            {
                "image_name": "image3",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-data-factory-pipelines-fabric/media/copy-data-activity.png",
                "image_alt": "Screenshot of a Copy Data activity in Microsoft Fabric."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Orchestrate processes and data movement with Microsoft Fabric",
        "topic": "Use pipeline templates",
        "text": "You can define pipelines from any combination of activities you choose, enabling to create custom data ingestion and transformation processes to meet your specific needs. However, there are many common pipeline scenarios for which Microsoft Fabric includes predefined pipeline templates that you can use and customize as required. To create a pipeline based on a template, select the Templates tile in a new pipeline as shown here. Selecting this option displays a selection of pipeline templates, as shown here. You can select the most appropriate template for your needs, and then edit the pipeline in the pipeline canvas to customize it to your needs.",
        "links": null,
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-data-factory-pipelines-fabric/media/start-pipeline.png",
                "image_alt": "Screenshot of the Choose a task to start tile."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-data-factory-pipelines-fabric/media/pipeline-templates.png",
                "image_alt": "Screenshot of pipeline templates."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Orchestrate processes and data movement with Microsoft Fabric",
        "topic": "Run and monitor pipelines",
        "text": "When you have completed a pipeline, you can use the Validate option to check that the configuration is valid, and then either run it interactively or specify a schedule. View run history You can view the run history for a pipeline to see details of each run, either from the pipeline canvas or from the pipeline item listed in the page for the workspace.",
        "links": null,
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-data-factory-pipelines-fabric/media/run-pipeline.png",
                "image_alt": "Screenshot of a the Run menu for a pipeline in Microsoft Fabric."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-data-factory-pipelines-fabric/media/pipeline-runs.png",
                "image_alt": "Screenshot of a pipeline run history in Microsoft Fabric."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Orchestrate processes and data movement with Microsoft Fabric",
        "topic": "Exercise - Ingest data with a pipeline",
        "text": "Now it's your chance to implement a pipeline in Microsoft Fabric. In this exercise, you create a pipeline that copies data from an external source into a lakehouse. Then enhance the pipeline by adding activities to transform the ingested data. Note You need a Microsoft Fabric trial license with the Fabric preview enabled in your tenant. See Getting started with Fabric to enable your Fabric trial license. Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/fundamentals/fabric-trial"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://go.microsoft.com/fwlink/?linkid=2259606"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Orchestrate processes and data movement with Microsoft Fabric",
        "topic": "Summary",
        "text": "With Microsoft Fabric, you can create pipelines that encapsulate complex data ingestion and transformation processes. Pipelines provide an effective way to orchestrate data processing tasks that can be run on-demand or at scheduled intervals. Tip For more information about pipelines in Microsoft Fabric, see Data pipelines in the Microsoft Fabric documentation.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-factory/data-factory-overview"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Organize a Fabric lakehouse using medallion architecture design",
        "topic": "Introduction",
        "text": "In this data-driven era, the need to harness vast and diverse data sources is critical for business success. The Fabric lakehouse, blending data lakes and data warehouses, offers an ideal platform to manage and analyze this data. The medallion architecture has become a standard across the industry for lakehouse-based analytics. The medallion architecture brings structure and efficiency to your lakehouse environment. This module guides you through its bronze, silver, and gold layers, ensuring you know how to organize, refine, and curate data effectively. Whether you're a seasoned data engineer or an analytics newcomer, this module caters to all levels of experience with practical examples and a hands-on exercise. You should be familiar with the Fabric data lakehouse and have a basic understanding of SQL and Power BI before you begin. In this module, you'll explore and build a medallion architecture for Fabric lakehouses, query and report on data in your Fabric lakehouse, and describe best practices for security and governance of your Fabric lakehouse.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Organize a Fabric lakehouse using medallion architecture design",
        "topic": "Describe medallion architecture",
        "text": "Data lakehouses in Fabric are built on the Delta Lake format, which natively supports ACID (Atomicity, Consistency, Isolation, Durability) transactions. Within this framework, the medallion architecture is a recommended data design pattern used to organize data in a lakehouse logically. It aims to improve data quality as it moves through different layers. The architecture typically has three layers  bronze (raw), silver (validated), and gold (enriched), each representing higher data quality levels. Some people also call it a \"multi-hop\" architecture, meaning that data can move between layers as needed. This architecture ensures that data is reliable and consistent as it goes through various checks and changes. It also guarantees that the data is safely stored in a way that makes it easier and faster to analyze. The medallion architecture complements other data organization methods,  rather than replacing them. You can think of the medallion architecture as the framework for data cleaning, rather than a data architecture or model. It ensures compatibility and flexibility for businesses to adopt its benefits alongside existing data models, allowing you to customize data solutions and preserve expertise while remaining adaptable in the ever-changing data landscape. Understand the medallion architecture format Bronze layer The bronze or raw layer of the medallion architecture is the first layer of the lakehouse. It's the landing zone for all data, whether it's structured, semi-structured, or unstructured. The data is stored in its original format, and no changes are made to it. Silver layer The silver or validated layer is the second layer of the lakehouse. It's where you'll validate and refine your data. Typical activities in the silver layer include combining and merging data and enforcing data validation rules like removing nulls and deduplicating. The silver layer can be thought of as a central repository across an organization or team, where data is stored in a consistent format and can be accessed by multiple teams. In the silver layer you're cleaning your data enough so that everything is in one place and ready to be refined and modeled in the gold layer. Gold layer The gold or enriched layer is the third layer of the lakehouse. In the gold layer, data undergoes further refinement to align with specific business and analytics needs. This could involve aggregating data to a particular granularity, such as daily or hourly, or enriching it with external information. Once the data reaches the gold stage, it becomes ready for use by downstream teams, including analytics, data science, or MLOps. Customize your medallion architecture Depending on your organization's specific use case, you may have a need for more layers. For example, you might have an additional \"raw\" layer for landing data in a specific format before it's transformed into the bronze layer. Or you might have a \"platinum\" layer for data that's been further refined and enriched for a specific use case. Regardless of the names and number of layers, the medallion architecture is flexible and can be tailored to meet your organization's particular requirements. Move data across layers in Fabric Moving data across medallion layers refines, organizes, and prepares it for downstream data activities. Within Fabric's lakehouse, there's more than one way to move data between layers, ensuring that you can choose the method that works for your team. There are a few things to consider when deciding how to move and transform data across layers. How much data are you working with? How complex are the transformations you need to make? How often will you need to move data between layers? What tools are you most comfortable with? Understanding the difference between data transformation and data orchestration helps you select the right tools for the job within Fabric. Data transformation involves altering the structure or content of data to meet specific requirements. Tools for data transformation in Fabric include Dataflows (Gen2) and notebooks. Dataflows are a great option for smaller semantic models and simple transformations. Notebooks are a better option for larger semantic models and more complex transformations. Notebooks also allow you to save your transformed data as a managed Delta table in the lakehouse, ready for reporting. Data orchestration refers to the coordination and management of multiple data-related processes, ensuring they work together to achieve a desired outcome. The primary tool for data orchestration in Fabric is pipelines . A pipeline is a series of steps that move data from one place to another, in this case, from one layer of the medallion architecture to the next. Pipelines can be automated to run on a schedule or triggered by an event.",
        "links": null,
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/describe-medallion-architecture/media/lakehouse-medallion.png",
                "image_alt": "Diagram of a medallion architecture where data flows from the source to the bronze, silver, and gold layers."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Organize a Fabric lakehouse using medallion architecture design",
        "topic": "Implement a medallion architecture in Fabric",
        "text": "Now that you have a solid understanding of the medallion architecture, let's explore how to put it into action within Fabric. Set up the foundation : Create your Fabric lakehouse . You can use the same lakehouse for multiple medallion architectures, or alternatively, you can use different lakehouses and even different lakehouses in different workspaces, depending on your use case. Design your architecture : Create your architecture layout, define your layers, and determine how data will flow between them. The most straightforward implementation is to use Bronze as the raw layer, Silver as the curated layer, and gold as the presentation layer. Your gold layer should be modeled in a star schema and optimized for reporting. Question Bronze Silver Gold What happens in that layer? Ingest raw data Cleanse and validate data Additional transformations and modeling What tool is used? Pipelines, dataflows, notebooks Dataflows or notebooks SQL analytics endpoint or semantic model Ingest data into bronze : Determine how you'll ingest data into your bronze layer. You can do this using pipelines, dataflows, or notebooks. Transform data and load to silver : Determine how you'll transform data in your silver layer. You can do this using dataflows or notebooks. Transformations at the silver level should be focused on data quality and consistency, not on data modeling. Generate a gold layer : Determine how you'll generate your gold layer, what it will contain, and how it will be used. The gold layer is where you'll model your data for reporting using a dimensional model. Here you'll establish relationships, define measures, and incorporate any other elements essential for effective reporting. You can have multiple gold layers for different audiences or domains. For example, you might have a gold layer for your finance team and a separate gold layer for your sales team. You might also have a gold layer for your data scientists that is optimized for machine learning. Depending on your needs, you might also use a Data Warehouse as your gold layer. In Fabric, you can transform your data using dataflows or notebooks, and then load it into a gold Delta table in the lakehouse. You can then connect to the Delta table using a SQL analytics endpoint and use SQL to model your data for reporting. Alternatively, you can use Power BI to connect to the SQL analytics endpoint of the gold layer and model your data for reporting. Enable downstream consumption : Determine how you'll enable downstream consumption of your data. You can do this using workspace or item permissions, or by connecting to the SQL analytics endpoint.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/onelake/create-lakehouse-onelake?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Organize a Fabric lakehouse using medallion architecture design",
        "topic": "Query and report on data in your Fabric lakehouse",
        "text": "Now that your medallion architecture is in place, data teams and the business can start using it to query and report on data. Fabric has several tools and technologies that enable you to query and report on data in your lakehouse, including SQL analytics endpoints and Direct Lake mode in Power BI semantic models. Query data in your lakehouse Teams can use SQL to explore and query data in the gold layer. You can analyze data in delta tables at any layer of the medallion architecture using the T-SQL language, save functions, generate views, and apply SQL security. You can also use the SQL analytics endpoint to connect to your lakehouse from querying tools and applications. The SQL analytics endpoint in Fabric enables you to write queries,  manage the semantic model, and query data using the visual query experience. Note The SQL analytics endpoint operates in read-only mode over lakehouse delta tables. To modify data in your lakehouse you can use dataflows, notebooks, or pipelines. In addition to using the SQL analytics endpoint for data exploration, you can use a Power BI semantic model to explore data. A semantic model is a way of viewing data using business friendly terminology.  It's typically represented as a star schema with facts that represent a domain of data relevant to a particular business area, and dimensions that allow you to analyze data in the data domain. A default semantic model is created automatically when you create a lakehouse. You can also create a non-default, custom Power BI semantic model. Data analysts can connect to the semantic model using Direct Lake mode, in which the semantic model accesses data directly from Delta tables in a lakehouse. Tailor your medallion layers for different needs Tailoring medallion layers to different needs allows you to optimize data processing and access for specific use cases. By customizing these layers, you can ensure that each layer's structure and organization align with the requirements of different user groups, improving performance, ease of use, and data relevance for diverse stakeholders. Creating multiple Gold layers tailored for diverse audiences or domains highlights the flexibility of the medallion architecture. Finance, sales, data science  each can have its optimized Gold layer, serving specific analytical requirements. Some applications, third-party tools, or systems require specific data formats. You can utilize your medallion architecture to generate cleansed and properly formatted data.",
        "links": null,
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/describe-medallion-architecture/media/sql-endpoint-silver.png",
                "image_alt": "Screenshot of the SQL analytics endpoint in the Fabric user interface."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/describe-medallion-architecture/media/dataset-relationships.png",
                "image_alt": "Screenshot of the Power BI semantic model with relationships between tables."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Organize a Fabric lakehouse using medallion architecture design",
        "topic": "Considerations for managing your lakehouse",
        "text": "There are several considerations when managing your lakehouse including how to secure your lakehouse and how to handle continuous integration and continuous delivery (CI/CD). Secure your lakehouse Secure your lakehouse by ensuring that only authorized users can access data. In Fabric, you can do this by setting permissions at the workspace or item level. Workspace permissions control access to all items within a workspace. Item level permissions control access to specific items within a workspace, and could be used when you're collaborating with colleagues who aren't in the same workspace, or they only need access to a single, specific item. You can strategically store different layers of your lakehouse in separate workspaces for improved security and efficient capacity management. This approach not only enhances security but also optimizes cost-effectiveness. Security and Access Considerations: Define who needs access at each layer, ensuring only authorized personnel can interact with sensitive data. Gold Layer Access Control: Restrict access to the Gold layer for read-only purposes, emphasizing the importance of minimal permissions. Silver Layer Utilization: Decide whether users will be allowed to build upon the Silver layer, balancing flexibility and security. Bronze Layer Access Control: Restrict access to the Bronze layer for read-only purposes, emphasizing the importance of minimal permissions. Sharing of Fabric content should be discussed with your organization's security team to ensure that it aligns with your organization's security policies. Considerations for Continuous Integration and Continuous Delivery (CI/CD) Designing a Continuous Integration/Continuous Deployment (CI/CD) process for a lakehouse architecture involves several considerations to ensure a smooth and efficient deployment process.  Considerations include implementing data quality checks, version control, automated deployments, monitoring, and security measures. Considerations should also include scalability, disaster recovery, collaboration, compliance, and continuous improvement to ensure reliable and efficient data pipeline deployments. While some of these are related to processes and practices, others are related to the tools and technologies used to implement CI/CD. Fabric natively provides several tools and technologies to support CI/CD processes. Git integration in Microsoft Fabric enables you to integrate development processes, tools, and best practices straight into the Fabric platform. Fabric's Git integration enables data teams to back up and version work, revert to previous stages as needed, collaborate with others or work alone using Git branches, and leverage the capabilities of familiar source control tools to manage Fabric items. Note Learn more about Git integration in Fabric in Introduction to git integration . CI/CD is crucial at the gold layer of a lakehouse because it ensures that high-quality, validated, and reliable data is available for consumption. Automated processes enable continuous integration of new data, data transformations, and updates, reducing manual errors and providing consistent and up-to-date insights to downstream users and applications. This enhances data accuracy, accelerates decision-making, and supports data-driven initiatives effectively.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/cicd/git-integration/intro-to-git-integration"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Organize a Fabric lakehouse using medallion architecture design",
        "topic": "Exercise - Organize your Fabric lakehouse using a medallion architecture",
        "text": "Now it's your turn to create a lakehouse in Fabric and move data through the medallion architecture. You'll also learn how to query and report on data in your Fabric lakehouse. Note You need a Microsoft Fabric trial license with the Fabric preview enabled in your tenant. See Getting started with Fabric to enable your Fabric trial license. Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://go.microsoft.com/fwlink/?linkid=2259609"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a Lakehouse with Microsoft Fabric",
        "unit": "Organize a Fabric lakehouse using medallion architecture design",
        "topic": "Summary",
        "text": "You now understand how the medallion architecture offers a structured approach to logically organize lakehouse data. Once you integrate this architecture into your Fabric environment, you're on your way to ensuring an organized, refined, and curated data system. You'll eliminate data silos and ensure that downstream teams and apps can access and use the data seamlessly. With this effective setup, you're not just storing data; you're setting it up to be quickly and easily analyzed by whenever it's needed.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Get started with Real-Time Intelligence in Microsoft Fabric",
        "topic": "Introduction",
        "text": "Imagine you're a data analyst for a delivery company, responsible for monitoring package delivery performance across your network of distribution centers, delivery vehicles, and customer routes. Your operations team needs to know immediately when delivery delays occur, which routes are experiencing issues, and how customer satisfaction is trending in real time. Currently, your delivery reports are generated overnight, meaning by the time you identify a problemsuch as a vehicle breakdown or weather-related delayshundreds of packages might already be behind schedule and customers are left waiting without updates. You need continuous monitoring of delivery trucks, customer feedback systems, and warehouse scanners to track package movements and delivery performance as events happen, not hours later. In this scenario, Real-Time Intelligence in Microsoft Fabric can be used to help you work with streaming data from GPS trackers on delivery vehicles, package scanning systems, and customer notification platforms as events occur. Unlike traditional batch processing that shows historical snapshots of delivery status, Real-Time Intelligence could help you analyze package movements and delivery performance as it happens. This approach could help you spot route delays quickly, estimate delivery windows based on current conditions, and set up automated customer notifications while creating workflows for route optimization. By the end of this module, you'll understand how Microsoft Fabric Real-Time Intelligence components work together to create real-time analytics solutions, how to ingest, process, store and query real-time data, and how to visualize data in motion and automate responses to changing conditions.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Get started with Real-Time Intelligence in Microsoft Fabric",
        "topic": "What is real-time data analytics?",
        "text": "Real-time analytics is the practice of processing, analyzing, and acting on data as it's generated, typically within seconds to minutes of when events occur. Unlike traditional analytics that works with static snapshots of historical data stored in databases, real-time analytics operates on data that's actively flowing through your systems, enabling immediate insights and rapid responses to changing conditions. This approach is also known as near real-time analytics, since there's always some degree of processing and network latency involved. Understand events and streams Events are records of things that happen in a system. They capture moments when something occurs, changes, or is completed. Examples include website clicks, stock price changes, customer purchases, patient vital sign changes, or equipment sensor readings. Think of them as digital records or log entries that document activity across your systems. A stream is essentially a sequence of events, typically ordered by the time an event occurred. Each event in the stream represents something that happened at a specific moment. Events flow through streams continuously as they occur. For example, a stream of equipment temperature sensor readings contains temperature readings over many points of time. This continuous flow of event information allows you to detect patterns over time, identify opportunities or risks, and take action immediately after something happens, or in real-time . Streams are the delivery mechanism that carries events from where they happen to where they need to be processed, analyzed, or acted upon. Components of real-time analytics solutions To build real-time analytics solutions, you need several integrated capabilities working together: Real-time data ingestion: Collect data from multiple sources simultaneously, as information is generated. For example: database changes from change data capture, sensors, applications, system logs, and APIs. Stream processing: Transform and analyze data while it flows from sources to destinations. This includes filtering, aggregating, joining with other data sources, and detecting patterns with minimal latency. Low-latency storage: Use specialized databases and storage systems designed to handle high-velocity data writes and provide fast query responses. Interactive dashboards: Create visualizations that update automatically as new data arrives, show current state and trends in real-time. Automated decision making: Set up event-driven rules and triggers that can initiate actions, send alerts, or start workflows based on real-time conditions. Use real-time analytics To use real-time data effectively, information has to be ingested, processed, stored, analyzed, and presented to be actionable. Real-time analytics enables you to: Respond immediately to opportunities or problems as they emerge Optimize operations by adjusting resources and configurations based on current conditions Enhance customer experiences through personalized, contextual interactions Prevent issues by detecting anomalies before they become critical problems Real-Time Intelligence in Microsoft Fabric brings all these capabilities together in a single platform. Through components like Eventstreams for data ingestion and transformation, Eventhouses for analytics-optimized storage, the Real-Time hub for data discovery, Real-Time Dashboards for visualization, and Activator for automated alerts and actions, Real-Time Intelligence enables you to monitor critical events, trigger automated responses, track business processes, and analyze patterns in real-time, turning what happens in your systems into actionable insights.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Get started with Real-Time Intelligence in Microsoft Fabric",
        "topic": "Real-Time Intelligence in Microsoft Fabric",
        "text": "As organizations generate increasing volumes of event-driven data, the ability to process, analyze, and act on data in motion becomes essential for competitive advantage. Real-Time Intelligence provides comprehensive capabilities for working with streaming data with minimal latency. Explore Real-Time Intelligence use cases Unlike batch systems that process data on scheduled intervals, Real-Time Intelligence helps you respond to events as they happen, delivering near real-time insights. Here are some common types of event data and examples of how Real-Time Intelligence can support downstream actions and business responsiveness: Delivery tracking : Monitor vehicle locations to alert customers when packages are delayed Equipment monitoring : Track machine temperature to prevent costly breakdowns Fraud detection : Analyze purchase patterns to block suspicious transactions immediately Website performance : Monitor page load times to improve user experience System health : Track application errors to maintain service reliability Real-Time Intelligence components Microsoft Fabric's Real-Time Intelligence is an integrated set of components that work together to handle streaming data from capture through automated response. The diagram shows how Real-Time Intelligence components work together for end-to-end processing. Each component handles a specific stage of the real-time analytics process: Ingest and process data in motion with Eventstreams Data ingestion and processing can happen through Eventstreams , which capture streaming data from various sources and apply real-time transformations as data flows through the system. Eventstreams can filter, enrich, and transform your data and route it to different destinations. Store real-time data in an Eventhouse Real-Time Intelligence stores data in KQL (Kusto Query Language) databases in Eventhouses . These databases are designed for time-series data and fast ingestion of streaming data. The storage integrates with OneLake, making your data available to other Fabric tools. Analyze data with KQL Queryset KQL Queryset provides a workspace for running and managing queries against KQL databases. The KQL Queryset allows you to save queries for future use, organize multiple query tabs, and share queries with others for collaboration. The KQL Queryset also supports T-SQL queries, allowing you to use familiar SQL syntax alongside KQL for data analysis. Visualize insights with Real-Time Dashboard Real-Time Dashboards connect directly to KQL databases and refresh automatically as new data arrives. These dashboards let you explore data interactively and monitor both current conditions and historical trends. Act on data with Activator Automated actions can be configured with Activator , which continuously monitors streaming data against user-defined rules and thresholds. When conditions are met, Activator can send notifications, trigger workflows in Power Automate, execute Fabric data pipelines or notebooks, creating event-driven automation that responds to real-time conditions. Discover streaming data with the Real-Time hub The Fabric Real-Time hub is a central location where you can discover and manage all of the data-in-motion that you have access to. It gives you a way to ingest streaming data from Azure and from external sources and it lets you subscribe to Azure and Fabric events. Think of the Real-Time hub as your streaming data catalog where you can see what's happening in near real-time across your organization. There are connectors you can use to ingest data into Microsoft Fabric from various sources. For example, you might connect to IoT sensor streams through Azure Event Hubs, subscribe to Azure Blob Storage events, use Change Data Capture (CDC) to stream database changes, or monitor Fabric workspace events. Once you have configured a connection to data source or event source, these items become the foundation for event driven decision making and a wide range of real-time analytics solutions, from building dashboards and setting up alerts to triggering automated workflows and analyzing trends in your data. To access the real-time hub, select the Real-Time icon in the main Fabric menu bar. The real-time hub organizes data-in-motion into several main categories: Data sources : Browse and connect to available streaming data sources, such as Microsoft sources, database change data capture feeds, and external sources from other cloud providers Azure sources : Discover and configure Azure streaming data sources such as Azure IoT Hub, Azure Service Bus, Azure Data Explorer DB, and more Fabric events : Subscribe to system-generated events in Fabric that you can access, like job status changes, events produced by action on files or folders in OneLake, and Fabric workspace item changes Azure events : Subscribe to system events from Azure services that can be used to trigger automated responses such as actions on files or folders in Azure blob storage In the Real-Time hub, you can preview and explore your streaming data by navigating directly to eventstreams or KQL databases in eventhouses for deeper analysis and querying. You can also build automated responses using Activator rules that trigger actions like notifications, workflows, or data processing when specific patterns are detected.",
        "links": [
            "https://learn.microsoft.com/wwl/get-started-kusto-fabric/media/real-time-intelligence-core.png#lightbox",
            "https://learn.microsoft.com/wwl/get-started-kusto-fabric/media/real-time-hub.png#lightbox"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-kusto-fabric/media/real-time-intelligence-core.png",
                "image_alt": "Diagram of Fabric Real-Time Intelligence capabilities."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-kusto-fabric/media/real-time-hub.png",
                "image_alt": "Screenshot of Microsoft Fabric Real-Time hub."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Get started with Real-Time Intelligence in Microsoft Fabric",
        "topic": "Ingest and transform real-time data",
        "text": "Real-Time Intelligence in Microsoft Fabric provides two primary approaches for ingesting streaming data: using eventstreams or directly ingesting data into a KQL database in an Eventhouse. Eventstreams for data ingestion and transformation Eventstreams are a way to bring real-time events into Fabric, to transform them, and then route data to a destination. The image shows the three main components of an Eventstream: sources where data originates, transformations optional processing applied to the data, and destinations where the processed data is sent. Think of the Eventstream components like a water pipe system. The source is your faucet, transformations are filters along the way and you need a destination like a sink or bucket to collect and use the water. Next, let's review each component of an Eventstream. Data sources for eventstreams Once you create an eventstream in Fabric, you can connect it to a wide range of data sources. You can stream data from Microsoft sources and also ingest data from non-Microsoft platforms including: Microsoft sources , like Azure Event Hubs, Azure IoT Hubs, Azure Service Bus, Change Data Capture (CDC) feeds in database services, and others. Azure events , like Azure Blob Storage events. Fabric events , such as changes to items in a Fabric workspace, data changes in OneLake data stores, and events associated with Fabric jobs. External sources , such as Apache Kafka, Google Cloud Pub/Sub, and MQTT (Message Queuing Telemetry Transport) (in preview) Tip To see all supported sources, see Supported sources . Event transformations in eventstreams Raw data from a source system is rarely in the exact format you need for analysis or storage. Transformations are what make your data useful and actionable. You can transform the data as it flows in an eventstream, enabling you to filter, summarize, and reshape it before storing it. Examples of available transformations include: SQL code, filter, manage fields, aggregate, group by, expand and join. Tip For more information about supported transformations, see Process event data with event processor editor and Process events using SQL code editor . Data destinations in eventstreams Streaming data flows continuously and is temporary by nature. It requires immediate processing and storage to retain its value. The destination in an eventstream is what makes your real-time data processing actionable. It's where your processed data becomes available for queries, reports, dashboards, alerts, actions, or integration with other systems. You can load the data from your stream into the following destinations: a KQL database in an Eventhouse, Lakehouse, a derived stream, Fabric Activator, or a custom endpoint. Tip For more information about supported destinations, see Add and manage a destination in an eventstream . Direct ingestion to a KQL database in an Eventhouse Data can also be directly ingested into a KQL (Kusto Query Language) database in an Eventhouse. Some examples of data ingestion sources include: local files, Azure storage, Amazon S3, Azure Event Hubs, OneLake, and more. Data ingestion can be configured using connectors or through the Get data option in a KQL database as shown in this image. Tip For more information about supported ingestion sources for KQL databases in Eventhouses, see Data sources and Data connectors overview . Data transformation in a KQL database in Eventhouse with update policies When directly ingesting data into a KQL database, data first lands in the database, then can be transformed using update policies . This is different from eventstream transformations that occur during stream processing, before routing data to a destination. Update policies are automation mechanisms triggered when new data is written to a table. They run a query to transform ingested data and save the result to a destination table.",
        "links": [
            "https://learn.microsoft.com/wwl/get-started-kusto-fabric/media/eventstream.png#lightbox",
            "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-streams/add-manage-eventstream-sources?pivots=enhanced-capabilities&azure-portal=true#supported-sources",
            "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-streams/process-events-using-event-processor-editor?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-streams/process-events-using-sql-code-editor?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-streams/add-manage-eventstream-destinations?azure-portal=true",
            "https://learn.microsoft.com/wwl/get-started-kusto-fabric/media/get-data.png#lightbox",
            "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/get-data-overview?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/data-connectors/data-connectors?azure-portal=true"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-kusto-fabric/media/eventstream.png",
                "image_alt": "Screenshot of an Eventstream in Microsoft Fabric."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-kusto-fabric/media/get-data.png",
                "image_alt": "Screenshot of the get data option in a KQL database in an Eventhouse in Microsoft Fabric."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Get started with Real-Time Intelligence in Microsoft Fabric",
        "topic": "Store and query real-time data",
        "text": "KQL databases in an Eventhouse are where you store and query real-time data that flows from Eventstreams and other streaming sources. Once data is loaded into tables, you can use the Kusto Query Language (KQL) or T-SQL to query your data. Within an eventhouse, you can create: KQL databases : Real-time optimized data stores that host a collection of tables, stored functions, materialized views, shortcuts and data streams. KQL querysets : Collections of KQL queries that you can use to work with data in KQL database tables. A KQL queryset supports queries written using Kusto Query Language (KQL) or a subset of the Transact-SQL language. Understand the power of Kusto Query Language (KQL) To query data in a table in a KQL database, you can use the KQL . KQL is specifically designed for analyzing large volumes of structured, semi-structured, and unstructured data with exceptional performance. KQL databases are optimized for time-series data and index incoming data by ingestion time and partition it for optimal query performance. KQL is the same language used in Azure Data Explorer, Azure Monitor Log Analytics, Microsoft Sentinel, and in Microsoft Fabric. Get familiar with KQL syntax KQL queries are made of one or more query statements. A query statement consists of a table name followed by operators that take , filter , transform , aggregate , or join data. For example, to print any 10 rows in the stock table, execute: stock\n| take 10 A more complex example might aggregate data to find average stock prices over the last 5 minutes: stock\n| where [\"time\"] > ago(5m)\n| summarize avgPrice = avg(todouble(bidPrice)) by symbol\n| project symbol, avgPrice Tip To learn more about KQL, see Kusto Query Language (KQL) overview . Automate data processing with management commands Beyond basic querying, you can automate data processing through management commands including: Update policies : Automatically transform incoming data and save it to different tables as it arrives. Materialized views : Precalculate and store summary results for faster queries. Stored functions : Save frequently used query logic that you can reuse across multiple queries. Tip For more information about working with KQL databases, including detailed examples of update policies, materialized views, and stored functions, see Work with real-time data in a Microsoft Fabric Eventhouse . Other query options Using SQL KQL databases in Eventhouses also support a subset of common T-SQL expressions for data professionals already familiar with T-SQL syntax. For example: SELECT TOP 10 * FROM stock; Use Copilot to help with queries Microsoft Fabric includes Copilot for Real-Time Intelligence, which can help you write queries to extract insights from your Eventhouse data. Copilot uses AI to understand what you're looking for and can generate the required query code. Tip To learn more about Copilot for Real-Time Intelligence, see Copilot for Real-Time Intelligence .",
        "links": [
            "https://learn.microsoft.com/wwl/get-started-kusto-fabric/media/eventhouse.png#lightbox",
            "https://learn.microsoft.com/en-us/kusto/query/?azure-portal=true",
            "https://learn.microsoft.com/en-us/training/modules/query-data-kql-database-microsoft-fabric/?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/copilot-writing-queries?azure-portal=true"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-kusto-fabric/media/eventhouse.png",
                "image_alt": "Screenshot of an Eventhouse in Microsoft Fabric."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Get started with Real-Time Intelligence in Microsoft Fabric",
        "topic": "Visualize real-time data",
        "text": "Real-Time Dashboards provide a way to pin data visualizations to a single visual interface, enabling you to surface real-time insights at a glance. Each tile in a dashboard shows you different information based on a KQL query that extracts real-time data from tables in an eventhouse. Create a Real-Time Dashboard You can create a Real-Time Dashboard in a workspace and then configure its source, or you can create one directly from a KQL queryset in an eventhouse. Dashboards are composed of one or more tiles , each containing a visualization based on a KQL query expression. By default, the visualization shows the results of the query as a table; but you can edit the tile to customize how the data is displayed. When published, tiles let you explore the data they contain interactively by drilling into the data and using a visual interface to filter and aggregate the data, and change the visualization type. Tip To learn more about Real-Time Dashboards, see Create a Real-Time Dashboard . Visualize real-time data with Power BI You also can create Power BI reports from your KQL database data. Tip To learn more about using Power BI with real-time data in Fabric, see Visualize data in a Power BI report .",
        "links": [
            "https://learn.microsoft.com/wwl/get-started-kusto-fabric/media/dashboard.png#lightbox",
            "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/dashboard-real-time-create?azure-portal=true",
            "https://learn.microsoft.com/wwl/get-started-kusto-fabric/media/power-bi.png#lightbox",
            "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/create-powerbi-report?azure-portal=true"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-kusto-fabric/media/dashboard.png",
                "image_alt": "Screenshot of a real-time dashboard in Microsoft Fabric."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-kusto-fabric/media/power-bi.png",
                "image_alt": "Screenshot of a Power BI report editor in Microsoft Fabric."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Get started with Real-Time Intelligence in Microsoft Fabric",
        "topic": "Automate actions",
        "text": "Activator is a technology in Microsoft Fabric that enables automated processing of events that trigger actions. For example, you can use Activator to notify you by email when a value in an Eventstream deviates from a specific range or to run a notebook to perform some Spark-based data processing logic when a Real-Time Dashboard is updated. This image shows a rule configured to alert when package delivery failures happen, demonstrating how you can automate responses to specific business events. Understand Activator key concepts Activator operates based on four core concepts: Events , Objects , Properties , and Rules . Events - Each record in a stream of data represents an event that has occurred at a specific point in time. Objects - The data in an event record can be used to represent an object , such as a sales order, a sensor, or some other business entity. Properties - The fields in the event data can be mapped to properties of the business object, representing some aspect of its state. For example, a total_amount field might represent a sales order total, or a temperature field might represent the temperature measured by an environmental sensor. Rules - The key to using Activator to automate actions based on events is to define rules that set conditions under which an action is triggered based on the property values of objects referenced in events. For example, you might define a rule that sends an email to a maintenance manager if the temperature measured by a sensor exceeds a specific threshold. Use cases for Activator Activator can help you in various scenarios, such as dynamic inventory management, real-time customer engagement, and effective resource allocation in cloud environments. It's a powerful tool for any circumstance that requires real-time data analysis and automated actions. Use Activator to: Initiate marketing actions when product sales drop. Send notifications when temperature changes could affect perishable goods. Flag real-time issues affecting the user experience on apps and websites. Trigger alerts when a shipment hasn't been updated within an expected time frame. Send alerts when a customer's account balance crosses a certain threshold. Respond to anomalies or failures in data processing workflows immediately. Run ads when same-store sales decline. Alert store managers to move food from failing grocery store freezers before it spoils. Tip For more information about Activator, see What is Activator? .",
        "links": [
            "https://learn.microsoft.com/wwl/get-started-kusto-fabric/media/activator.png#lightbox",
            "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/data-activator/activator-introduction?azure-portal=true"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-kusto-fabric/media/activator.png",
                "image_alt": "Screenshot of an Activator alert in Microsoft Fabric."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Get started with Real-Time Intelligence in Microsoft Fabric",
        "topic": "Exercise - Explore Real-Time Intelligence in Fabric",
        "text": "Now it's time to try out Fabric Real-Time Intelligence yourself. In this exercise, you'll ingest real-time data into Microsoft Fabric. You'll then query and visualize the data before defining an alert to automate an action based on a threshold value in the real-time data stream. Note You need a Microsoft Fabric tenant to complete this exercise. For more information about accessing Microsoft Fabric, see Getting started with Fabric . Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://go.microsoft.com/fwlink/?linkid=2260722&azure-portal=true"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Get started with Real-Time Intelligence in Microsoft Fabric",
        "topic": "Summary",
        "text": "In this module, you learned how Microsoft Fabric RealTime Intelligence enables you to ingest, transform, store, visualize, and act on data in motion. You explored the RealTime Hub for discovering streams, Eventstreams for ingesting and processing event data, and KQL databases in Eventhouses for fast, timebased analytics using KQL. You also learned how to visualize streaming insights with RealTime dashboards and Power BI, and how to automate responses using Activator. Tip For more information about Real-Time Intelligence in Microsoft Fabric, see Real-Time Intelligence documentation in Microsoft Fabric .",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Use Eventstream in Microsoft Fabric",
        "topic": "Introduction",
        "text": "Eventstream in Real-Time Intelligence enables you to capture, transform, and route real-time events without needing to write any code. You can set up event sources, destinations, and transformations in the eventstream, enabling you to collect, then optionally process data before routing it into a destination. Some of the main benefits of Eventstream in Fabric are: You can capture, transform, and route data to various destinations. You can use the drag and drop feature to create your event data processing logic without writing any code. There are multiple source connectors to ingest event data from different sources, such as Azure Event Hubs, Azure IoT Hub, Azure storage, Apache Kafka, and others. You can send data to a wide range of destinations for storage, further analysis or processing. In this module, you'll learn how to create eventstreams and use them to ingest and transform data in motion, and route it to destinations for further processing and analysis.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Use Eventstream in Microsoft Fabric",
        "topic": "Components of Eventstream",
        "text": "The Eventstream feature in Fabric works by creating a pipeline that ingests events from streaming data sources, processes them through optional transformations, and delivers them to various destinations. Eventstream is the delivery mechanism that carries events from where they happen to where they need to be processed, analyzed, or acted upon. You can use the eventstream canvas, which is a visual editor, to design your pipeline by dragging and dropping different nodes, such as sources, transformations, and destinations. You can also see the event data flowing through the pipeline in real-time. You don't need to write any code or manage any infrastructure to use Eventstream. This image shows the eventstream canvas. There's a real-time data source called Bicycles , which includes: city bike rental data including bike locations, bike station street names and more. Bicycle-data is an eventstream that ingests data from the Bicycles data source. The data is transformed by an operation named GroupByStreet that sums the number of bikes by bike station street name. This data is stored in a table in an Eventhouse called Bikes-by-street-table . The main components of an eventstream are: Sources : Sources are where your event data comes from. You can stream data from Microsoft sources and also ingest data from non-Microsoft platforms. Transformations : You can transform the data as it flows in an eventstream, enabling you to filter, summarize, and reshape it before storing it. Examples of available transformations include: SQL code, filter, manage fields, aggregate, group by, expand and join. Destinations : Destinations are where your transformed event data goes for storage, further processing, alerts, or integration with other systems. You can route the data from your stream to various destinations such as tables in an Eventhouse or lakehouse, custom endpoints, derived streams for more processing, or Fabric Activator to trigger actions.",
        "links": [
            "https://learn.microsoft.com/wwl/explore-event-streams-microsoft-fabric/media/real-time-intelligence-eventstream-workflow.png#lightbox"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/explore-event-streams-microsoft-fabric/media/real-time-intelligence-eventstream-workflow.png",
                "image_alt": "Screenshot of an eventstream."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Use Eventstream in Microsoft Fabric",
        "topic": "Eventstream sources and destinations",
        "text": "Once you create an eventstream in Fabric, you can connect it to a wide range of data sources, optionally transform it, and route the transformed, or processed data to multiple destinations. In this unit, we'll review eventstream sources and destinations. Eventstream sources You can stream data from Microsoft sources and also ingest data from non-Microsoft platforms including: Microsoft sources , like Azure Event Hubs, Azure IoT Hubs, Azure Service Bus, Change Data Capture (CDC) feeds in database services, and others. Azure events , like Azure Blob Storage events. Fabric events , such as changes to items in a Fabric workspace, data changes in OneLake data stores, and events associated with Fabric jobs. External sources , such as Apache Kafka, Google Cloud Pub/Sub, and MQTT (Message Queuing Telemetry Transport) Configure eventstream sources After you create an eventstream, you can add data sources using the eventstream canvas. You can either create a new source or connect to an existing source from the Real-Time Hub: Tip To see all supported sources, see Add and manage eventstream sources . Eventstream destinations Streaming data requires immediate processing and storage to retain its value. Destinations in an eventstream serve as endpoints where your processed data becomes available for queries, reports, dashboards, alerts, actions, or integration with other systems. You can load the data from your stream into the following destinations: Eventhouse : This destination lets you ingest your real-time event data into an Eventhouse, where you can use Kusto Query Language (KQL) to query and analyze the data. Lakehouse : This destination gives you the ability to transform your real-time events before ingesting them into your lakehouse. Real-time events are converted into Delta Lake format and then stored in designated lakehouse tables. Derived stream : You can think of derived streams as transformed versions of your original data stream that enable content-based routing . Derived streams let you route subsets of data from your default or original stream to different destinations based on the content of data. For example, you could filter IoT sensor data to send high-temperature alerts to Fabric Activator while routing hourly averages to a KQL database. Fabric Activator : Directly connect your real-time event data to an event detection engine that automatically triggers actions when specific patterns or conditions are detected in your streaming data. When data reaches certain thresholds or matches patterns, Activator can send notifications, launch Power Automate workflows, or trigger other automated responses. Custom endpoint : With this destination, you can route your real-time events to a custom endpoint. This destination is useful when you want to direct real-time data to an external system or custom application outside Microsoft Fabric. You can attach to multiple destinations within an event stream at the same time without impacting or colliding with each other. Tip For more information about supported destinations, see Add and manage a destination in an eventstream . Configure eventstream destinations Eventstream destinations can be configured in the eventstream canvas. A destination can be specified after a datasource is connected or after optional transformations are applied. The eventstream canvas in the image shows: Add destination dropdown : for configuring new destinations Three configured destinations : a derived stream, a Fabric Activator, and an Eventhouse Content-based routing where the output of the GroupByStreet transformation is routed to a derived stream that's then routed to both an Activator to check if there are bikes at every station and to an Eventhouse to insert bike counts by street into a KQL database",
        "links": [
            "https://learn.microsoft.com/wwl/explore-event-streams-microsoft-fabric/media/configure-sources.png#lightbox",
            "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-streams/add-manage-eventstream-sources?pivots=enhanced-capabilities&azure-portal=true#supported-sources",
            "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-streams/add-manage-eventstream-destinations?azure-portal=true",
            "https://learn.microsoft.com/wwl/explore-event-streams-microsoft-fabric/media/eventstream-destinations.png#lightbox"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/explore-event-streams-microsoft-fabric/media/configure-sources.png",
                "image_alt": "Screenshot showing how to configure sources in eventstream canvas."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/explore-event-streams-microsoft-fabric/media/eventstream-destinations.png",
                "image_alt": "Screenshot showing how to configure destinations in eventstream canvas."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Use Eventstream in Microsoft Fabric",
        "topic": "Eventstream transformations",
        "text": "Raw streaming data rarely arrives in the exact format needed for analysis or action. Transformations allow you to clean, enrich, and reshape data before routing it to destinations, ensuring each endpoint receives data optimized for its specific purpose. Common transformation scenarios include: Data quality : Filter out invalid or incomplete data before processing Content-based routing : Route different data subsets to appropriate destinations based on the actual data values or content Data enrichment : Add calculated fields, rename columns for clarity, or convert data types for downstream compatibility Aggregation and summarization : Calculate running totals, averages, or counts over time windows for dashboard displays Format standardization : Ensure consistent data structure across multiple data sources before combining streams Transform event data The eventstream canvas gives you a way to create event data processing workflows. Eventstream provide several no-code transformations that you can drag onto the canvas: Filter : Filter events based on the value of a field in the input. Keep only events that meet specific conditions. For example: temperature > 80, status = \"error\", customer type = \"premium\". Manage fields : This transformation allows you to add, remove, change data type, or rename fields coming in from an input or another transformation. Add calculated fields, remove unnecessary columns, rename fields, or change data types to match destination requirements. Aggregate : Use the aggregate transformation to calculate an aggregation (Sum, Minimum, Maximum, or Average) every time a new event occurs over a period of time. This operation also lets you rename calculated columns, and filter the aggregation based on other dimensions in your data. You can have one or more aggregations in the same transformation. Group by : Calculate aggregations across events within time windows, for example, hourly sales totals, or daily temperature averages. This transformation supports various time windows including tumbling windows (fixed intervals) and sliding windows (overlapping intervals). Union : Use the union transformation to connect two or more nodes in the event canvas and add events with shared fields (with the same name and data type) into one table. Fields that don't match are dropped and not included in the output. Join : Combine data from two streams based on a matching condition between them. Expand : Use this array transformation to create a new row for each value within an array. Create transformation workflows Transformations can be used together to create data processing pipelines. For example, if you had a stream of equipment temperature readings, you could start by using filter to remove sensor errors from incoming IoT data. Next, you might use manage fields to add a calculated \"priority\" column based on temperature thresholds. Then group by could calculate hourly averages by location. Finally, you'd route the processed data to appropriate destinations: temperature data to Fabric Activator for rule evaluation and hourly summaries to a Lakehouse for historical analysis.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Use Eventstream in Microsoft Fabric",
        "topic": "Exercise - Ingest real-time data with Eventstream in Microsoft Fabric",
        "text": "In this exercise, you'll explore how to use an eventstream to capture a source of streaming data, transform it, and load the transformed event data into a table for further analysis. This lab takes approximately 30 minutes to complete. Important You'll need a Microsoft Fabric license to complete this exercise. See Getting started with Fabric for details of how to enable a free Fabric trial license. Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://go.microsoft.com/fwlink/?linkid=2260610&azure-portal=true"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Use Eventstream in Microsoft Fabric",
        "topic": "Summary",
        "text": "In this module, you learned how Microsoft Fabric Eventstream enables real-time data processing through a no-code interface. You explored using eventstream to capture data from various sources, transform it as needed, and route it to various destinations. Tip For more information about Real-Time Intelligence in Microsoft Fabric, see Microsoft Fabric Eventstream - overview .",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-streams/overview?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Work with real-time data in an Eventhouse in Microsoft Fabric",
        "topic": "Introduction",
        "text": "An Eventhouse in Microsoft Fabric provides a data store for large volumes of data. It's a container that houses one or more KQL databases, each optimized for storing and analyzing real-time data that arrives continuously from various sources. You can load data into a KQL database in an Eventhouse using an Eventstream or you can directly ingest data into a KQL database. Once you have ingested data, you can then: Query the data using Kusto Query Language (KQL) or T-SQL in a KQL queryset. Use Real-Time Dashboards to visualize the data. Use Fabric Activator to automate actions based on the data. Understanding how KQL databases work helps you write effective queries to analyze real-time data. In this module, you'll learn about the characteristics that make KQL databases ideal for real-time data, then apply this knowledge by exploring KQL querying techniques and database objects like materialized views and stored functions. How do KQL databases work with real-time data? KQL databases automatically partition data by ingestion time, making recent data quickly accessible while storing historical data for trend analysis. Partitioning means the database organizes data into separate storage locations based on when it arrived, so when you query for recent data, the database knows exactly where to search rather than scanning all the data. Think of it like a digital conveyor belt - events flow in continuously, get organized automatically by when they arrive, and are immediately available for analysis while the stream keeps flowing. This automatic time-based organization works because real-time data has a unique characteristic: it represents immutable events that happened at specific moments in time. Immutable means these events can't be changed once they've occurred - a temperature reading at 3:15 PM will always be that reading because it represents what actually happened at that moment. Since each event is permanently tied to when it happened, this creates what we call time-series data - data where the timestamp is often as important as the event itself. Time-series data follows an append-only pattern where new events are continuously added, and data is rarely updated or deleted because events represent what actually happened at specific moments. This is fundamentally different from traditional relational databases, where you typically update existing records and maintain relationships between different data tables.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Work with real-time data in an Eventhouse in Microsoft Fabric",
        "topic": "Get started with an Eventhouse",
        "text": "When you create an Eventhouse, a default KQL database is automatically created with the same name. An Eventhouse contains one or more KQL databases, where you can create tables, stored procedures, materialized views, functions, data streams, and shortcuts to manage your data. You can use the default KQL database or create other KQL databases as needed. Work with data in your Eventhouse There are several ways to access and work with data in a KQL database within an Eventhouse: Data ingestion You can ingest data directly into your KQL database from various sources: Local files, Azure storage, Amazon S3 Azure Event Hubs, Fabric Eventstream, Real-Time hub OneLake, Data Factory copy, Dataflows Connectors to sources such as Apache Kafka, Confluent Cloud Kafka, Apache Flink, MQTT (Message Queuing Telemetry Transport), Amazon Kinesis, Google Cloud Pub/Sub Database shortcuts You can create database shortcuts to existing KQL databases in other eventhouses or Azure Data Explorer databases. These shortcuts let you query data from external KQL databases as if the data were stored locally in your eventhouse, without actually copying the data. OneLake availability You can enable OneLake availability for individual KQL databases or tables, making your data accessible throughout the Fabric ecosystem for cross-workload integration with Power BI, Warehouse, Lakehouse, and other Fabric services. Query data in a KQL database To query data in a KQL database, you can use KQL or T-SQL in KQL querysets . When you create a KQL database, an attached KQL queryset is automatically created for running and saving queries. Basic KQL syntax KQL uses a pipeline approach where data flows from one operation to the next using the pipe ( | ) character. Think of it like a funnel - you start with an entire data table, and each operator filters, rearranges, or summarizes the data before passing it to the next step. The order of operators matters because each step works on the results from the previous step. Important KQL is case-sensitive for everything including table names, column names, function names, operators, keywords, and string values. All identifiers must match exactly. For example, TaxiTrips is different from taxitrips or TAXITRIPS . Here's an example that shows the funnel concept: TaxiTrips\n| where fare_amount > 20\n| project trip_id, pickup_datetime, fare_amount\n| take 10 This query starts with all data in the TaxiTrips table, filters it to show only trips with fares over $20, selects specific columns using the project operator, and uses the take operator to return the first 10 rows that match the criteria in the where clause. The simplest KQL query consists of a table name: TaxiTrips This returns all columns from the TaxiTrips table, but the number of rows displayed is limited by your query tool's default settings. To retrieve a sample of data from potentially large tables, use the take operator: TaxiTrips\n| take 100 This returns the first 100 rows from the TaxiTrips table, which is useful for exploring data structure without processing the entire table. You can also aggregate data: TaxiTrips\n| summarize trip_count = count() by taxi_id This returns a summary table showing the total number of trips ( trip_count ) for each unique taxi_id , effectively counting how many trips each taxi has made. Analyze data with KQL queryset KQL queryset provides a workspace for running and managing queries against KQL databases. The KQL queryset allows you to save queries for future use, organize multiple query tabs, and share queries with others for collaboration. The KQL queryset also supports T-SQL queries, allowing you to use T-SQL syntax alongside KQL for data analysis. You can also create data visualizations while exploring your data, rendering query results as charts, tables, and other visual formats. Use Copilot to assist with queries For AI-based assistance with KQL querying, you can use Copilot for Real-Time Intelligence When your administrator enables Copilot, you see the option in the queryset menu bar. Copilot opens as a pane to the side of the main query interface. When you ask a question about your data, Copilot generates the KQL code to answer your question.",
        "links": [
            "https://learn.microsoft.com/wwl/query-data-kql-database-microsoft-fabric/media/eventhouse.png#lightbox",
            "https://learn.microsoft.com/wwl/query-data-kql-database-microsoft-fabric/media/get-data.png#lightbox",
            "https://learn.microsoft.com/wwl/query-data-kql-database-microsoft-fabric/media/queryset-visual.png#lightbox",
            "https://learn.microsoft.com/wwl/query-data-kql-database-microsoft-fabric/media/kql-copilot.png#lightbox"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/query-data-kql-database-microsoft-fabric/media/eventhouse.png",
                "image_alt": "Screenshot of an Eventhouse in Microsoft Fabric."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/query-data-kql-database-microsoft-fabric/media/get-data.png",
                "image_alt": "Screenshot of the Get Data menu for an eventhouse in Microsoft Fabric."
            },
            {
                "image_name": "image3",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/query-data-kql-database-microsoft-fabric/media/queryset-visual.png",
                "image_alt": "Screenshot of a visualization in a queryset."
            },
            {
                "image_name": "image4",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/query-data-kql-database-microsoft-fabric/media/kql-copilot.png",
                "image_alt": "Screenshot of Copilot for Real-Time Intelligence."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Work with real-time data in an Eventhouse in Microsoft Fabric",
        "topic": "Use KQL effectively",
        "text": "Understanding how to write efficient KQL queries is essential for getting good performance when working with Eventhouses. This unit covers key optimization techniques and explains why they matter for your queries. Why query optimization matters Query performance in KQL databases depends on the amount of data processed. When you understand how KQL processes data, you can write queries that: Run faster by reducing the data scanned - For example, instead of scanning millions of rows, filter early to process only thousands Use fewer resources - For example, selecting only 3 columns instead of all 50 columns reduces processing overhead Work reliably with growing data - For example, a query that works on 1 million rows today will still perform well when your data grows to 10 million rows The key principle is: the less data your query needs to process, the faster it runs. Understand key optimization techniques Filter data early and effectively Filtering reduces the amount of data that subsequent operations need to process, and KQL databases use indexes and data organization techniques that make early filtering especially efficient. Time-based filtering is effective because Eventhouses typically contain time-series data: TaxiTrips\n| where pickup_datetime > ago(30min)  // Filter first - uses time index\n| project trip_id, vendor_id, pickup_datetime, fare_amount\n| summarize avg_fare = avg(fare_amount) by vendor_id Order your filters by how much data they eliminate - put filters that eliminate the most data first. Think of it like a funnel: start with the filter that removes the most rows, then apply more specific filters to the remaining data: TaxiTrips\n| where pickup_datetime > ago(1d)    // Time filter first - eliminates most data\n| where vendor_id == \"VTS\"           // Specific vendor - eliminates some data  \n| where fare_amount > 0              // Value filter - eliminates least data\n| summarize trip_count = count() Reduce columns early Projecting or selecting only the columns you need reduces resource usage. This is especially important when working with wide tables that have many columns. TaxiTrips\n| project trip_id, pickup_datetime, fare_amount  // Select columns early\n| where pickup_datetime > ago(1d)                // Then filter\n| summarize avg_fare = avg(fare_amount) Optimize aggregations and joins Aggregations and joins are resource-intensive operations because they need to process and combine large amounts of data. How you structure them can significantly affect query performance. For aggregations, limit results when exploring data : TaxiTrips\n| where pickup_datetime > ago(1d)\n| summarize trip_count = count() by trip_id, vendor_id\n| limit 1000  // Limit results for exploration For joins, put the smaller table first . When joining tables, KQL processes the first table to match with the second table. Starting with a smaller table means fewer rows to process, making the join more efficient. // Good: Small vendor table first\nVendorInfo        \n| join kind=inner TaxiTrips on vendor_id\n\n// Avoid: Large taxi table first\nTaxiTrips         \n| join kind=inner VendorInfo on vendor_id",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Work with real-time data in an Eventhouse in Microsoft Fabric",
        "topic": "Materialized views and stored functions",
        "text": "Now that you understand basic KQL querying and optimization techniques, let's explore materialized views and stored functions in eventhouses. Understand materialized views Materialized views are precomputed aggregations that solve a common performance challenge in KQL databases. KQL databases in eventhouses often contain millions or billions of rows from streaming data sources like IoT sensors, application logs, and other events. Running aggregation queries across these large datasets can take significant time and computing resources. Materialized views store precomputed aggregation results and automatically update them as new data arrives. Instead of recalculating metrics from all historical data every time you query, the materialized view maintains the results and only processes the new data to update the aggregations. This provides instant results for dashboards and reports, even when working with massive datasets. How automatic updates work A materialized view consists of two parts that work together to provide always-current results: A materialized part : Precomputed aggregation results from data that has already been processed A delta : New data that has arrived since the last background update When you query a materialized view, the system automatically combines both parts at query time to give you fresh, up-to-date results. This means materialized views always return current data, regardless of when the background materialization process last ran. Meanwhile, a background process periodically moves data from the delta part into the materialized part, keeping the precomputed results current. This approach provides the speed of precomputed results with the freshness of real-time data. Create materialized views A materialized view encapsulates a KQL summarize statement that automatically updates as new data arrives. Here's an example that tracks trip metrics by vendor and day: .create materialized-view TripsByVendor on table TaxiTrips\n{\n    TaxiTrips\n    | summarize trips = count(), avg_fare = avg(fare_amount), total_revenue = sum(fare_amount)\n    by vendor_id, pickup_date = format_datetime(pickup_datetime, \"yyyy-MM-dd\")\n} Query materialized views Once created, materialized views can be queried like regular tables: TripsByVendor\n| where pickup_date >= ago(7d)\n| project pickup_date, vendor_id, trips, avg_fare, total_revenue\n| sort by pickup_date desc, total_revenue desc Understand stored functions KQL includes the ability to encapsulate a query as a function, making it easier to repeat common queries. You can also specify parameters for a function, so you can repeat the same query with variable values. Stored functions are useful in eventhouses where you have streaming data and multiple people writing queries. Instead of writing the same filtering or transformation logic repeatedly, you can define it once as a function and reuse it across different queries. Functions also help ensure that calculations are performed consistently when different team members need to apply the same logic to the data. Create a function .create-or-alter function trips_by_min_passenger_count(num_passengers:long)\n{\n    TaxiTrips\n    | where passenger_count >= num_passengers \n    | project trip_id, pickup_datetime\n} To call the function, use it like a table. In this example, the trips_by_min_passenger_count function is used to find 10 trips with at least three passengers: trips_by_min_passenger_count(3)\n| take 10",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Work with real-time data in an Eventhouse in Microsoft Fabric",
        "topic": "Exercise - Work with data in an Eventhouse",
        "text": "Now it's your chance to work with real-time data. In this exercise, you use KQL and T-SQL to query bike-sharing data in a KQL database. This lab takes approximately 30 minutes to complete. Important You need a Microsoft Fabric license to complete this exercise. See Getting started with Fabric for details of how to enable a free Fabric trial license. Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://go.microsoft.com/fwlink/?linkid=2335175&azure-portal=true"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Work with real-time data in an Eventhouse in Microsoft Fabric",
        "topic": "Summary",
        "text": "In this module, you explored how to work with real-time data in eventhouses using KQL. You learned how to write effective KQL queries and use advanced features like materialized views and stored functions. To learn more about KQL, explore these resources: Kusto Query Language (KQL) overview Kusto Query Language best practices Copilot for Real-Time Intelligence",
        "links": [
            "https://learn.microsoft.com/en-us/kusto/query/?azure-portal=true",
            "https://learn.microsoft.com/en-us/kusto/query/best-practices?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/get-started/copilot-real-time-intelligence?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Create Real-Time Dashboards with Microsoft Fabric",
        "topic": "Introduction",
        "text": "Imagine you're an analyst at a bike sharing company that operates across neighborhoods in a city. As bikes are rented and returned throughout the day, your company needs to monitor current bike availability and dock capacity to make operational decisions like redistributing bikes to high-demand areas. You can use Real-Time Dashboards in Microsoft Fabric to monitor this changing data. Real-Time Dashboards connect to KQL databases in eventhouses and display visualizations that refresh automatically to show current data. Unlike traditional reports that show historical snapshots, Real-Time Dashboards can be configured to refresh frequently, enabling you to monitor changing conditions and respond to current information. In this module, you'll learn how to create Real-Time Dashboards in Microsoft Fabric and connect them to KQL databases that contain your data.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Create Real-Time Dashboards with Microsoft Fabric",
        "topic": "Get started with real-time dashboards",
        "text": "Real-Time Dashboards in Microsoft Fabric are built on real-time streaming data sources. Each dashboard consists of one or more tiles , each displaying a real-time data visualization. Create a Real-Time Dashboard To create a Real-Time Dashboard, you'll need a source of real-time data; such as a KQL database in an eventhouse. You can then create a Real-Time Dashboard with a data source that references the KQL database. Configure authorization for data sources When connecting the dashboard to the data source, you can specify who has permission to access the data: Pass-through identity - Each person viewing the dashboard accesses data using their own permissions. Dashboard editor's identity - Everyone viewing the dashboard accesses data using the dashboard creator's permissions. Create tiles A dashboard contains at least one tile, in which the results of a KQL query are displayed. Specify a query When you first add a tile, you can enter and test the query you want to use to query the underlying data source. For example, you might use a KQL query similar to the following example to query a table named bikes and retrieve details of available bikes and empty bike parking docks for a bike rental system in a city: bikes\n| where ingestion_time() between (ago(30min) .. now())\n| summarize latest_observation = arg_max(ingestion_time(), *) by Neighbourhood\n| project Neighbourhood, latest_observation, No_Bikes, No_Empty_Docks\n| order by Neighbourhood asc The query in the example retrieves the most recent observation (the latest record ingested into the source table) within the last 30 minutes for each neighborhood. Initially, the tile displays the results of the query as a table as shown in the following image. Visualize the data After creating a tile, you can edit it to define a visual in which the data is represented as a chart, map, or other data visualization. For example, it might make more sense to display the rental bike data as a bar chart that shows the number of bikes and empty parking docks in each neighborhood. You can add multiple tiles to a dashboard and arrange them to organize the way the data is visualized. You can also add text tiles to provide additional information.",
        "links": [
            "https://learn.microsoft.com/wwl/create-real-time-dashboards-microsoft-fabric/media/dashboard.png#lightbox",
            "https://learn.microsoft.com/wwl/create-real-time-dashboards-microsoft-fabric/media/tile-table.png#lightbox",
            "https://learn.microsoft.com/wwl/create-real-time-dashboards-microsoft-fabric/media/tile-bar-chart.png#lightbox"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/create-real-time-dashboards-microsoft-fabric/media/dashboard.png",
                "image_alt": "Screenshot of a Real-Time Dashboard."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/create-real-time-dashboards-microsoft-fabric/media/tile-table.png",
                "image_alt": "Screenshot of a dashboard with a tile containing a table."
            },
            {
                "image_name": "image3",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/create-real-time-dashboards-microsoft-fabric/media/tile-bar-chart.png",
                "image_alt": "Screenshot of a tile being edited to include a bar chart."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Create Real-Time Dashboards with Microsoft Fabric",
        "topic": "Organize and filter dashboard data",
        "text": "Real-Time Dashboards include several features that help you organize and interact with your data more effectively. You can organize your dashboard using multiple pages, add interactive parameters to filter data, and set up automatic data refresh. Pages By default, a Real-Time Dashboard has one page. However, you can add more pages as containers for more tiles. This helps you organize related content into logical groups. For example, you might create separate pages for different data sources, or subject areas. Parameters Parameters add flexibility to your dashboard by allowing users to filter the data displayed in tiles. For example, users might want to view data for only a specific time period or focus on a particular subset of the data. You can create multiple parameters for a dashboard. Parameter values can be based on specific text or on the results of a query. Each parameter has a variable name that you can reference in your tile queries using an underscore prefix. For example, in a bike rental dashboard, if you created a parameter that lets users filter by neighborhood and named it _selected_neighbourhoods , you would reference it in your tile queries like this: bikes\n| where ingestion_time() between (ago(30min) .. now())\n    and (isempty(_selected_neighbourhoods) or Neighbourhood in (_selected_neighbourhoods))\n| summarize latest_observation = arg_max(ingestion_time(), *) by Neighbourhood This query includes a filter that checks whether the _selected_neighbourhoods parameter is empty (which happens when no neighborhoods are selected, showing all neighborhoods) or contains specific values (showing only those neighborhoods). Auto refresh Auto refresh automatically updates your dashboard data without needing to manually reload the page. Dashboard editors can set a default refresh rate and viewers can adjust this rate during their session. However, editors can also set a minimum refresh rate to manage system performance and prevent users from refreshing too frequently.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Create Real-Time Dashboards with Microsoft Fabric",
        "topic": "Dashboard management and optimization",
        "text": "When multiple tiles in a dashboard use related data, you can improve maintainability by defining base queries that retrieve a general set of records relevant for multiple tiles. Tile-specific queries then filter or group this data for particular visualizations. Base queries help you avoid duplicating the same logic across multiple tiles. Instead of writing similar queries for each tile, you create one base query and reference it from multiple tiles. For example, in a bike rental dashboard, you could create a base query through the dashboard's Base queries menu. When creating the base query, you would: Assign it the variable name _base_bike_data Define a query that returns all data fields in each neighborhood within the last 30 minutes: bikes\n| where ingestion_time() between (ago(30min) .. now())\n| summarize latest_observation = arg_max(ingestion_time(), *) by Neighbourhood Then, you can create individual tile queries that reference this base query by using its variable name: _base_bike_data\n| project Neighbourhood, latest_observation, No_Bikes, No_Empty_Docks\n| order by Neighbourhood asc",
        "links": [
            "https://learn.microsoft.com/wwl/create-real-time-dashboards-microsoft-fabric/media/base-query.png#lightbox"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/create-real-time-dashboards-microsoft-fabric/media/base-query.png",
                "image_alt": "Screenshot showing a base query configuration."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Create Real-Time Dashboards with Microsoft Fabric",
        "topic": "Exercise - Get started with real-time dashboards",
        "text": "Now it's your opportunity to create a Real-Time Dashboard using bike-sharing data from an eventstream. In this exercise, you build visualizations, configure parameters, and set up auto refresh functionality. This exercise takes approximately 30 minutes to complete. Important You need a Microsoft Fabric tenant to complete this exercise. See Getting started with Fabric for details. Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://go.microsoft.com/fwlink/?linkid=2272538&azure-portal=true"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Create Real-Time Dashboards with Microsoft Fabric",
        "topic": "Summary",
        "text": "In this module, you learned how to create Real-Time Dashboards in Microsoft Fabric and connect them to data sources. You also learned to add and configure tiles to visualize real-time data using KQL queries and organize dashboard content using multiple pages for logical grouping. You also discovered how to add interactive parameters to dynamically filter dashboard data and configure auto refresh to keep data current automatically. Finally, you learned to use base queries to improve maintainability when multiple tiles share related data. Real-Time Dashboards are a key component of Microsoft Fabric's Real-Time Intelligence capabilities, enabling you to provide users with current data visualizations that support time-sensitive decision making as events happen. Learn more Create a Real-Time Dashboard Add parameters to a Real-Time Dashboard",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/dashboard-real-time-create?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/dashboard-parameters?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Use Activator in Microsoft Fabric",
        "topic": "Introduction",
        "text": "Imagine you manage package delivery for a company with temperature-sensitive medicines that must stay cold during transport. You track the temperature readings and delivery status from your trucks and need to take immediate action when temperature variances threaten medicine safety. Activator in Microsoft Fabric can help you automatically evaluate these conditions and trigger actions like alerting your dispatch team or starting workflows that could initiate delivery changes when problems occur. What is Activator? Activator is Microsoft Fabric's event detection and rules engine within Real-Time Intelligence. To use Activator, first you connect to real-time data sources. Then you create rules to check for specific conditions, and when those conditions are met, Activator executes actions like sending email alerts, posting Teams messages, starting Power Automate flows, or running Fabric notebooks. Real-world scenarios Activator is useful in scenarios where timely responses matter: Manufacturing operations can automatically alert maintenance teams when equipment temperatures exceed safe operating ranges Supply chain managers can be notified when shipments deviate from planned routes or experience unexpected delays Retail managers can trigger inventory reorders when stock levels fall below critical thresholds IT operations teams can automatically restart services when performance metrics indicate system degradation Financial institutions can flag unusual transaction patterns for immediate review Healthcare facilities can alert staff when patient monitoring devices detect critical changes In this module, you'll learn how to connect Activator to your streaming data sources, create rules that automatically detect important conditions, and configure the actions that should be triggered when those conditions occur.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Use Activator in Microsoft Fabric",
        "topic": "Configure Activator for your data",
        "text": "Before you can evaluate conditions and trigger actions on real-time data, you need to configure how Activator should interpret your data for rule evaluation. Rules evaluate conditions against specific data points - like \"when this package's temperature exceeds 86F\" or \"when any delivery takes longer than 48 hours.\" There are several ways to configure Activator for monitoring your data. One approach uses business objects - representing of the real-world things you want to monitor, like packages, devices, or customers. Understand business objects Business objects represent the entities you want to monitor. For a package delivery company, each package becomes an object in Activator. Each object has properties which are the specific data points you want to monitor. Here's how this organization works: Objects represent individual instances (Package001, Package002, Package003) Properties represent the data attributes for each instance (Temperature, City, DeliveryState, HoursInTransit) Events contain the actual data values that flow in from your data sources Your data sources send events containing information for multiple packages. Activator uses this incoming data to automatically update the property values for each package object. Create objects from eventstreams When you configure Activator as a destination for an eventstream, you can then create business objects in Activator. During this setup, you tell Activator which data fields represent objects and which fields are properties to evaluate. Here's how you create objects from an eventstream: Configure Activator as a destination - In an eventstream Open the Activator - Then select the option to create a new object Choose your unique identifier - Select the field that uniquely identifies the object (like PackageId or DeviceID) Select properties - Choose which data fields you want to monitor as properties For example, if your eventstream contains package delivery data with fields like PackageId , Temperature , City , DeliveryState , and HoursInTransit , you might: Use PackageId as your unique identifier to create separate package objects Select Temperature and HoursInTransit as properties you want to monitor Let Activator automatically create objects for each unique package as data flows in Once configured, your eventstream continuously feeds data to Activator. New events update existing object properties, new unique identifiers automatically create new objects, and property values always reflect the most recent data from your stream.\nThis configuration means your rules can evaluate conditions as soon as new data arrives. Alternative alerting approaches The Activator engine also supports creating alerts. You can create dashboard alerts directly from Real-Time Dashboard visualizations, set up system event alerts to monitor Fabric workspace activities and OneLake file operations, or create query alerts from KQL Queryset results and visualizations. These approaches use the same Activator engine but interpret data differently than the business objects model.",
        "links": [
            "https://learn.microsoft.com/wwl/use-fabric-activator/media/create-object.png#lightbox"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-fabric-activator/media/create-object.png",
                "image_alt": "Screenshot showing the Activator interface for creating a new object"
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Use Activator in Microsoft Fabric",
        "topic": "Create rules in Activator",
        "text": "Rules define the conditions you want to detect on your objects and the actions to take when those conditions are met. Build your first rule Imagine you're managing a package delivery company with temperature-sensitive medicines. Each package has sensors sending temperature and location data to your Eventstream. You need to catch temperature problems before they damage valuable cargo. Let's walk through creating a rule for this scenario. When you create a rule in the Activator interface, a Definition pane opens with several sections to configure. We'll focus on the first three sections in the Definition pane now - Monitor, Condition, and Property filter - and cover Actions in the next unit. Choose what to monitor The Monitor section is where you configure what Activator watches. This section directs Activator to monitor specific data properties. First, you select an Attribute - the specific property from your event data. For our package delivery scenario, you'd choose Temperature from your package sensor data. Raw temperature readings can be noisy. A package might show brief spikes when moved or dips when passing through different environments. That's where Summarization becomes crucial for seeing the bigger picture: Average - Smooth out the noise by averaging readings over time Minimum/Maximum - Catch the extreme values that matter most Count - Track how many readings you're getting (useful for detecting sensor failures) Total - Add up values when you're counting events rather than measuring levels When you add summarization, you control two timing settings: Window size : How much historical data to include (maybe 10 minutes of temperature readings) Step size : How often to recalculate (perhaps every 5 minutes for near real-time monitoring) For example, instead of reacting to every single temperature reading, you might monitor the average temperature over 10-minute windows, updating every 5 minutes. This approach catches sustained problems while ignoring brief fluctuations. Define when to act The Condition section sets your execution point - when Activator should spring into action. You choose from different detection approaches: Threshold monitoring : Alert when values cross your safety limits (Temperature is greater than 68F) Change detection : Monitor trends (Temperature increases above baseline) Range monitoring : Track entry/exit from safe zones Missing data : Catch sensor failures ( No new events for more than 30 minutes) You also set the Value (your threshold like 68F) and Occurrence behavior: Every time for immediate alerts When it has been true for to identify persistent problems (stays high for 15 minutes) Focus your scope You might want to apply your rule only to specific events, not every event in your stream. The Property filter section lets you narrow down which events get evaluated. Maybe you only want to monitor: Cold chain packages: ColdChainType equals \"medicine\" High-priority routes: City equals \"Seattle\" Warm shipments: Temperature is greater than 68F You can combine up to three filters to create precise targeting. With your rule defined, the next step is configuring what actions Activator should take when the rule conditions are met.",
        "links": [
            "https://learn.microsoft.com/wwl/use-fabric-activator/media/activator-definition-pane.png#lightbox"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/use-fabric-activator/media/activator-definition-pane.png",
                "image_alt": "Screenshot of Microsoft Fabric Activator interface showing the Definition pane with Monitor, Condition, and Property filter sections for configuring a temperature rule."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Use Activator in Microsoft Fabric",
        "topic": "Configure actions in Activator",
        "text": "Actions are what happens when your rule conditions are met. They complete your Activator setup, turning condition evaluation into notifications and automated workflows. Activator offers four types of actions. Email actions Email actions send detailed information that people can review and respond to within hours or days. Use email when the recipient needs comprehensive context or when immediate response isn't critical. Teams actions Teams actions send immediate messages to channels or individuals. Use Teams when you need quick responses and team coordination. Teams notifications appear immediately and let teams discuss and coordinate their response. Power Automate actions Power Automate is Microsoft's workflow automation service that connects different apps and services together. Power Automate actions execute multi-step business processes across different systems - automatically performing a series of tasks that would normally require manual steps across multiple applications. Fabric item actions Fabric item actions execute data pipelines or notebooks for other processing and analysis. Use Fabric items when you need to process more data or perform advanced analysis in response to evaluated conditions. With data sources connected, rules defined, and actions configured, you have everything needed for Activator to automatically take action when conditions occur in your streaming data.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Use Activator in Microsoft Fabric",
        "topic": "Exercise - Use Activator in Fabric",
        "text": "Now it's your turn to work with Microsoft Fabric Activator. In this exercise, you create an Activator that monitors data and set up detection rules that automatically send email alerts. You create objects, define filters, and configure automated responses. Note You need a Microsoft Fabric trial to complete this exercise. Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://go.microsoft.com/fwlink/?linkid=2335360&azure-portal=true"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement Real-Time Intelligence with Microsoft Fabric",
        "unit": "Use Activator in Microsoft Fabric",
        "topic": "Summary",
        "text": "In this module, you learned how to use Microsoft Fabric Activator to automatically detect conditions in streaming data and execute actions. You explored how to configure Activator and create rules that monitor specific conditions in your data streams. You also learned to configure automated actions that execute when your defined conditions are met. Additional resources Continue your learning with these resources: What is Fabric Activator Fabric Activator rules",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/data-activator/activator-introduction?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/data-activator/activator-rules-overview?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Introduction to end-to-end analytics using Microsoft Fabric",
        "topic": "Introduction",
        "text": "Microsoft Fabric is an end-to-end analytics platform that provides a single, integrated environment for data professionals and the business to collaborate on data projects. Fabric provides a set of integrated services that enable you to ingest, store, process, and analyze data in a single environment. Microsoft Fabric provides tools for both citizen and professional data practitioners, and integrates with tools the business needs to make decisions. Fabric includes the following services: Data engineering Data integration Data warehousing Real-time intelligence Data science Business intelligence Additionally, Microsoft Fabric integrates Copilot, a generative AI assistant that enhances productivity across all workloads by providing intelligent code completion, natural language to SQL conversion, automated insights, and contextual assistance for data professionals and business users alike. This module introduces the Fabric platform, discusses who Fabric is for, explores Fabric services, and examines how Copilot enhances the analytics experience.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Introduction to end-to-end analytics using Microsoft Fabric",
        "topic": "Explore end-to-end analytics with Microsoft Fabric",
        "text": "Scalable analytics can be complex, fragmented, and expensive. Microsoft Fabric simplifies analytics solutions by providing a single, easy-to-use product that integrates various tools and services into one platform. Fabric is a unified software-as-a-service (SaaS) platform where all data is stored in a single open format in OneLake. OneLake is accessible by all analytics engines in the platform, ensuring scalability, cost-effectiveness, and accessibility from anywhere with an internet connection. OneLake OneLake is Fabric's centralized data storage architecture that enables collaboration by eliminating the need to move or copy data between systems. OneLake unifies your data across regions and clouds into a single logical lake without moving or duplicating data. OneLake is built on Azure Data Lake Storage (ADLS) and supports various formats, including Delta, Parquet, CSV, and JSON. All compute engines in Fabric automatically store their data in OneLake, making it directly accessible without the need for movement or duplication. For tabular data, the analytical engines in Fabric write data in delta-parquet format and all engines interact with the format seamlessly. Shortcuts are references to files or storage locations external to OneLake, allowing you to access existing cloud data without copying it. Shortcuts ensure data consistency and enable Fabric to stay in sync with the source. Workspaces In Microsoft Fabric, workspaces serve as logical containers that help you organize and manage your data, reports, and other assets. They provide a clear separation of resources, making it easier to control access and maintain security. Each workspace has its own set of permissions, ensuring that only authorized users can view or modify its contents. This structure supports team collaboration while maintaining strict access control for both business and IT users. Workspaces allow you to manage compute resources and integrate with Git for version control. You can optimize performance and cost by configuring compute settings, while Git integration helps track changes, collaborate on code, and maintain a history of your work. Administration and governance Fabric's OneLake is centrally governed and open for collaboration. Data is secured and governed in one place, which allows users to easily find and access the data they need. Fabric administration is centralized in the Admin portal . In the admin portal you can manage groups and permissions, configure data sources and gateways, and monitor usage and performance. You can also access the Fabric admin APIs and SDKs in the admin portal, which can automate common tasks and integrate Fabric with other systems. The OneLake catalog helps you analyze, monitor, and maintain data governance. It provides guidance on sensitivity labels, item metadata, and data refresh status, offering insights into the governance status and actions for improvement. Note Review the Microsoft Fabric administration documentation for more information.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/admin"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/introduction-end-analytics-use-microsoft-fabric/media/onelake-architecture.png",
                "image_alt": "Diagram of the OneLake architecture with workloads accessing the same OneLake data storage and Delta-Parquet storage format as the foundation for serverless compute."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Introduction to end-to-end analytics using Microsoft Fabric",
        "topic": "Explore data teams and Microsoft Fabric",
        "text": "Microsoft Fabric's unified data analytics platform makes it easier for data professionals to collaborate on projects. Fabric increases collaboration between data professionals by removing data silos and the need for multiple systems. Traditional roles and challenges In a traditional analytics development process, data teams often face several challenges due to the division of data tasks and workflows. Data engineers process and curate data for analysts, who then use it to create business reports. This process requires extensive coordination, often leading to delays and misinterpretations. Data analysts often need to perform downstream data transformations before creating Power BI reports. This process is time-consuming and can lack the necessary context, making it harder for analysts to connect directly with the data. Data scientists face difficulties integrating native data science techniques with existing systems, which are often complex, and makes it challenging to efficiently provide data-driven insights. Evolution of collaborative workflows Microsoft Fabric simplifies the analytics development process by unifying tools into a SaaS platform. Fabric allows different roles to collaborate effectively without duplicating efforts. Data engineers can ingest, transform, and load data directly into OneLake using Pipelines, which automate workflows and support scheduling. They can store data in lakehouses, using the Delta-Parquet format for efficient storage and versioning. Notebooks provide advanced scripting capabilities for complex transformations. Analytics engineers bridge the gap between data engineering and analysis by curating data assets in lakehouses, ensuring data quality, and enabling self-service analytics. They can create semantic models in Power BI to organize and present data effectively. Data analysts can transform data upstream using dataflows and connect directly to OneLake with Direct Lake mode, reducing the need for downstream transformations. They can create interactive reports more efficiently using Power BI. Data scientists can use integrated notebooks with support for Python and Spark to build and test machine learning models. They can store and access data in lakehouses and integrate with Azure Machine Learning to operationalize and deploy models. Low-to-no-code users and citizen developers can discover curated datasets through the OneLake catalog and use Power BI templates to quickly create reports and dashboards. They can also use dataflows to perform simple ETL tasks without relying on data engineers.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Introduction to end-to-end analytics using Microsoft Fabric",
        "topic": "Enable and use Microsoft Fabric",
        "text": "Before you can explore the end-to-end capabilities of Microsoft Fabric, it must be enabled for your organization. You might need to work with your IT department to enable Fabric for your organization, including one of the following roles: Fabric admin (formerly Power BI admin) : Manages Fabric settings and configurations. Power Platform admin : Oversees Power Platform services, including Fabric. Microsoft 365 admin : Manages organization-wide Microsoft services, including Fabric. Enable Microsoft Fabric Admins can enable Fabric in the Admin portal > Tenant settings in the Power BI service. Fabric can be enabled for the entire organization or for specific Microsoft 365 or Microsoft Entra security groups. Admins can also delegate this ability to other users at the capacity level. Note If your organization isn't using Fabric or Power BI today, you can sign up for a free Fabric trial to explore its features. Create workspaces Workspaces are collaborative environments where you can create and manage items like lakehouses, warehouses, and reports. All data is stored in OneLake and accessed through workspaces. Workspaces also support data lineage view, providing a visual view of data flow and dependencies to enhance transparency and decision-making. In Workspace settings , you can configure: License type to use Fabric features. OneDrive access for the workspace. Azure Data Lake Gen2 Storage connection. Git integration for version control. Spark workload settings for performance optimization. You can manage workspace access through four roles: admin , contributor , member , and viewer . These roles apply to all items in a workspace and should be reserved for collaboration. For more granular access control, use item-level permissions based on business needs. Note Learn more about workspaces in the Fabric documentation . Discover data with OneLake catalog The OneLake catalog in Microsoft Fabric helps users easily find and access various data sources within their organization. Users explore and connect to data sources, ensuring they have the right data for their needs. Users only see items shared with them. Here are some considerations when using OneLake catalog: Narrow results by workspaces or domains (if implemented). Explore default categories to quickly locate relevant data. Filter by keyword or item type. Create items with Fabric workloads After you create your Fabric enabled workspace, you can start creating items in Fabric. Each workload in Fabric offers different item types for storing, processing, and analyzing data. Fabric workloads include: Data Engineering : Create lakehouses and operationalize workflows to build, transform, and share your data estate. Data Factory : Ingest, transform, and orchestrate data. Data Science : Detect trends, identify outliers, and predict values using machine learning. Data Warehouse : Combine multiple sources in a traditional warehouse for analytics. Databases : Create and manage databases with tools to insert, query, and extract data. Industry Solutions : Use out-of-the-box industry data solutions. Real-Time Intelligence : Process, monitor, and analyze streaming data. Power BI : Create reports and dashboards to make data-driven decisions. Fabric integrates capabilities from existing Microsoft tools like Power BI, Azure Synapse Analytics, and Azure Data Factory into a unified platform. Fabric also supports a data mesh architecture, allowing decentralized data ownership while maintaining centralized governance. This design eliminates the need for direct Azure resource access, simplifying data workflows. Enhance productivity with Copilot in Fabric Microsoft Copilot in Fabric is a generative AI assistant that enhances productivity and accelerates insights across all Fabric workloads. Copilot uses large language models (LLMs) to provide intelligent assistance for data professionals, self-service users, and business users. Copilot capabilities across workloads Copilot provides tailored assistance for each Fabric workload: Data Engineering and Data Science : Intelligent code completion, automated routine tasks, industry-standard code templates, and contextual code suggestions that adapt to specific tasks. Copilot helps with data preparation, pipeline building, and insight generation. Data Factory : AI-enhanced toolset supporting both citizen and professional data wranglers with intelligent code generation for data transformation and code explanations for complex tasks. Data Warehouse and SQL Database : Natural language to SQL conversion, code completion, quick actions, and intelligent insights. Users can describe what they want in plain language, and Copilot generates the appropriate SQL queries. Power BI : Automatic report generation, summary creation for report pages, synonym generation for better Q&A capabilities, and natural language querying of data. Business users can also use Copilot to extract more insights and chat with the report data. Real-Time Intelligence : Advanced AI tool that translates natural language questions into Kusto Query Language (KQL) queries, streamlining data analysis for both experienced users and citizen data scientists. Enabling Copilot Administrators must enable Copilot in the Admin portal > Tenant settings in the Power BI service. Copilot helps users work more efficiently by providing AI assistance for common tasks like writing code, generating queries, and creating reports. This support is available to both technical and business users while maintaining organizational security policies. Tip For more information, see the Copilot for Microsoft Fabric and Power BI documentation.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial",
            "https://learn.microsoft.com/en-us/fabric/get-started/workspaces",
            "https://learn.microsoft.com/wwl/introduction-end-analytics-use-microsoft-fabric/media/onelake-catalog.png#lightbox",
            "https://learn.microsoft.com/en-us/fabric/fundamentals/copilot-faq-fabric"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/introduction-end-analytics-use-microsoft-fabric/media/enable-fabric.png",
                "image_alt": "Screenshot of the Fabric admin portal where you can enable Fabric items."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/introduction-end-analytics-use-microsoft-fabric/media/onelake-catalog.png",
                "image_alt": "Screenshot of the OneLake catalog."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Introduction to end-to-end analytics using Microsoft Fabric",
        "topic": "Summary",
        "text": "Data professionals are increasingly expected to be able to work with data at scale, and to be able to do so in a way that is secure, compliant, and cost-effective. At the same time, the business wants to use that data more effectively and quickly to make better decisions. Microsoft Fabric is a collection of tools and services that enables organizations to do just that. In this module, you learned about Fabric's OneLake storage, the workloads that are included in Fabric, how to enable and use Fabric in your organization, and how Copilot enhances productivity across all Fabric workloads with intelligent AI assistance.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Get started with data warehouses in Microsoft Fabric",
        "topic": "Introduction",
        "text": "Relational data warehouses are at the center of most enterprise business intelligence (BI) solutions. While the specific details may vary across data warehouse implementations, a common pattern based on a denormalized, multidimensional schema has emerged as the standard design for a relational data warehouse. Microsoft Fabric's data warehouse is a modern version of the traditional data warehouse. It centralizes and organizes data from different departments, systems, and databases into a single, unified view for analysis and reporting purposes. Fabric's data warehouse provides full SQL semantics, including the ability to insert, update, and delete data in the tables. Fabric's data warehouse is unique because it's built on the Lakehouse, which is stored in Delta format and can be queried using SQL. It's designed for use by the whole data team, not just data engineers. Fabric's data warehouse experience is designed to address these challenges. Fabric enables data engineers, analysts, and data scientists to work together to create and query a data warehouse that is optimized for their specific needs. In this module, you'll learn about data warehouses in Fabric, create a data warehouse, load, query, and visualize data.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Get started with data warehouses in Microsoft Fabric",
        "topic": "Understand data warehouse fundamentals",
        "text": "The process of building a modern data warehouse typically consists of: Data ingestion - moving data from source systems into a data warehouse. Data storage - storing the data in a format that is optimized for analytics. Data processing - transforming the data into a format that is ready for consumption by analytical tools. Data analysis and delivery - analyzing the data to gain insights and delivering those insights to the business. Microsoft Fabric enables data engineers and analysts to ingest, store, transform, and visualize data all in one tool with both a low-code and traditional experience. Understand Fabric's data warehouse experience Fabric's data warehouse is a relational data warehouse that supports the full transactional T-SQL capabilities you'd expect from an enterprise data warehouse. It's a fully managed, scalable, and highly available data warehouse that can be used to store and query data in the Lakehouse. Using the data warehouse, you're fully in control of creating tables, loading, transforming, and querying data using either the Fabric portal or T-SQL commands. You can use SQL to query and analyze the data, or use Spark to process the data and create machine learning models. Data warehouses in Fabric facilitate collaboration between data engineers and data analysts, working together in the same experience. Data engineers build a relational layer on top of data in the Lakehouse, where analysts can use T-SQL and Power BI to explore the data. Design a data warehouse Like all relational databases, Fabric's data warehouse contains tables to store your data for analytics later. Most commonly, these tables are organized in a schema that is optimized for multidimensional modeling. In this approach, numerical data related to events (e.g. sales orders) are grouped by different attributes (e.g. date, customer, store). For instance, you can analyze the total amount paid for sales orders that occurred on a specific date or at a particular store. Tables in a data warehouse Tables in a data warehouse are typically organized in a way that supports efficient and effective analysis of large amounts of data. This organization is often referred to as dimensional modeling, which involves structuring tables into fact tables and dimension tables. Fact tables contain the numerical data that you want to analyze. Fact tables typically have a large number of rows and are the primary source of data for analysis. For example, a fact table might contain the total amount paid for sales orders that occurred on a specific date or at a particular store. Dimension tables contain descriptive information about the data in the fact tables. Dimension tables typically have a small number of rows and are used to provide context for the data in the fact tables. For example, a dimension table might contain information about the customers who placed sales orders. In addition to attribute columns, a dimension table contains a unique key column that uniquely identifies each row in the table. In fact, it's common for a dimension table to include two key columns: A surrogate key is a unique identifier for each row in the dimension table. It's often an integer value that is automatically generated by the database management system when a new row is inserted into the table. An alternate key is often a natural or business key that identifies a specific instance of an entity in the transactional source system - such as a product code or a customer ID. You need both surrogate and alternate keys in a data warehouse, because they serve different purposes. Surrogate keys are specific to the data warehouse and help to maintain consistency and accuracy in the data. Alternate keys on the other hand are specific to the source system and help to maintain traceability between the data warehouse and the source system. Special types of dimension tables Special types of dimensions provide additional context and enable more comprehensive data analysis. Time dimensions provide information about the time period in which an event occurred. This table enables data analysts to aggregate data over temporal intervals. For example, a time dimension might include columns for the year, quarter, month, and day in which a sales order was placed. Slowly changing dimensions are dimension tables that track changes to dimension attributes over time, like changes to a customer's address or a product's price. They're significant in a data warehouse because they enable users to analyze and understand changes to data over time. Slowly changing dimensions ensure that data stays up-to-date and accurate, which is imperative to making good business decisions. Data warehouse schema designs In most transactional databases that are used in business applications, the data is normalized to reduce duplication. In a data warehouse however, the dimension data is generally de-normalized to reduce the number of joins required to query the data. Often, a data warehouse is organized as a star schema , in which a fact table is directly related to the dimension tables, as shown in this example: You can use the attributes of something to group together numbers in the fact table at different levels. For example, you could find the total sales revenue for a whole region or just for one customer. The information for each level can be stored in the same dimension table. Tip See What is a star schema? for more information on designing star schemas for Fabric. If there are lots of levels or some information is shared by different things, it might make sense to use a snowflake schema instead. Here's an example: In this case, the DimProduct table has been split up (normalized) to create separate dimension tables for product categories and suppliers. Each row in the DimProduct table contains key values for the corresponding rows in the DimCategory and DimSupplier tables . A DimGeography table has been added containing information on where customers and stores are located. Each row in the DimCustomer and DimStore tables contains a key value for the corresponding row in the DimGeography table.",
        "links": [
            "https://learn.microsoft.com/en-us/power-bi/guidance/star-schema"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-data-warehouse/media/star-schema.png",
                "image_alt": "Diagram of a star schema design displaying a FactSales table with five dimensions that form the shape of a star."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-data-warehouse/media/snowflake-schema.png",
                "image_alt": "Diagram of a snowflake schema design displaying multiple dimensions."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Get started with data warehouses in Microsoft Fabric",
        "topic": "Understand data warehouses in Fabric",
        "text": "Fabric's Lakehouse is a collection of files, folders, tables, and shortcuts that act like a database over a data lake. It's used by the Spark engine and SQL engine for big data processing and has features for ACID transactions when using the open-source Delta formatted tables. Fabric's data warehouse experience allows you to transition from the lake view of the Lakehouse (which supports data engineering and Apache Spark) to the SQL experiences that a traditional data warehouse would provide. The Lakehouse gives you the ability to read tables and use the SQL analytics endpoint, whereas the data warehouse enables you to manipulate the data. In the data warehouse experience, you'll model data using tables and views, run T-SQL to query data across the data warehouse and Lakehouse, use T-SQL to perform DML operations on data inside the data warehouse, and serve reporting layers like Power BI. Now that you understand the basic architectural principles for a relational data warehouse schema, let's explore how to create a data warehouse. Describe a data warehouse in Fabric In the data warehouse experience in Fabric, you can build a relational layer on top of physical data in the Lakehouse and expose it to analysis and reporting tools. You can create your data warehouse directly in Fabric from the create hub or within a workspace . After creating an empty warehouse, you can add objects to it. Once your warehouse is created, you can create tables using T-SQL directly in the Fabric interface. Ingest data into your data warehouse There are a few ways to ingest data into a Fabric data warehouse, including Pipelines , Dataflows , cross-database querying , and the COPY INTO command. After ingestion, the data becomes available for analysis by multiple business groups, who can use features such as cross-database querying and sharing to access it. Create tables To create a table in the data warehouse, you can use SQL Server Management Studio (SSMS) or another SQL client to connect to the data warehouse and run a CREATE TABLE statement. You can also create tables directly in the Fabric UI. You can copy data from an external location into a table in the data warehouse using the COPY INTO syntax. For example: COPY INTO dbo.Region \nFROM 'https://mystorageaccountxxx.blob.core.windows.net/private/Region.csv' WITH ( \n            FILE_TYPE = 'CSV'\n            ,CREDENTIAL = ( \n                IDENTITY = 'Shared Access Signature'\n                , SECRET = 'xxx'\n                )\n            ,FIRSTROW = 2\n            )\nGO This SQL query loads data from a CSV file stored in Azure Blob Storage into a table called \"Region\" in the Fabric data warehouse. Clone tables You can create zero-copy table clones with minimal storage costs in a data warehouse. These clones are essentially replicas of tables created by copying the metadata while still referencing the same data files in OneLake. This means that the underlying data stored as parquet files is not duplicated, which helps in saving storage costs. Table clones are particularly useful in several scenarios. Development and testing: Clones allow developers and testers to create copies of tables in lower environments, facilitating development, debugging, testing, and validation processes. Data recovery: In the event of a failed release or data corruption, table clones can retain the previous state of data, enabling data recovery. Historical reporting: They help create historical reports that reflect the state of data at specific points in time and preserve data at specific business milestones. You can create a table clone using the CREATE TABLE AS CLONE OF T-SQL command. To learn more about table clones, see Tutorial: Clone a table using T-SQL in Microsoft Fabric . Table considerations After creating tables in a data warehouse, it's important to consider the process of loading data into those tables. A common approach is to use staging tables . In Fabric, you can use T-SQL commands to load data from files into staging tables in the data warehouse. Staging tables are temporary tables that can be used to perform data cleansing, data transformations, and data validation. You can also use staging tables to load data from multiple sources into a single destination table. Usually, data loading is performed as a periodic batch process in which inserts and updates to the data warehouse are coordinated to occur at a regular interval (for example, daily, weekly, or monthly). Generally, you should implement a data warehouse load process that performs tasks in the following order: Ingest the new data to be loaded into a data lake, applying pre-load cleansing or transformations as required. Load the data from files into staging tables in the relational data warehouse. Load the dimension tables from the dimension data in the staging tables, updating existing rows or inserting new rows and generating surrogate key values as necessary. Load the fact tables from the fact data in the staging tables, looking up the appropriate surrogate keys for related dimensions. Perform post-load optimization by updating indexes and table distribution statistics. If you have tables in the lakehouse, and you want to be able to query it in your warehouse - but not make changes - with a Fabric data warehouse, you don't have to copy data from the lakehouse to the data warehouse. You can query data in the lakehouse directly from the data warehouse using cross-database querying. Important Working with tables in the Fabric data warehouse currently has some limitations. See Tables in data warehousing in Microsoft Fabric for more information.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/clone-table",
            "https://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-as-clone-of-transact-sql",
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/tutorial-clone-table",
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/tables"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-data-warehouse/media/create-data-warehouse.png",
                "image_alt": "Screenshot of the Fabric UI with an arrow pointing to the create hub."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-data-warehouse/media/create-table-manual.png",
                "image_alt": "Screenshot of the SQL query editor with a query open."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Get started with data warehouses in Microsoft Fabric",
        "topic": "Query and transform data",
        "text": "Now that you know how to implement a data warehouse in Fabric, let's prepare the data for analytics. There are two ways to query data from your data warehouse. The Visual query editor provides a no-code, drag-and-drop experience to create your queries. If you're comfortable with T-SQL, you may prefer to use the SQL query editor to write your queries. In both cases, you can create tables, views, and stored procedures to query data in the data warehouse and Lakehouse. There's also a SQL analytics endpoint, where you can connect from any tool. Query data using the SQL query editor The SQL query editor provides a query experience that includes intellisense, code completion, syntax highlighting, client-side parsing, and validation. If you've written T-SQL in SQL Server Management Studio (SSMS) or Azure Data Studio (ADS), you'll find it familiar. To create a new query, use the New SQL query button in the menu. You can author and run your T-SQL queries here. In the example below we're creating a new view for analysts to use for reporting in Power BI. Query data using the Visual query editor The Visual query editor provides an experience similar to the Power Query online diagram view . Use the New visual query button to create a new query. Drag a table from your data warehouse onto the canvas to get started. You can then use the Transform menu at the top of the screen to add columns, filters, and other transformations to your query. You can use the  (+) button on the visual itself to perform similar transformations.",
        "links": [
            "https://learn.microsoft.com/en-us/power-query/diagram-view"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-data-warehouse/media/create-view.png",
                "image_alt": "Screenshot of the SQL Query Editor displaying a T-SQL query creating a view."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-data-warehouse/media/visual-query.png",
                "image_alt": "Screenshot of the Visual Query Editor."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Get started with data warehouses in Microsoft Fabric",
        "topic": "Prepare data for analysis and reporting",
        "text": "A semantic model defines the relationships between the different tables in the semantic model, the rules for how data is aggregated and summarized, and the calculations or measures that are used to derive insights from the data. These relationships and measures are included in the semantic model, which is then used to create reports in Power BI. You can easily switch between the Data , Query , and Model view Fabric using the menu in the bottom left corner of the screen. The Data view shows the tables in the semantic model, the Query view shows the SQL queries that are used to create the semantic model, and the Model view shows the semantic model. Tip See Analyze data in a relational data warehouse to learn more about data models and data warehouse schema. Build relationships Relationships allow you to connect tables in the semantic model. Create relationships between tables in your data warehouse using a click-and-drag interface in Fabric in the Model view. Tip See Create and manage relationships for detailed information on creating relationships. Create measures Measures are the metrics that you want to analyze in your data warehouse. You can create measures in Fabric by using the New measure button in the Model view. Measures are calculated fields that are based on the data in the tables in your data warehouse using the Data Analysis Expressions (DAX) formula language. Note Fabric offers many tools to create data transformations. The creation of measures using DAX is one of many ways to create data transformations. To learn more about DAX, see Use DAX in Power BI . Hide fields Building out the semantic model is a critical component to preparing your data for use in downstream reporting.  To simplify things for your report builders, you can hide elements from view, either a table or a column. Right-click on the table or column and select Hide .  Hiding fields removes the table or column from the model view, but it will still be available for use in the semantic model. Understand semantic models Every time a data warehouse is created, Fabric creates a semantic model for analysts and/or business users to connect to for reporting. Semantic model has metrics that are used to create reports. Simply put, analysts use the semantic model you created in your warehouse, which is stored in a semantic model. If you're familiar with Power BI, working with semantic models created by the data warehouse experience will be straightforward. Semantic models are automatically kept in sync with the data warehouse, so you don't have to worry about maintaining them. You can also create custom semantic models to meet your specific needs. Understand the default semantic model There's also a default semantic model automatically created for you in Fabric. It inherits business logic from the parent lakehouse or warehouse, which initiates the downstream analytics experience for business intelligence and analysis. This semantic model is managed, optimized, and kept in sync for you. New tables in the Lakehouse are automatically added to the default semantic model. Users can also manually select tables or views from the warehouse they want included in the model for more flexibility. Objects that are in the default semantic model are created as a layout in the model view. Note Default semantic models follow the current limitations for semantic models in Power BI. See Default Power BI semantic models for more information. Visualize data Fabric enables you to visualize the results of a single query or your entire data warehouse, without leaving the data warehouse experience. Exploring data while you work to ensure you have all the necessary data and transformations for your analysis is particularly useful. Use the New report button to create a new Power BI report from the contents of your entire data warehouse. Using the New report button opens the Power BI service experience where you can build and save your report for use by the business.",
        "links": [
            "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/",
            "https://learn.microsoft.com/en-us/power-bi/transform-model/desktop-create-and-manage-relationships",
            "https://learn.microsoft.com/en-us/training/paths/dax-power-bi/",
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/default-power-bi-semantic-model"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-data-warehouse/media/create-relationships.png",
                "image_alt": "Screenshot of the model view in Fabric displaying relationships between a fact table and three dimension tables."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-data-warehouse/media/create-measure.png",
                "image_alt": "Screenshot of the model view in Fabric displaying the new measure button and a draft measure being written for month over month sales."
            },
            {
                "image_name": "image3",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-data-warehouse/media/hide-fields.png",
                "image_alt": "Screenshot of the FactSalesOrder table with hidden fields highlighted in yellow."
            },
            {
                "image_name": "image4",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/get-started-data-warehouse/media/sales-report.png",
                "image_alt": "Screenshot of a Power BI report."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Get started with data warehouses in Microsoft Fabric",
        "topic": "Secure and monitor your data warehouse",
        "text": "Security and monitoring are critical aspects of managing your data warehouse. Security Data warehouse security is important to protect your data from unauthorized access. Fabric provides a number of security features to help you secure your data warehouse. These include: Role-based access control (RBAC) to control access to the warehouse and its data. TLS encryption to secure the communication between the warehouse and the client applications. Azure Storage Service Encryption to protect the data in transit and at rest. Azure Monitor and Azure Log Analytics to monitor the warehouse activity and audit the access to the data. Multifactor authentication (MFA) to add an extra layer of security to user accounts. Microsoft Entra ID integration to manage the user identities and access to the warehouse. Workspace permissions Data in Fabric is organized into workspaces, which are used to control access and manage the lifecycle of data and services. Appropriate workspace roles are the first line of defense in securing your data warehouse. In addition to workspace roles, you can grant item permissions and access through SQL. Tip See Workspaces in Power BI for more information on workspace roles. Item permissions In contrast to workspace roles, which apply to all items within a workspace, you can use item permissions to grant access to individual warehouses. This enables you to share a single data warehouse for downstream consumption. You can grant permissions to users via T-SQL or in the Fabric portal. Grant the following permissions to users who need to access your data warehouse: Read: Allows the user to CONNECT using the SQL connection string. ReadData: Allows the user to read data from any table/view within the warehouse. ReadAll: Allows user to read data the raw parquet files in OneLake that can be consumed by Spark. A user connection to the SQL analytics endpoint will fail without Read permission at a minimum. Monitoring Monitoring activities in your data warehouse is crucial to ensure optimal performance, efficient resource utilization, and security. It helps you identify issues, detect anomalies, and take action to keep the data warehouse running smoothly and securely. You can use dynamic management views (DMVs) to monitor connection, session, and request status to see live SQL query lifecycle insights. With DMVs, you can get details like the number of active queries and identify which queries are running for an extended period and require termination. There are currently three DMVs available to use in Fabric: sys.dm_exec_connections: Returns information about each connection established between the warehouse and the engine. sys.dm_exec_sessions: Returns information about each session authenticated between the item and engine. sys.dm_exec_requests: Returns information about each active request in a session. Query monitoring Use 'sys.dm_exec_requests' to identify long-running queries that may be impacting the overall performance of the database, and take appropriate action to optimize or terminate those queries. Start by identifying the queries that have been running for a long time. Use the following query to identify which queries have been running the longest, in descending order: SELECT request_id, session_id, start_time, total_elapsed_time\n    FROM sys.dm_exec_requests\n    WHERE status = 'running'\n    ORDER BY total_elapsed_time DESC; You can continue investigating to understand which user ran the session with the long-running query, by running: SELECT login_name\n    FROM sys.dm_exec_sessions\n    WHERE session_id = 'SESSION_ID WITH LONG-RUNNING QUERY'; Finally, you can use the KILL command to terminate the session with the long-running query: KILL 'SESSION_ID WITH LONG-RUNNING QUERY'; Important You must be a workspace Admin to run the KILL command. Workspace Admins can execute all three DMVs. Member, Contributor, and Viewer roles can see their own results within the warehouse, but cannot see other users' results.",
        "links": [
            "https://learn.microsoft.com/en-us/power-bi/collaborate-share/service-new-workspaces#roles-and-licenses"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Get started with data warehouses in Microsoft Fabric",
        "topic": "Exercise - Analyze data in a data warehouse",
        "text": "Now it's your turn to create a data warehouse in Fabric and analyze your data. Note You need a Microsoft Fabric trial license with the Fabric preview enabled in your tenant. See Getting started with Fabric to enable your Fabric trial license. Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://go.microsoft.com/fwlink/?linkid=2259608"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Get started with data warehouses in Microsoft Fabric",
        "topic": "Summary",
        "text": "In this module, you learned about data warehouses and dimensional modeling, created a data warehouse, loaded, queried, and visualized data, and described semantic models and how they're used for downstream reporting.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Load data into a Microsoft Fabric data warehouse",
        "topic": "Introduction",
        "text": "Microsoft Fabric Data Warehouse is a complete platform for data, analytics, and AI (Artificial Intelligence). It refers to the process of storing, organizing, and managing large volumes of structured and semi-structured data. Data warehouse in Microsoft Fabric is powered up with Synapse Analytics by offering a rich set of features that make it easier to manage and analyze data. It includes advanced query processing capabilities, and supports the full transactional T-SQL capabilities like an enterprise data warehouse. Unlike a dedicated SQL pool in Synapse Analytics, a warehouse in Microsoft Fabric is centered around a single data lake. The data in the Microsoft Fabric warehouse is stored in the Parquet file format. This setup allows users to focus on tasks such as data preparation, analysis, and reporting. It takes advantage of the SQL engines extensive capabilities, where a unique copy of their data is stored in Microsoft OneLake . Understand the ETL (Extract, Transform, and Load) process ETL provides the foundation for data analytics and data warehouse workstreams. Let's review some aspects of data manipulation in an ETL process. Description Data extraction It involves connecting to the source system and collecting necessary data for analytical processing. Data transformation It involves a series of steps performed on the extracted data to convert it into a standard format. Combining data from different tables, cleaning data, deduplicating data, and performing data validations. Data loading The extracted and transformed data are loaded into the fact and dimension tables. For an incremental load, this involves periodically applying ongoing changes as per requirement. This process often involves reformatting the data to ensure its quality and compatibility with the data warehouse schema. Post-load optimizations Once the data is loaded, certain optimizations can be performed to enhance the performance of the data warehouse. All these steps in the ETL process can run in parallel depending on the scenario. As soon as some data is ready, it's loaded without waiting for the previous steps to be completed. In the next units, we'll explore various ways of loading data in a warehouse, and how they can facilitate the tasks of building a data warehouse workload.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/onelake/onelake-overview?azure-portal=true",
            "https://learn.microsoft.com/wwl-data-ai/load-data-into-microsoft-fabric-data-warehouse/media/1-access-onelake-data-other-tools.png#lightbox"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/load-data-into-microsoft-fabric-data-warehouse/media/1-access-onelake-data-other-tools.png",
                "image_alt": "Diagram showing the function and structure of OneLake."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Load data into a Microsoft Fabric data warehouse",
        "topic": "Explore data load strategies",
        "text": "In Microsoft Fabric, there are many ways you can choose to load data in a warehouse. This step is fundamental as it ensures that high-quality, transformed, or processed data is integrated into a single repository. Also, the efficiency of data loading directly impacts the timeliness and accuracy of analytics, making it vital for real-time decision-making processes. Investing time and resources in designing and implementing a robust data loading strategy is essential for the success of the data warehouse project. Understand data ingestion and data load operations While both processes are part of the ETL (Extract, Transform, Load) pipeline in a data warehouse scenario, they usually serve different purposes. Data ingestion/extract is about moving raw data from various sources into a central repository. On the other hand, data loading involves taking the transformed or processed data and loading it into the final storage destination for analysis and reporting. Fabric data warehouses and lakehouses automatically store their data in OneLake using the Delta Parquet format. Stage your data You might have to build and work with auxiliary objects involved in a load operation, such as tables, stored procedures, and functions. These auxiliary objects are commonly known as staging . Staging objects act as temporary storage and transformation areas. They can either share resources with a data warehouse or reside in their own storage area. Staging serves as an abstraction layer, simplifying and facilitating the load operation to the final tables in the data warehouse. Also, staging area provides a buffer that can help to minimize the impact of the load operation on the performance of the data warehouse. This is important in environments where the data warehouse needs to remain operational and responsive during the data loading process. Review type of data loads There are two types of data loads to consider when loading a data warehouse. Load Type Description Operation Duration Complexity Best used Full (initial) load The process of populating the data warehouse for the first time. All the tables are truncated and reloaded, and the old data is lost It may take longer to complete due to the amount of data being handled Easier to implement as there's no history preserved This method is typically used when setting up a new data warehouse, or when a complete refresh of the data is required Incremental load The process of updating the data warehouse with the changes since the last update The history is preserved, and tables are updated with new information Takes less time than the initial load Implementation is more complex than the initial load This method is commonly used for regular updates to the data warehouse, such as daily or hourly updates. It requires mechanisms to track changes in the source data since the last load. An ETL (Extract, Transform, Load) process for a data warehouse doesn't always need both the full load and the incremental load. In some cases, a combination of both methods might be used. The choice between a full load and an incremental load depends on many factors such as the amount of data, the characteristics of the data, and the requirements of the data warehouse. To learn more about how to perform an incremental load, see Incremental load . Understand business key and surrogate key In a data warehouse, both surrogate keys and business keys are essential for effective data warehousing and data integration, but they serve different purposes. Surrogate key: A surrogate key is a system-generated identifier that is used to uniquely identify a record in a table within the data warehouse. It has no business meaning and is typically an integer or a unique identifier. Surrogate keys are used to maintain consistency and accuracy in the data warehouse, especially when integrating data from multiple sources. They help to avoid issues that can arise from changes in the source systems, such as reusing or changing business keys. Business Key: A business key, also known as a natural key, is an identifier that comes from the source system and has business meaning. It's used to uniquely identify a record in the source system. Examples of business keys include product codes, customer IDs, and employee numbers. Business keys are important for maintaining traceability between the data warehouse and the source systems. They help to ensure that data in the warehouse can be accurately matched to the corresponding records in the source systems. Load a dimension table Think of a dimension table as the \"who, what, where, when, why of your data warehouse. Its like the descriptive backdrop that gives context to the raw numbers found in the fact tables. For example, if youre running an online store, your fact table might contain the raw sales data - how many units of each product were sold. But without a dimension table, you wouldnt know who bought those products, when they were bought, or where the buyer is located. Slowly changing dimensions (SCD) Slowly Changing Dimensions evolve over time, but at a slow pace and unpredictably. Take, for instance, a customer's address in a retail business. When a customer relocates, their address changes. If you overwrite the old address with the new one, you lose the historical data. However, if you need to analyze historical sales data, it's crucial to know where the customer lived at the time of each sale. This is where SCDs become essential. There are several types of slowly changing dimensions in a data warehouse, with type 1 and type 2 being the most frequently used. Type 0 SCD: The dimension attributes never change. Type 1 SCD : Overwrites existing data, doesn't keep history. Type 2 SCD : Adds new records for changes, keeps full history for a given natural key. Type 3 SCD: History is added as a new column. Type 4 SCD : A new dimension is added. Type 5 SCD : When certain attributes of a large dimension change over time, but using type 2 isn't feasible due to the dimensions large size. Type 6 SCD : Combination of type 2 and type 3. In type 2 SCD, when a new version of the same element is brought to the data warehouse, the old version is considered expired and the new one becomes active. The following code shows a simple example on how to handle the business key in a type 2 SCD for the Dim_Products table using T-SQL. IF EXISTS (SELECT 1 FROM Dim_Products WHERE SourceKey = @ProductID AND IsActive = 'True')\nBEGIN\n    -- Existing product record\n    UPDATE Dim_Products\n    SET ValidTo = GETDATE(), IsActive = 'False'\n    WHERE SourceKey = @ProductID \n        AND IsActive = 'True';\nEND\nELSE\nBEGIN\n    -- New product record\n    INSERT INTO Dim_Products (SourceKey, ProductName, StartDate, EndDate, IsActive)\n    VALUES (@ProductID, @ProductName, GETDATE(), '9999-12-31', 'True');\nEND The mechanism for detecting changes in source systems is crucial for determining when records are inserted, updated, or deleted. Change Data Capture (CDC) , change tracking , and triggers are all features available for managing data tracking in source systems such as SQL Server. Load a fact table Typically, a standard data warehouse load operation involves loading fact tables after dimension tables. This ensures that the dimensions, which the facts reference, are already present in the data warehouse. The staged fact data usually includes business keys for the related dimensions, so your loading logic must look up the corresponding surrogate keys. When dealing with slowly changing dimensions in the data warehouse, it's crucial to identify the appropriate version of the dimension record to ensure the correct surrogate key is used. This matches the event recorded in the fact table with the state of the dimension at the time the fact occurred. In many cases, you can retrieve the latest \"current\" version of the dimension. However, sometimes you might need to find the correct dimension record based on DateTime columns that indicate the period of validity for each version of the dimension. The following example assumes that dimension records have incrementing surrogate keys and that the most recently added version of a specific dimension instance, which will have the highest key value, might be used. -- Lookup keys in dimension tables\nINSERT INTO dbo.FactSales\nSELECT  (SELECT MAX(DateKey)\n         FROM dbo.DimDate\n         WHERE FullDateAlternateKey = stg.OrderDate) AS OrderDateKey,\n        (SELECT MAX(CustomerKey)\n         FROM dbo.DimCustomer\n         WHERE CustomerAlternateKey = stg.CustNo) AS CustomerKey,\n        (SELECT MAX(ProductKey)\n         FROM dbo.DimProduct\n         WHERE ProductAlternateKey = stg.ProductID) AS ProductKey,\n        (SELECT MAX(StoreKey)\n         FROM dbo.DimStore\n         WHERE StoreAlternateKey = stg.StoreID) AS StoreKey,\n        OrderNumber,\n        OrderLineItem,\n        OrderQuantity,\n        UnitPrice,\n        Discount,\n        Tax,\n        SalesAmount\nFROM dbo.StageSales AS stg",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-factory/tutorial-incremental-copy-data-warehouse-lakehouse?azure-portal=true",
            "https://learn.microsoft.com/wwl-data-ai/load-data-into-microsoft-fabric-data-warehouse/media/2-slowly-changing-dimension.png#lightbox",
            "https://learn.microsoft.com/en-us/sql/relational-databases/track-changes/about-change-data-capture-sql-server?azure-portal=true",
            "https://learn.microsoft.com/en-us/sql/relational-databases/track-changes/about-change-tracking-sql-server?azure-portal=true",
            "https://learn.microsoft.com/en-us/sql/relational-databases/triggers/dml-triggers?azure-portal=true"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/load-data-into-microsoft-fabric-data-warehouse/media/1-data-warehouse-process.png",
                "image_alt": "Diagram of sequential steps in the data science process."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/load-data-into-microsoft-fabric-data-warehouse/media/2-slowly-changing-dimension.png",
                "image_alt": "Diagram showing the function and structure of OneLake."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Load data into a Microsoft Fabric data warehouse",
        "topic": "Use data pipelines to load a warehouse",
        "text": "Microsoft Fabrics Warehouse provides integrated data ingestion tools, enabling users to load and ingest data into warehouses on a large scale through either coding or noncoding experiences. Data pipeline is the cloud-based service for data integration, which enables the creation of workflows for data movement and data transformation at scale. You can create and schedule data pipelines that can ingest and load data from disparate data stores. You can build complex ETL, or ELT processes that transform data visually with data flows. Most of the functionality of data pipelines in Microsoft Fabric comes from Azure Data Factory, allowing for seamless integration and utilization of its features within the Microsoft Fabric ecosystem. Note All data in a Warehouse is automatically stored in the Delta Parquet format in OneLake. Create a data pipeline There are a few ways to launch the data pipeline editor. From the workspace: Select + New , then select Data pipeline . If it's not visible in the list, select More options , then find Data pipeline under the Get data section. From the warehouse asset - Select Get Data , and then New data pipeline . There are three options available when creating a pipeline. Option Description 1. Add pipeline activity Launches the pipeline editor where you can create your own pipeline. 2. Copy data Launches an assistant to copy data from various data sources to a data destination. A new pipeline activity is generated at the end with a preconfigured Copy Data task. 3. Choose a task to start You can choose from a collection of predefined templates to assist you in initiating pipelines based on many scenarios. Configure the copy data assistant The copy data assistant provides a step-by-step interface that facilitates the configuration of a Copy Data task. Choose data source: Select a connector, and provide the connection information. Connect to a data source: Select, preview, and choose the data. This can be done from tables or views, or you can customize your selection by providing your own query. Choose data destination: Select the data store as the destination. Connect to data destination: Select and map columns from source to destination. You can load to a new or existing table. Settings: Configure other settings like staging, and default values. After you copy the data, you can use other tasks to further transform and analyze it. You can also use the Copy Data task to publish transformation and analysis results for business intelligence (BI) and application consumption. Schedule a data pipeline You can schedule your data pipeline by selecting Schedule from the data pipeline editor. You can also configure the schedule by selecting Settings in the Home menu in the data pipeline editor. We recommend data pipelines for a code-free or low-code experience due to the graphical user interface. They're ideal for data workflows that run at a schedule, or that connects to different data sources. To learn more about data pipelines, see Ingest data into your Warehouse using data pipelines .",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/ingest-data-pipelines?azure-portal=true"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/load-data-into-microsoft-fabric-data-warehouse/media/3-create-data-pipeline.png",
                "image_alt": "Screenshot showing the shortcuts for a few features in the Warehouse asset."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/load-data-into-microsoft-fabric-data-warehouse/media/3-build-pipeline.png",
                "image_alt": "Screenshot showing the options available when creating a pipeline."
            },
            {
                "image_name": "image3",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/load-data-into-microsoft-fabric-data-warehouse/media/3-copy-data-assistant.png",
                "image_alt": "Screenshot showing the copy data assistant."
            },
            {
                "image_name": "image4",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/load-data-into-microsoft-fabric-data-warehouse/media/3-schedule-data-pipeline.png",
                "image_alt": "Screenshot showing where to schedule a data pipeline from the pipeline designer."
            },
            {
                "image_name": "image5",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/load-data-into-microsoft-fabric-data-warehouse/media/3-schedule-configuration.png",
                "image_alt": "Screenshot showing the configuration properties when you schedule a data pipeline."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Load data into a Microsoft Fabric data warehouse",
        "topic": "Load data using T-SQL",
        "text": "SQL developers or citizen developers, who are often well-versed in the SQL engine and adept at using T-SQL, will find the Warehouse in Microsoft Fabric favorable. This is because the Warehouse is powered by the same SQL engine they're familiar with, enabling them to perform complex queries and data manipulations. These operations include filtering, sorting, aggregating, and joining data from different tables. The SQL engines wide range of functions and operators further allows for sophisticated data analysis and transformations at the database level. Use COPY statement The COPY statement serves as the main method for importing data into the Warehouse. It facilitates efficient data ingestion from an external Azure storage account. It offers flexibility, allowing you to specify the format of the source file, designate a location for storing rows that are rejected during the import process, skip header rows, among other configurable options. The option to store rejected rows separately is useful for data cleaning and quality control. It allows you to easily identify and investigate any issues with the data that weren't successfully imported. To connect to an Azure storage account, you need to use either Shared Access Signature (SAS) or Storage Account Key (SAK). Handle error The option to use a different storage account for the ERRORFILE location ( REJECTED_ROW_LOCATION ) allows for better error handling and debugging. It makes it easier to isolate and investigate any issues that occur during the data loading process. ERRORFILE only applies to CSV. Load multiple files The ability to specify wildcards and multiple files in the storage location path allows the COPY statement to handle bulk data loading efficiently. This is useful when dealing with large datasets distributed across multiple files. Multiple file locations can only be specified from the same storage account and container via a comma-separated list. COPY INTO my_table\nFROM 'https://myaccount.blob.core.windows.net/myblobcontainer/folder0/*.csv, \n    https://myaccount.blob.core.windows.net/myblobcontainer/folder1/'\nWITH (\n    FILE_TYPE = 'CSV',\n    CREDENTIAL=(IDENTITY= 'Shared Access Signature', SECRET='<Your_SAS_Token>')\n    FIELDTERMINATOR = '|'\n) The following example shows how to load a PARQUET file. COPY INTO test_parquet\nFROM 'https://myaccount.blob.core.windows.net/myblobcontainer/folder1/*.parquet'\nWITH (\n    CREDENTIAL=(IDENTITY= 'Shared Access Signature', SECRET='<Your_SAS_Token>')\n) Ensure that all the files have the same structure (that is, same columns in the same order) and that this structure matches the structure of the target table. Load table from other warehouses and lakehouses You can load data from various data assets in a workspace, such as other warehouses and lakehouses. To reference the data asset, ensure that you use three-part naming to combine data from tables on these workspace assets. You can then use CREATE TABLE AS SELECT (CTAS) and INSERT...SELECT to load the data into the warehouse. SQL Statement Description CREATE TABLE AS SELECT Allows you to create a new table based on the output of a SELECT statement. This operation is often used for creating a copy of a table or for transforming and loading the results of complex queries. INSERT...SELECT Allows you to insert data from one table into another. Its useful when you want to copy data from one table to another without creating a new table. In a scenario where an analyst needs data from both a warehouse and a lakehouse, they can use this feature to combine the data. They can then load this combined data into the warehouse for analysis. This feature is useful when data is distributed across many assets in a workspace. The following query creates a new table in the analysis_warehouse that combines data from the sales_warehouse and the social_lakehouse using the product_id as the common key. The new table can then be used for further analysis. CREATE TABLE [analysis_warehouse].[dbo].[combined_data]\nAS\nSELECT *\nFROM [sales_warehouse].[dbo].[sales_data] sales\nINNER JOIN [social_lakehouse].[dbo].[social_data] social\nON sales.[product_id] = social.[product_id]; All the Warehouses that share the same workspace are integrated into the same logical SQL server. If you use SQL client tools such as SQL Server Management Studio , you can easily perform a cross-database query like in any SQL Server instance. MyWarehouse and Sales are both warehouse assets that share the same workspace. If youre using the object Explorer from the workspace to query your Warehouses, you need to add them explicitly. The warehouses added will also be visible from the Visual query editor. Data can be efficiently loaded into a warehouse in Microsoft Fabric through the COPY statement, or from other warehouses and lakehouses within the same workspace, allowing for seamless data management and analysis.",
        "links": [
            "https://learn.microsoft.com/en-us/sql/t-sql/statements/copy-into-transact-sql?azure-portal=true",
            "https://learn.microsoft.com/en-us/sql/t-sql/language-elements/transact-sql-syntax-conventions-transact-sql?azure-portal=true",
            "https://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-as-select-azure-sql-data-warehouse?azure-portal=true",
            "https://learn.microsoft.com/en-us/sql/t-sql/statements/insert-transact-sql?azure-portal=true",
            "https://learn.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?azure-portal=true"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/load-data-into-microsoft-fabric-data-warehouse/media/4-load-using-ssms.gif",
                "image_alt": "Animated GIF showing how to reference other Warehouses in a workspace from SQL Server Management Studio."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/load-data-into-microsoft-fabric-data-warehouse/media/4-query-using-workspace.gif",
                "image_alt": "Animated GIF showing how to query other Warehouses in a workspace from the Fabric workspace."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Load data into a Microsoft Fabric data warehouse",
        "topic": "Load and transform data with Dataflow Gen2",
        "text": "Dataflow Gen2 is the new generation of dataflows. It provides a comprehensive Power Query experience, guiding you through each step of importing data into your dataflow. The process of creating dataflows has been simplified, reducing the number of steps involved. You can use dataflows in data pipelines to ingest data into a lakehouse or warehouse, or to define a dataset for a Power BI report. Create a dataflow To create a new dataflow, navigate to your workspace, then select + New . If Dataflow Gen2 isn't visible in the list, select More options , then find Dataflow Gen2 under the Data Factory section. Import data Once the Dataflow Gen2 launches, there are many options to load your data available. You can load different file types with just a few steps. For example, to load a text or CSV file from your local computer. Once the data is imported you can start authoring your dataflow, you might decide to clean your data, reshape, remove columns, and create new ones. All the steps you perform are saved. Transform data with Copilot Copilot can be a valuable tool for assisting with dataflow transformations. Let's say we have a Gender column that contains 'Male' and 'Female' and we want to transform it. The first step is to activate Copilot within your dataflow. Once that's done, you can then provide specific instructions on the transformation you want to perform. For instance, you might input the following command: \"Transform the Gender column. If Male 0, if Female 1. Then convert it to integer.\" Copilot adds a new step automatically, and you can always revert it if you want, or continue to build on it for further transformations. Add a data destination With the Add data destination feature, you can separate your ETL logic and destination storage. This separation can lead to cleaner, more maintainable code and can make it easier to modify either the ETL process or the storage configuration without affecting the other. Once the data is transformed, the next step is to add a destination step. On the Query settings tab, select + to add a destination step in your dataflow. The following destination options are available. Azure SQL Database Lakehouse Azure Data Explorer (Kusto) Azure Synapse Analytics (SQL DW) Warehouse Data thats loaded into a destination like a warehouse can be easily accessed and analyzed using various tools. This improves the accessibility of your data and allows for more flexible and comprehensive data analysis. When you select a warehouse as a destination, you can choose the following update methods. Append: Add new rows to an existing table. Replace: Replace the entire content of a table with a new set of data. Publish a dataflow After you choose your update method, the final step is to publish your dataflow. Publishing makes your transformations and data loading operations live, allowing the dataflow to be executed either manually or on a schedule. This process encapsulates your ETL operations into a single and reusable unit, streamlining your data management workflow. Any changes made in the dataflow take effect when its published. So, always ensure to publish your dataflow after making any relevant modifications.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-factory/dataflows-gen2-overview?azure-portal=true"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/load-data-into-microsoft-fabric-data-warehouse/media/5-load-using-dataflow.gif",
                "image_alt": "Animated GIF showing how to launch Dataflow Gen2 from the workspace."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/load-data-into-microsoft-fabric-data-warehouse/media/5-import-options.png",
                "image_alt": "Screenshot showing how to launch Data Pipeline from the Warehouse asset."
            },
            {
                "image_name": "image3",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/load-data-into-microsoft-fabric-data-warehouse/media/5-load-file.png",
                "image_alt": "Screenshot showing how to load a text or CSV file."
            },
            {
                "image_name": "image4",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/load-data-into-microsoft-fabric-data-warehouse/media/5-copilot.png",
                "image_alt": "Screenshot showing how to use Copilot to apply transformation in a dataflow."
            },
            {
                "image_name": "image5",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/load-data-into-microsoft-fabric-data-warehouse/media/5-add-destination.png",
                "image_alt": "Screenshot showing the option to add a data destination in a dataflow."
            },
            {
                "image_name": "image6",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/load-data-into-microsoft-fabric-data-warehouse/media/5-update-table-options.png",
                "image_alt": "Diagram showing visually the difference between the append and replace methods to update a row."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Load data into a Microsoft Fabric data warehouse",
        "topic": "Exercise: Load data into a warehouse in Microsoft Fabric",
        "text": "Now it's your chance to load data into a warehouse in Microsoft Fabric. In this exercise, youll learn how to load data into a warehouse using T-SQL. Note You need a Microsoft Fabric trial license with the Fabric preview enabled in your tenant. See Getting started with Fabric to enable your Fabric trial license. Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/06a-data-warehouse-load.html?azure-portal=true"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Load data into a Microsoft Fabric data warehouse",
        "topic": "Summary",
        "text": "Theres no one-size-fits-all solution for loading your data. The best approach depends on the specifics of your business requirement and the question youre trying to answer. When it comes to loading data in a data warehouse, there are several considerations to keep in mind. Description Load volume & frequency Assess data volume and load frequency to optimize performance. Governance Any data that lands in OneLake is governed by default. Data mapping Manage mapping from source to staging to warehouse. Dependencies Understand dependencies in the data model for loading dimensions. Script design Design efficient import scripts considering column names, filtering rules, value mapping, and database indexing. For additional reading, you can refer to the following URLs: Create a Warehouse in Microsoft Fabric Ingest data into the Warehouse Compare the Warehouse and the SQL analytics endpoint of the Lakehouse",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/create-warehouse?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/ingest-data?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/data-warehousing#compare-the-warehouse-and-the-sql-endpoint-of-the-lakehouse?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Query a data warehouse in Microsoft Fabric",
        "topic": "Introduction",
        "text": "Microsoft Fabric Data Warehouse is a complete platform for data, analytics, and AI (Artificial Intelligence). It refers to the process of storing, organizing, and managing large volumes of structured and semi-structured data. Data warehouse in Microsoft Fabric is powered up with Synapse Analytics by offering a rich set of features that make it easier to manage and analyze data. It includes advanced query processing capabilities, and supports the full transactional T-SQL capabilities like an enterprise data warehouse. The process of querying a data warehouse is a key component of business intelligence. It involves the extraction and manipulation of the data stored in a data warehouse, allowing users to extract valuable insights from large volumes of data. Star schema design In a typical data warehouse, the data is organized using a schema, often a star schema or a snowflake schema . The star schema and snowflake schema are mature modeling approaches widely adopted by relational data warehouses. It requires you to classify tables as either dimension or fact. Fact tables store the measurable, quantitative data about a business, while dimension tables contain descriptive attributes related to fact data. Think of a dimension table as the \"who, what, where, when, why of your data warehouse. Its like the descriptive backdrop that gives context to the raw numbers found in the fact tables. For example, if youre running an online store, your fact table might contain the raw sales data - how many units of each product were sold. But without a dimension table, you wouldnt know who bought those products, when they were bought, or where the buyer is located. We'll explore different ways to connect and query a data warehouse, and how they can facilitate the tasks of effectively extracting information. For more information, see Understand star schema and the importance for Power BI .",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/?azure-portal=true",
            "https://learn.microsoft.com/en-us/power-bi/guidance/star-schema?azure-portal=true",
            "https://learn.microsoft.com/en-us/power-bi/guidance/star-schema#snowflake-dimensions?azure-portal=true",
            "https://learn.microsoft.com/en-us/power-bi/guidance/star-schema?azure-portal=true"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/query-data-warehouse-microsoft-fabric/media/1-star-schema.png",
                "image_alt": "Diagram of a star schema design with a fact table in the center and dimension tables forming the points of the star."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Query a data warehouse in Microsoft Fabric",
        "topic": "Query data",
        "text": "Once the dimension and fact tables in a data warehouse are populated with data, you can use T-SQL to query these tables and perform data analysis. The Transact-SQL (T-SQL) syntax used for querying tables in a warehouse in Fabric closely resembles the SQL syntax used in SQL Server or Azure SQL Database. This familiarity allows for an easy transition for those already used to working with these platforms. Aggregate measures by dimension attributes In most data analytics scenarios involving a data warehouse, the process typically revolves around aggregating numeric measures from fact tables based on attributes in dimension tables. Due to the structure of a star or snowflake schema, such aggregation queries depend on JOIN clauses to link fact tables with dimension tables. Also, they use aggregate functions and GROUP BY clauses to define the aggregation hierarchies. The following SQL query aggregates sales amounts by year and quarter from the FactSales and DimDate tables in a hypothetical data warehouse: SELECT  dates.CalendarYear,\n        dates.CalendarQuarter,\n        SUM(sales.SalesAmount) AS TotalSales\nFROM dbo.FactSales AS sales\nJOIN dbo.DimDate AS dates ON sales.OrderDateKey = dates.DateKey\nGROUP BY dates.CalendarYear, dates.CalendarQuarter\nORDER BY dates.CalendarYear, dates.CalendarQuarter; The results from this query would look similar to the following table. CalendarYear CalendarQuarter TotalSales 2020 1 25980.16 2020 2 27453.87 2020 3 28527.15 2020 4 31083.45 2021 1 34562.96 2021 2 36162.27 ... ... ... You can join as many dimension tables as needed to calculate the aggregations you need. For example, the following code extends the previous example to break down the quarterly sales totals by city based on the customer's address details in the DimCustomer table. SELECT  dates.CalendarYear,\n        dates.CalendarQuarter,\n        custs.City,\n        SUM(sales.SalesAmount) AS TotalSales\nFROM dbo.FactSales AS sales\nJOIN dbo.DimDate AS dates ON sales.OrderDateKey = dates.DateKey\nJOIN dbo.DimCustomer AS custs ON sales.CustomerKey = custs.CustomerKey\nGROUP BY dates.CalendarYear, dates.CalendarQuarter, custs.City\nORDER BY dates.CalendarYear, dates.CalendarQuarter, custs.City; This time, the results include a quarterly sales total for each city. CalendarYear CalendarQuarter City TotalSales 2020 1 Amsterdam 5982.53 2020 1 Berlin 2826.98 2020 1 Chicago 5372.72 ... ... ... .. 2020 2 Amsterdam 7163.93 2020 2 Berlin 8191.12 2020 2 Chicago 2428.72 ... ... ... .. 2020 3 Amsterdam 7261.92 2020 3 Berlin 4202.65 2020 3 Chicago 2287.87 ... ... ... .. 2020 4 Amsterdam 8262.73 2020 4 Berlin 5373.61 2020 4 Chicago 7726.23 ... ... ... .. 2021 1 Amsterdam 7261.28 2021 1 Berlin 3648.28 2021 1 Chicago 1027.27 ... ... ... .. Joins in a snowflake schema When using a snowflake schema, dimensions may be partially normalized; requiring multiple joins to relate fact tables to snowflake dimensions. For example, suppose your data warehouse includes a DimProduct dimension table from which the product categories have been normalized into a separate DimCategory table. A query to aggregate items sold by product category might look similar to the following example: SELECT  cat.ProductCategory,\n        SUM(sales.OrderQuantity) AS ItemsSold\nFROM dbo.FactSales AS sales\nJOIN dbo.DimProduct AS prod ON sales.ProductKey = prod.ProductKey\nJOIN dbo.DimCategory AS cat ON prod.CategoryKey = cat.CategoryKey\nGROUP BY cat.ProductCategory\nORDER BY cat.ProductCategory; The results from this query include the number of items sold for each product category: ProductCategory ItemsSold Accessories 28271 Bits and pieces 5368 ... ... Note JOIN clauses for FactSales and DimProduct and for DimProduct and DimCategory are both required, even though no fields from DimProduct are returned by the query. Using ranking functions Another common kind of analytical query is to partition the results based on a dimension attribute and rank the results within each partition. For example, you might want to rank stores each year by their sales revenue. To accomplish this goal, you can use Transact-SQL ranking functions such as ROW_NUMBER , RANK , DENSE_RANK , and NTILE . These functions enable you to partition the data over categories, each returning a specific value that indicates the relative position of each row within the partition: ROW_NUMBER returns the ordinal position of the row within the partition. For example, the first row is numbered 1, the second 2, and so on. RANK returns the ranked position of each row in the ordered results. For example, in a partition of stores ordered by sales volume, the store with the highest sales volume is ranked 1. If multiple stores have the same sales volumes, they'll be ranked the same, and the rank assigned to subsequent stores reflects the number of stores that have higher sales volumes - including ties. DENSE_RANK ranks rows in a partition the same way as RANK , but when multiple rows have the same rank, subsequent rows are ranking positions ignore ties. NTILE returns the specified percentile in which the row falls. For example, in a partition of stores ordered by sales volume, NTILE(4) returns the quartile in which a store's sales volume places it. For example, consider the following query: SELECT  ProductCategory,\n        ProductName,\n        ListPrice,\n        ROW_NUMBER() OVER\n            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS RowNumber,\n        RANK() OVER\n            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS Rank,\n        DENSE_RANK() OVER\n            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS DenseRank,\n        NTILE(4) OVER\n            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS Quartile\nFROM dbo.DimProduct\nORDER BY ProductCategory; The query partitions products into groupings based on their categories, and within each category partition, the relative position of each product is determined based on its list price. The results from this query might look similar to the following table: ProductCategory ProductName ListPrice RowNumber Rank DenseRank Quartile Accessories Widget 8.99 1 1 1 1 Accessories Knicknak 8.49 2 2 2 1 Accessories Sprocket 5.99 3 3 3 2 Accessories Doodah 5.99 4 3 3 2 Accessories Spangle 2.99 5 5 4 3 Accessories Badabing 0.25 6 6 5 4 Bits and pieces Flimflam 7.49 1 1 1 1 Bits and pieces Snickity wotsit 6.99 2 2 2 1 Bits and pieces Flange 4.25 3 3 3 2 ... ... ... ... ... ... ... Note The sample results demonstrate the difference between RANK and DENSE_RANK . Note that in the Accessories category, the Sprocket and Doodah products have the same list price; and are both ranked as the 3rd highest priced product. The next highest priced product has a RANK of 5 (there are four products more expensive than it) and a DENSE_RANK of 4 (there are three higher prices). To learn more about ranking functions, see the Use built-in functions and GROUP BY in Transact-SQL module. Retrieving an approximate count While the purpose of a data warehouse is primarily to support analytical data models and reports for the enterprise; data analysts and data scientists often need to perform some initial data exploration, just to determine the basic scale and distribution of the data. For example, the following query uses the COUNT function to retrieve the number of sales for each year in a hypothetical data warehouse: SELECT dates.CalendarYear AS CalendarYear,\n    COUNT(DISTINCT sales.OrderNumber) AS Orders\nFROM FactSales AS sales\nJOIN DimDate AS dates ON sales.OrderDateKey = dates.DateKey\nGROUP BY dates.CalendarYear\nORDER BY CalendarYear; The results of this query might look similar to the following table: CalendarYear Orders 2019 239870 2020 284741 2021 309272 ... ... The volume of data in a data warehouse can mean that even simple queries to count the number of records that meet specified criteria can take a considerable time to run. In many cases, a precise count isn't required - an approximate estimate will suffice. In such cases, you can use the APPROX_COUNT_DISTINCT function as shown in the following example: SELECT dates.CalendarYear AS CalendarYear,\n    APPROX_COUNT_DISTINCT(sales.OrderNumber) AS ApproxOrders\nFROM FactSales AS sales\nJOIN DimDate AS dates ON sales.OrderDateKey = dates.DateKey\nGROUP BY dates.CalendarYear\nORDER BY CalendarYear; The APPROX_COUNT_DISTINCT function uses a HyperLogLog algorithm to retrieve an approximate count. The result is guaranteed to have a maximum error rate of 2% with 97% probability, so the results of this query with the same hypothetical data as before might look similar to the following table: CalendarYear ApproxOrders 2019 235552 2020 290436 2021 304633 ... ... The counts are less accurate, but still sufficient for an approximate comparison of yearly sales. With a large volume of data, the query using the APPROX_COUNT_DISTINCT function completes more quickly, and the reduced accuracy may be an acceptable trade-off during basic data exploration. Note See the APPROX_COUNT_DISTINCT function documentation for more details.",
        "links": [
            "https://learn.microsoft.com/en-us/sql/t-sql/functions/ranking-functions-transact-sql",
            "https://learn.microsoft.com/en-us/sql/t-sql/functions/approx-count-distinct-transact-sql"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Query a data warehouse in Microsoft Fabric",
        "topic": "Use the SQL query editor",
        "text": "The SQL query editor in Microsoft Fabric is a versatile tool that supports Transact-SQL (T-SQL), allowing you to create and run scripts to query your data warehouse. It also provides features such as IntelliSense and debugging to aid in the development process. T-SQL allows users, analysts and developers to manipulate the data stored in a warehouse. Launch a SQL query editor The SQL query editor isn't just about querying; its about managing your data effectively. Whether youre creating a new table, inserting rows into a table, granting objects permission or executing complex queries, the SQL query editor is designed to make these tasks seamless. To launch the SQL query editor, you need to select your warehouse asset from My workspace . This step connects to your data warehouse automatically, without you having to prompt any connection information. From the warehouse Explorer , there are a few ways to create a SQL query editor. Home menu: Select New SQL query or select any of the templates available. Queries node: Select ... in My queries , then select New SQL Query . Query tab: This option opens a new query editor. No matter how you initiate a new query editor, a new query item is automatically created in My queries folder in the Explorer . Any new query item updated is automatically saved. Run queries and view results To run a query, type it into a new query editor and select Run at the top of the editor. The Results section shows a preview, limited to 10,000 rows if exceeded. Export results Your query results can be exported as an Excel file. To export it, select Download Excel file . You need to select the text of one SELECT statement in your query to export results to Excel. Similarly, the query editor offers the ability to save your highlighted query as either a view or a table, each with distinct features: Save as view : Allows you to create a view in the warehouse using the SELECT statement highlighted in the query editor. Save as table : Creates a new table with your query results. For both options, you must provide the schema and the name before confirming the creation. To learn more about SQL query editor, see Query using the SQL query editor .",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/sql-query-editor?azure-portal=true"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/query-data-warehouse-microsoft-fabric/media/2-sql-editor.png",
                "image_alt": "Screenshot showing how to invoke a new SQL editor for a data warehouse in Microsoft Fabric."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/query-data-warehouse-microsoft-fabric/media/2-query-editor.gif",
                "image_alt": "Animated GIF showing how to run a query for a data warehouse in Microsoft Fabric."
            },
            {
                "image_name": "image3",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/query-data-warehouse-microsoft-fabric/media/2-export-query.gif",
                "image_alt": "Animated GIF showing how to export query results for a data warehouse in Microsoft Fabric."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Query a data warehouse in Microsoft Fabric",
        "topic": "Explore the visual query editor",
        "text": "The visual query editor in Microsoft Fabric data warehouse is a tool that provides an intuitive interface for building SQL queries. It simplifies the process of writing and managing queries, especially for those who might not be comfortable with SQL syntax. Graphical interface : The editor provides a graphical interface where you can drag and drop tables, and visually design your queries. This makes it easier to understand the structure of your query and the relationships between tables. Automatic query generation : As you design your query using the visual interface, the corresponding SQL query is automatically generated. This allows you to focus on the logic of your query, rather than the syntax. All workspace users can save their queries in My queries folder. Build your query visually The visual query editor enhances intuitive understanding of data relationships by allowing users to visually organize tables and fields. Also, the visual query editor is user-friendly, even for those with little to no SQL experience. Nontechnical team members can use it to extract valuable insights from your data, democratizing data access within your organization and speeding up the decision-making process. Similarly to the SQL query editor, the Save as table and Save as view options are also available. These features can be useful for reusing your queries or for creating more complex queries based on the results of previous queries. To learn more about SQL query editor, see Query using the visual query editor .",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/visual-query-editor?azure-portal=true"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/query-data-warehouse-microsoft-fabric/media/3-visual-editor.gif",
                "image_alt": "Animated GIF showing how to build a query using the visual query editor for a data warehouse in Microsoft Fabric."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Query a data warehouse in Microsoft Fabric",
        "topic": "Use client tools to query a warehouse",
        "text": "Using SQL Server Management Studio (SSMS) to connect to a data warehouse in Fabric can facilitate your workflow, especially if youre already familiar with the tool. Connect to your data warehouse SQL Server Management Studio provides a familiar interface for those who regularly work with SQL Server. Follow these steps to connect to data warehouse in Fabric from SSMS: Navigate to your Microsoft Fabric workspace. On your warehouse asset, select more options, then select Copy SQL connection string . Launch SQL Server Management Studio, and paste the SQL connection string copied into the Server name box, and provide the appropriate credentials for authentication. After establishing a connection, SSMS shows the connected warehouse, along with its corresponding tables and views, all ready for querying. Authentication options In Microsoft Fabric, two types of authenticated users are supported through the SQL connection string: Microsoft Entra ID (formerly Azure Active Directory) user principals, or user identities Microsoft Entra ID (formerly Azure Active Directory) service principals Note SQL authentication is not supported. Other tools Any third-party tool can use the SQL connection string via ODBC or OLE DB drivers to connect to a Microsoft Fabric Warehouse or SQL analytics endpoint, using Microsoft Entra ID (formerly Azure Active Directory) authentication.",
        "links": null,
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/query-data-warehouse-microsoft-fabric/media/4-connection-string.gif",
                "image_alt": "Animated GIF showing how to generate the connection string for a data warehouse in Microsoft Fabric."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/query-data-warehouse-microsoft-fabric/media/4-ssms.png",
                "image_alt": "Screenshot showing how to connect to a warehouse using SQL Server Management Studio."
            },
            {
                "image_name": "image3",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/query-data-warehouse-microsoft-fabric/media/4-ssms-warehouse.png",
                "image_alt": "Screenshot showing the connected warehouse in SQL Server Management Studio."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Query a data warehouse in Microsoft Fabric",
        "topic": "Exercise: Query a data warehouse in Microsoft Fabric",
        "text": "Now it's your chance to query data in a data warehouse in Microsoft Fabric. In this exercise, youll learn how to query data using the SQL editor in Microsoft Fabric. Note You need a Microsoft Fabric trial license with the Fabric preview enabled in your tenant. See Getting started with Fabric to enable your Fabric trial license. Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/06b-data-warehouse-query.html?azure-portal=true"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Query a data warehouse in Microsoft Fabric",
        "topic": "Summary",
        "text": "Learning how to query a data warehouse empowers individuals to extract, analyze, and interpret large volumes of stored data, transforming raw data into meaningful insights. These insights can drive strategic decision-making, optimize operations, and uncover hidden patterns and trends. Theres no one-size-fits-all solution for querying your data. The best approach depends on the specifics of your data warehouse and the question youre trying to answer. For additional reading, you can refer to the following URLs: Connectivity to data warehousing in Microsoft Fabric Query the SQL analytics endpoint or Warehouse in Microsoft Fabric View data in the Data preview in Microsoft Fabric",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/connectivity?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/query-warehouse?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/data-preview?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Monitor a Microsoft Fabric data warehouse",
        "topic": "Introduction",
        "text": "A data warehouse is often central to enterprise analysis and reporting, and as such is a critical business asset. It's important to monitor a data warehouse to track and manage costs, identify and resolve query performance issues, and to gain insights into how your data is being used. In this module, we'll explore some tools and techniques you can use to monitor your Microsoft Fabric data warehouse.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Monitor a Microsoft Fabric data warehouse",
        "topic": "Monitor capacity metrics",
        "text": "When your organization uses Microsoft Fabric, the license used to purchase the service determines the capacity available. A capacity is a pool of resources that you can use to implement Fabric capabilities. The cost of using Fabric is based on capacity units (CUs). Each action you perform in a Fabric resource can consume CUs, for which your organization is billed. It's therefore important to be able to monitor capacity usage to plan and manage costs. In data warehouse workloads, CUs are consumed by data read and write activities, so queries in your data warehouse and the underlying file operations to OneLake storage are a significant factor in the cost of your Fabric analytics solution. Using the Microsoft Fabric Capacity Metrics app The Microsoft Fabric Capacity Metrics app is an app that an administrator can install in a Fabric environment and use to monitor capacity utilization. To monitor capacity utilization related to data warehousing, you can filter the interface to show only warehouse activity, like this: By using the Fabric Capacity Metrics app, you can observe capacity utilization trends to determine what processes are consuming CUs in your Fabric environment and whether any throttling is occurring (which indicates that your processes require more capacity than is available within the constraints of your purchased capacity license). With this information, you can optimize your capacity license for your needs. Tip For more information about Microsoft Fabric Capacity Metrics app, refer to Billing and utilization reporting in Synapse Data Warehouse in the Microsoft Fabric documentation.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/usage-reporting"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/monitor-fabric-data-warehouse/media/fabric-capacity-metrics-app.gif",
                "image_alt": "Screenshot of the Fabric Capacity Metrics app showing warehouse activity."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Monitor a Microsoft Fabric data warehouse",
        "topic": "Monitor current activity",
        "text": "You can use dynamic management views (DMVs) to retrieve information about the current state of the data warehouse. Specifically, Microsoft Fabric data warehouses include the following DMVs: sys.dm_exec_connections : Returns information about data warehouse connections. sys.dm_exec_sessions : Returns information about authenticated sessions. sys.dm_exec_requests : Returns information about active requests. The schema of these tables is shown here: Querying DMVs You can retrieve detailed information about current activities in the data warehouse by querying the dm_exec-* DMVs. For example, consider the following query: SELECT sessions.session_id, sessions.login_name,\n    connections.client_net_address,\n    requests.command, requests.start_time, requests.total_elapsed_time\nFROM sys.dm_exec_connections AS connections\nINNER JOIN sys.dm_exec_sessions AS sessions\n    ON connections.session_id=sessions.session_id\nINNER JOIN sys.dm_exec_requests AS requests\n    ON requests.session_id = sessions.session_id\nWHERE requests.status = 'running'\n    AND requests.database_id = DB_ID()\nORDER BY requests.total_elapsed_time DESC This query returns details about the active requests in the current database, ordered by the duration for which they have been executing; which may be useful to identify long-running queries that could benefit from optimization. An example result set from the query is shown here: session_id login_name client_net_address command start_time total_elapsed_time 60 fred@contoso.com 10.23.139.162 SELECT 2023-12-07T14:56:41.3530000 57266 126 nandita@contoso.com 10.23.137.98 SELECT 2023-12-07T14:57:22.7800000 15840 137 zoe@contoso.com 10.23.119.171 SELECT 2023-12-07T14:57:38.6070000 4 Tip For more information about using DMVs, refer to Monitor connections, sessions, and requests using DMVs in the Microsoft Fabric documentation.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/monitor-using-dmv"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/monitor-fabric-data-warehouse/media/dynamic-management-views.png",
                "image_alt": "Diagram of dynamic management views."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Monitor a Microsoft Fabric data warehouse",
        "topic": "Monitor queries",
        "text": "Microsoft Fabric data warehouses include query insights feature that provides historical, aggregated information about the queries that have been run; enabling you to identify frequently used or long-running queries, and helping you analyze and tune query performance. Query insights are provided through the following views: queryinsights.exec_requests_history : Details of each completed SQL query. queryinsights.long_running_queries : Details of query execution time. queryinsights.frequently_run_queries : Details of frequently run queries. The schema for these tables is shown here: Retrieving query insights The query insights views are a useful source of information about the queries that are being run in your data warehouse. For example, consider the following query: SELECT start_time, login_name, command\nFROM queryinsights.exec_requests_history \nWHERE start_time >= DATEADD(MINUTE, -60, GETUTCDATE()) This query uses the queryinsights.exec_requests_history view to identify queries that were run in the previous hour. Note Depending on the concurrent workloads, queries may take up to 15 minutes to be reflected in the query insights views. You can get details of long-running queries from the queryinsights.long_running_queries view like this: SELECT last_run_command, number_of_runs, median_total_elapsed_time_ms, last_run_start_time\nFROM queryinsights.long_running_queries\nWHERE number_of_runs > 1\nORDER BY median_total_elapsed_time_ms DESC; This query identifies long-running SQL commands that have been used more than once and returns them in descending order of their median time to complete. Note To enable the views to provide aggregated metrics, queries with predicates are parameterized and considered the same query if the parameterized statements are an exact match. For example, the following queries would be considered to be the same command: SELECT * FROM sales WHERE orderdate > '01/01/2023' SELECT * FROM sales WHERE orderdate > '12/31/2021' To find commonly used queries, you can use the queryinsights.frequently_run_queries view, like this: SELECT last_run_command, number_of_runs, number_of_successful_runs, number_of_failed_runs\nFROM queryinsights.frequently_run_queries\nORDER BY number_of_runs DESC; This query returns details of successful and failed runs for frequently run commands. Tip For more information about using query insights refer to Query insights in Fabric data warehousing in the Microsoft Fabric documentation.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/query-insights"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/monitor-fabric-data-warehouse/media/query-insights.png",
                "image_alt": "Diagram of query insights views."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Monitor a Microsoft Fabric data warehouse",
        "topic": "Exercise - Monitor a data warehouse in Microsoft Fabric",
        "text": "Now it's time to try monitoring a Microsoft Fabric data warehouse for yourself. Note You need a Microsoft Fabric trial license with the Fabric preview enabled in your tenant. See Getting started with Fabric to enable your Fabric trial license.\nLaunch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://go.microsoft.com/fwlink/?linkid=2255540&azure-portal=true"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Monitor a Microsoft Fabric data warehouse",
        "topic": "Summary",
        "text": "Microsoft Fabric data warehouses are an important asset in any organization, and it's important to monitor them to track and manage costs, identify and resolve query performance issues, and to gain insights into how your data is being used. This module explored the following ways in which you can monitor a data warehouse: The Microsoft Fabric Capacity Metrics app Dynamic Management Views Query Insights views",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Secure a Microsoft Fabric data warehouse",
        "topic": "Introduction",
        "text": "Microsoft Fabric Data Warehouse is a complete platform for data, analytics, and AI (Artificial Intelligence). It refers to the process of storing, organizing, and managing large volumes of structured and semi-structured data. In a warehouse, administrators have access to a suite of technologies aimed at safeguarding sensitive information. These security measures are capable of securing or masking data from users or roles without proper authorization, ensuring data protection across both Warehouse and SQL analytics endpoints. This ensures a smooth and secure user experience, with no need for alterations to the existing applications. Understand security features Data engineers who are often well-versed in the SQL engine and adept at using T-SQL, will find warehouses in Microsoft Fabric easy to use. This is because warehouses are powered by the same SQL engine they're familiar with, enabling them to perform complex queries and data manipulations. The SQL engines wide range of security features further allows for sophisticated security mechanism at the warehouse level. Workspaces roles  Designed to provide different levels of access and control within the workspace. You can assign users to the various workspace roles such as Admin , Member , Contributor , and Viewer . These roles are crucial for maintaining the security and efficiency of data warehousing operations within an organization. Item permissions  Individual warehouses can have item permissions assigned directly to them. The main intent behind assigning such permissions is to facilitate the sharing of the Warehouse for downstream use. Data protection security  For more precise control, you can use T-SQL to grant specific permissions to users. Warehouse supports a range of data protection features that enable administrators to shield sensitive data from unauthorized access. This includes object-level security for database objects, column-level security for table columns, row-level security for table rows using WHERE clause filters, and dynamic data masking to obscure sensitive data like email addresses. These features ensure data protection across Warehouses and SQL analytics endpoints without necessitating changes to applications. In the next units, we'll explore various ways of enabling security in a warehouse, and how they can facilitate the tasks of keeping your data warehouse workload protected.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/workspace-roles/?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/share-warehouse-manage-permissions/?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/security#granular-security?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Secure a Microsoft Fabric data warehouse",
        "topic": "Explore dynamic data masking",
        "text": "Dynamic Data Masking (DDM) is a security feature that limits data exposure to nonprivileged users by obscuring sensitive information. Dynamic data masking offers several key benefits that enhance the security and manageability of your data. One of the primary advantages is its real-time masking feature. When querying sensitive data, DDM applies dynamic masking to it in real time. This process means that the actual data is never exposed to unauthorized users, thus enhancing the security of your data. Furthermore, DDM is straightforward to implement. It doesnt require complex coding, making it accessible for users of all skill levels. Another benefit of DDM is that the data in the database isnt changed when DDM is applied. Instead, the masking rules are applied to the query results. This benefit means that the actual data remains intact and secure, while nonprivileged users only see a masked version of the data. Define masking rule Dynamic data masking, which is set up at the column level, offers a suite of features including comprehensive and partial masking capabilities, along with a random masking function designed for numeric data. Masking Type Description Use Case Limitations Masking Rule Default Full masking according to the data types of the designated fields. Useful when you want to completely hide the actual data. Completely mask the data. No information is visible. default() Email Exposes the first letter of an email address and the constant suffix \".com\" Useful when you want to show that the data field contains an email without revealing the actual email. Only applicable to email fields. email() Custom Text Exposes the first and last 'n' characters and adds a custom padding string in the middle. Useful when you want to partially hide the actual data. Not suitable for numeric, date, and time data types. partial(prefix_padding, padding_string, suffix_padding) Random Replaces any numeric or binary value with a random number within a specified range. Useful when you want to hide the actual numeric or binary data. Only applicable to numeric and binary data types. random(low, high) The prefix_padding and suffix_padding parameters in the partial() function specify the number of characters to expose at the beginning and end of the string, and the padding_string parameter specifies the string to use for masking the remaining characters. The low and high parameters in the random() function specify the range of random numbers to generate. These masking types help prevent unauthorized viewing of sensitive data by enabling administrators to specify how much sensitive data to reveal, with minimal effect on the application layer. They're applied to query results, so the data in the database isn't changed. This approach allows many applications to mask sensitive data without modifying existing queries. Configure data masking Let's consider an example of a warehouse that stores customer information. The warehouse contains a Customer table with fields such as CustomerName , Email , PhoneNumber , and CreditCardNumber . To apply data masking on the CustomerName , Email , PhoneNumber , and CreditCardNumber columns, run the following command: -- For Email\nALTER TABLE Customers\nALTER COLUMN Email ADD MASKED WITH (FUNCTION = 'email()');\n\n-- For PhoneNumber\nALTER TABLE Customers\nALTER COLUMN PhoneNumber ADD MASKED WITH (FUNCTION = 'partial(0,\"XXX-XXX-\",4)');\n\n-- For CreditCardNumber\nALTER TABLE Customers\nALTER COLUMN CreditCardNumber ADD MASKED WITH (FUNCTION = 'partial(0,\"XXXX-XXXX-XXXX-\",4)'); View masked results Without Dynamic Data Masking, if a nonprivileged user runs a query to fetch customer details, they might see something like this: CustomerName: John Doe\nEmail: johndoe@contoso.com\nPhoneNumber: 123-456-7890\nCreditCardNumber: 1234-5678-9012-3456 However, with DDM applied to the Email , PhoneNumber , and CreditCardNumber fields, the same query would return: CustomerName: John Doe\nEmail: j*****@contoso.com\nPhoneNumber: XXX-XXX-7890\nCreditCardNumber: XXXX-XXXX-XXXX-3456 As you can see, the sensitive data is hidden from the nonprivileged user, enhancing the security of your data. This scenario is a basic example of how Dynamic Data Masking works. It helps to ensure that sensitive data isn't exposed to users who don't have the necessary privileges to view it. Note Unprivileged users with query permissions can infer the actual data since the data isnt physically obfuscated. DDM should be used as part of a comprehensive data security strategy that includes proper management of object-level security with SQL granular permissions and adherence to the principle of minimal required permissions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/dynamic-data-masking?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Secure a Microsoft Fabric data warehouse",
        "topic": "Implement row-level security",
        "text": "Row-Level Security (RLS) is a feature that provides granular control over access to rows in a table based on group membership or execution context. For example, in an e-commerce platform, you can ensure that sellers only have access to order rows that are related to their own products. This way, each seller can manage their orders independently, while maintaining the privacy of other sellers order information. If you have experience with SQL Server, you find that row-level security shares similar characteristics and features. Protect your data Row-Level Security (RLS) works by associating a function, known as a security predicate, with a table. This function is defined to return true or false based on certain conditions, typically involving the values of one or more columns in the table. When a user attempts to access data in the table, the security predicate function is invoked. If the function returns true , the row is accessible to the user; if it returns false , the row doesn't show up in the query results. Depending on the business requirements, an RLS predicate can be as simple as WHERE CustomerId = 29 or as complex as required. This process is transparent to the user and is enforced automatically by SQL Server, ensuring consistent application of security rules. Row-level security is implemented in two main steps: Filter predicates - It's an inline table-valued function that filters the results based on the predicate defined. Access Definition SELECT Can't view rows that are filtered. UPDATE Can't update rows that are filtered. DELETE Can't delete rows that are filtered. INSERT Not applicable. Security policy - It's a security policy that invokes an inline table-valued function to protect access to the rows in a table. Because access control is configured and applied at the warehouse level, application changes are minimal - if any. Also, users can directly have access to the tables and can query their own data. Configure row-level security The T-SQL commands below demonstrate how to use RLS in a scenario where user access is segregated by tenant: -- Create supporting objects for this example\nCREATE TABLE [Sales] (SalesID INT, \n    ProductID INT, \n    TenantName NVARCHAR(50), \n    OrderQty INT, \n    UnitPrice MONEY)\nGO\n\nINSERT INTO [Sales]  VALUES (1, 3, 'tenant1@contoso.com', 5, 10.00);\nINSERT INTO [Sales]  VALUES (2, 4, 'tenant2@contoso.com', 2, 57.00);\nINSERT INTO [Sales]  VALUES (3, 7, 'tenant3@contoso.com', 4, 23.00);\nINSERT INTO [Sales]  VALUES (4, 2, 'tenant4@contoso.com', 2, 91.00);\nINSERT INTO [Sales]  VALUES (5, 9, 'tenant5@contoso.com', 5, 80.00);\n\n-- View all the rows in the table  \nSELECT * FROM Sales; Next, we create a new schema, an inline table-valued function, and grant user access to the new function. The WHERE @TenantName = USER_NAME() OR USER_NAME() = 'TenantAdmin' predicate evaluates if the user name executing the query matches the TenantName column values. --Create a schema\nCREATE SCHEMA [Sec];  \nGO  \n\n--Create the filter predicate\nCREATE FUNCTION sec.tvf_SecurityPredicatebyTenant(@TenantName AS NVARCHAR(10))  \n    RETURNS TABLE  \nWITH SCHEMABINDING  \nAS  \n    RETURN\tSELECT 1 AS result\n\t\t\tWHERE @TenantName = USER_NAME() OR USER_NAME() = 'tenantAdmin@contoso.com';  \nGO\n\n--Create security policy and add the filter predicate\nCREATE SECURITY POLICY sec.SalesPolicy  \nADD FILTER PREDICATE sec.tvf_SecurityPredicatebyTenant(TenantName) ON [dbo].[Sales]\nWITH (STATE = ON);  \nGO The tenantAdmin@contoso.com user should see all the rows. The tenant1@contoso.com to tenant5@contoso.com users should only see their own rows. If you alter the security policy with WITH (STATE = OFF); , you notice that users see all the rows. Note There is a risk of information leakage if an attacker writes a query with a specially crafted WHERE clause and, for example, a divide-by-zero error, to force an exception if the WHERE condition is true. This is known as a side-channel attack . Explore use cases Row-level security is ideal for many scenarios, including: When you need to isolate departmental access at the row level. When you need to restrict customers' data access to only the data relevant to their company. When you need to restrict access for compliance purposes. Apply best practices Here are a few best practices to consider when implementing RLS: It's recommended to create a separate schema for predicate functions, and security policies. Whenever possible, avoid type conversions in predicate functions. To maximize performance, avoid using excessive table joins and recursion in predicate functions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/row-level-security?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Secure a Microsoft Fabric data warehouse",
        "topic": "Implement column-level security",
        "text": "Column-level security (CLS) allows you to restrict column access in order to protect sensitive data. It provides granular control over who can access specific pieces of data, enhancing the overall security of your data warehouse. Secure sensitive data Let's consider a practical example of column-level security (CLS) in the healthcare industry. Suppose we have a table named Patients with the following columns: PatientID , Name , Address , DateOfBirth , and MedicalHistory . The MedicalHistory column contains sensitive health information about the patients. As per the healthcare regulations and privacy laws, this information should only be accessible to authorized medical personnel such as doctors or nurses. Here's how you might implement column-level security in this scenario: Identify the sensitive columns : In this case, the MedicalHistory column is identified as containing sensitive data. Define access roles : Define roles such as Doctor and Nurse who are allowed to access the MedicalHistory column. Other roles such as Receptionist or Patient might be restricted from accessing this column. Assign roles to users : Assign the appropriate roles to each user in the warehouse. For example, user DrSmith might be assigned the Doctor role, while user JohnDoe might be assigned the Patient role. Implement access control : Restrict access to the MedicalHistory column based on the user's role. Column-level security can help ensure that sensitive health information is only accessible to those who are authorized to see it, though protecting patient privacy and complying with healthcare regulations. Configure column-level security In the scenario we've recently explored, the syntax to use for implementing column-level security might look as follows: -- Create roles\nCREATE ROLE Doctor AUTHORIZATION dbo;\nCREATE ROLE Nurse AUTHORIZATION dbo;\nCREATE ROLE Receptionist AUTHORIZATION dbo;\nCREATE ROLE Patient AUTHORIZATION dbo;\nGO\n\n-- Grant SELECT on all columns to all roles\nGRANT SELECT ON dbo.Patients TO Doctor;\nGRANT SELECT ON dbo.Patients TO Nurse;\nGRANT SELECT ON dbo.Patients TO Receptionist;\nGRANT SELECT ON dbo.Patients TO Patient;\nGO\n\n-- Deny SELECT on the MedicalHistory column to the Receptionist and Patient roles\nDENY SELECT ON dbo.Patients (MedicalHistory) TO Receptionist;\nDENY SELECT ON dbo.Patients (MedicalHistory) TO Patient;\nGO In this example, we first create the roles Doctor , Nurse , Receptionist , and Patient . We then grant SELECT permissions on all columns in the Patients table to all roles. Finally, we deny SELECT permissions on the MedicalHistory column to the Receptionist and Patient roles. This ensures that only users with the Doctor or Nurse role can access the MedicalHistory column. Understand the benefits In the realm of warehouse security, two commonly used techniques are column-level security and views. Both methods serve to restrict access to sensitive data, but they do so in different ways and offer different advantages. The following table provides a comparative analysis of these two techniques across various aspects such as granularity of access control, maintenance, performance, transparency, and flexibility. This comparison can help you understand the strengths and weaknesses of each method and guide you in choosing the most suitable approach for your specific application requirements. Aspect Column-Level Security Views Granularity of Access Control Allows control at a more granular level. Can specify different access rights for different users or roles on different columns within the same table. Would need to create different views for different sets of permissions. Maintenance Permissions are tied to the columns themselves, so they automatically adapt to changes in table structure. If the underlying table structure changes, views may need to be updated to reflect these changes. Performance Generally more efficient because it operates directly on the table data. Can introduce performance overhead, especially if they are complex or if the underlying tables are large. Transparency The restrictions are transparent to the user. The user queries the table as usual, and the database engine takes care of applying the security rules. The user needs to query a different object (the view instead of the table). Flexibility Less flexible than views. Very flexible and can provide row-level security (by including a WHERE clause in the view definition) in addition to column-level security. They can also transform the data (for example, by calculating derived columns) which is not possible with column-level security. The choice between using column-level security or views will depend on the specific requirements of your application. Always make sure to test any security changes in a safe environment before applying them to a production warehouse.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/column-level-security?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Secure a Microsoft Fabric data warehouse",
        "topic": "Configure SQL granular permissions using T-SQL",
        "text": "If you're familiar with relational databases and enterprise warehouses, it's common knowledge that there are four fundamental permissions governing Data Manipulation Language (DML) operations. These permissions, namely SELECT , INSERT , UPDATE , and DELETE , are universally applicable across all database platforms. All of these permissions can be granted, revoked or denied on tables and views. If a permission is granted using the GRANT statement, then the permission is given to the user or role referenced in the GRANT statement. Users can also be denied permissions using the DENY command. If a user is granted a permission and denied the same permission, the DENY will always supersede the grant, and the user will be denied access to the specific object. Table and view permissions Tables and views represent the objects on which permissions can be granted within a warehouse. Within those tables and views, you can additionally restrict the columns that are accessible to a given security principal. Permission Definition SELECT Allows the user to view the data within the object (table or view). When denied, the user will be prevented from viewing the data within the object. INSERT Allows the user to insert data into the object. When denied, the user will be prevented from inserting data into the object. UPDATE Allows the user the update data within the object. When denied, the user will be prevented from updating data in the object. DELETE Allows the user to delete data within the object. When denied, the user will be prevented from deleting data from the object. Function and stored procedure permissions Like tables and views, functions and stored procedures have several permissions, which can be granted or denied. Permission Definition ALTER Grants the user the ability to change the definition of the object. CONTROL Grants the user all rights to the object. Principle of least privilege The basic idea of the principle of least privilege is that users and applications should only be given the permissions needed in order for them to complete the task. Applications should only have permissions that they need to do in order to complete the task at hand. As an example, if an application accesses all data through stored procedures, then the application should only have the permission to execute the stored procedures, with no access to the tables. Dynamic SQL Dynamic SQL is a concept where a query is built programmatically. Dynamic SQL allows T-SQL statements to be generated within a stored procedure or a query itself. A simple example is shown below. CREATE PROCEDURE sp_TopTenRows @tableName NVARCHAR(128)\nAS\nBEGIN\n    DECLARE @query NVARCHAR(MAX);\n    SET @query = N'SELECT TOP 10 * FROM ' + QUOTENAME(@tableName);\n    EXEC sp_executesql @query;\nEND; In this example, @tableName is the parameter that you can replace with the name of the table you want to inspect. The QUOTENAME function is used to safely quote the table name, preventing SQL injection attacks. The sp_executesql stored procedure is then used to execute the dynamically built query. Please note that this is a simple example and real-world data warehouse tasks might require more complex dynamic SQL queries. Always be cautious when using dynamic SQL due to the risk of SQL injection attacks. Always use parameterization methods like sp_executesql or QUOTENAME to sanitize inputs.",
        "links": [
            "https://learn.microsoft.com/en-us/sql/t-sql/queries/queries?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Secure a Microsoft Fabric data warehouse",
        "topic": "Exercise: Secure a warehouse in Microsoft Fabric",
        "text": "Now it's your chance to secure a warehouse in Microsoft Fabric. In this exercise, youll learn how to secure a warehouse using the concepts explored in this module. Note You need a Microsoft Fabric trial license with the Fabric preview enabled in your tenant. See Getting started with Fabric to enable your Fabric trial license. This exercise includes optional steps that require a second user account to view the applied security measures. If you're unable to create a second account, you can still complete the exercise using your own account. In that case, please refer to the screenshots provided at the end of each section to see the expected results. How to create a second user for this exercise If you're part of an organization that has an Entra or Microsoft 365 tenant: Work with your identity administrator to create the second user either in Entra or the Microsoft 365 admin center . If you're not a member of an organization with an Entra or Microsoft 365 tenant: You're unable to sign up for a Fabric trial with your personal email address. You can sign up for a Microsoft 365 trial and a new organizational tenant will be created and you'll become the User and Billing administrator of the tenant and can then create users. Create the second user in the Microsoft 365 admin center See: Add users Enable the Fabric trial using Getting started with Fabric . Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial?azure-portal=true",
            "https://entra.microsoft.com?azure-portal=true",
            "https://admin.cloud.microsoft?azure-portal=true",
            "https://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-signing-up-for-power-bi-with-a-new-office-365-trial?azure-portal=true",
            "https://admin.cloud.microsoft/#/users?azure-portal=true",
            "https://learn.microsoft.com/en-us/microsoft-365/admin/add-users/add-users?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://go.microsoft.com/fwlink/?linkid=2277744"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Implement a data warehouse with Microsoft Fabric",
        "unit": "Secure a Microsoft Fabric data warehouse",
        "topic": "Summary",
        "text": "In this module, you learned about Dynamic Data Masking (DDM), Row-Level Security (RLS), Column-level security (CLS), and granular SQL permissions in Fabric warehouses. The main takeaways from this module include understanding how DDM, RLS, and CLS work and their use cases. DDM operates at the column level, offering full, email, custom text, and random masking types. RLS works by associating a security predicate function with a table. CLS can be implemented by identifying sensitive columns, defining access roles, assigning roles to users, and implementing access control. In addition, you learned about the principle of least privilege, which suggests that users and applications should only be given the permissions necessary to complete their tasks. For additional reading, you can refer to the following URLs: Create a Warehouse in Microsoft Fabric Security for data warehousing in Microsoft Fabric Share your warehouse and manage permissions",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/create-warehouse?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/security?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/share-warehouse-manage-permissions?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Implement continuous integration and continuous delivery (CI/CD) in Microsoft Fabric",
        "topic": "Introduction",
        "text": "As a Fabric data engineer or developer, you're responsible for designing, building, and maintaining Fabric items that store, process and, analyze data. You collaborate with your team to release changes from development through production in different Fabric workspaces. With multiple engineers working on projects, there's a need to incrementally release and integrate Fabric item changes and move those changes through a deployment process where they can be tested and quickly released. Lifecycle management in Fabric lets you do this. Lifecycle management in Fabric uses deployment pipelines and source control integration to help you achieve continuous integration and continuous delivery (CI/CD). You can integrate Fabric with version control systems like GitHub and Azure DevOps and improve your delivery process, and integrate automated testing. By the end of this module, you'll know how to work with source control in Fabric, how to use Fabric deployment pipelines and how Fabric REST APIs can be used for CI/CD automation.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Implement continuous integration and continuous delivery (CI/CD) in Microsoft Fabric",
        "topic": "Understand Continuous Integration and Continuous Delivery (CI/CD)",
        "text": "When you and members of your team are each responsible developing and maintaining different parts of your Fabric environment, a best practice is to work in isolated development environments until you're ready to combine your development efforts and publish your changes to a particular pre-production environment. When you're ready to publish your changes, you need to make sure that your changes don't break existing code or interfere with changes made by other developers. There's also a need to ensure that code changes are saved and can be reverted if there are issues. The built-in continuous integration and continuous delivery capabilities in Fabric can help facilitate this. Continuous integration and continuous delivery is a process for integrating code contributions from multiple developers into a main codebase. Contributions are frequently committed, and automated processes build and test the new code. Code is continuously moving into production, reducing feature development time. Continuous integration If developers work on separate code branches on their local machines for long periods of time and only merge their changes to the main codebase once their work is finished, this increases the likelihood of conflicts and bugs that might only be identified in later development stages and can slow down delivery of features to users. Continuous integration (CI) helps you avoid bugs and code failures and lets you continuously develop and release functionality. In CI, you frequently commit code to a shared code branch or trunk in a version control system and once it's merged, changes are validated by a build process and automated testing. Conflicts between new and existing code are identified earlier in the development process and are easier and faster to fix. Continuous delivery Continuous delivery happens after continuous integration. Once CI is complete, code is deployed to a staging environment where more automated testing is performed before code is released into production. Continuous deployment Continuous deployment is a process that automatically releases updates into production environments through structured deployment stages, once they pass automated tests. Use CI/CD in Fabric Managing the lifecycle of Fabric items using CI/CD has two parts: integration and deployment. Integration is implemented using Git. Deployment is implemented using Fabric deployment pipelines. Automation of deployment or integration is implemented using Fabric REST APIs. Git: Lets your team collaborate using branches, and provides version control. It helps you manage incremental code changes, and see code history. Deployment pipelines: Lets you promote code changes to different environments like development, test, and production. Fabric REST APIs: Enables automation and lets you programmatically manage CI/CD processes.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Implement continuous integration and continuous delivery (CI/CD) in Microsoft Fabric",
        "topic": "Implement version control and Git integration",
        "text": "To support continuous integration, you frequently merge your code changes into a shared repository. The shared repository is part of a version control system like GitHub or Azure DevOps. Version control is a way of managing changes to code over time. It lets you track code revisions, contribute collaboratively to code development, and revert to prior versions of code if needed. GitHub and Azure DevOps are the version control systems that are supported in Fabric. These version control systems allow you to create a copy of a code repository that's called a branch. You can use the branch to work on your own code independently from the main version of your team's code. When you have changes to submit, you can commit them to the repository and merge your changes with a main code branch. Integration with version control is at the workspace level in Fabric. You can version items you develop within a workspace. Connect to a Git Repository A Fabric workspace is shared environment that accesses live items. Any changes made directly in the workspace overrides and affect all other workspace users. A best practice is for you to develop in an isolated workspace, outside of a shared, live workspace. In your own protected workspace, you can connect to your own branch and sync content from the live workspace into your protected workspace, and then commit your changes back to your branch or the main branch. Set up a git repository: The first step in implementing Git integration is to set up a Git repository in either GitHub or Azure DevOps. The repository is the central location for storing and managing items. Connect a Fabric workspace to a Git repository: Next, within the workspace that you want to connect to your repository, establish a connection to the repository from the Git integration option in workspace settings. When you connect a workspace to Git, you create or select an existing a Git repository branch to sync with. Fabric syncs the content between the workspace and Git so they have the same content. Commit and update the Fabric workspace and Git repository After you connect to the repository, the workspace shows a Git status column indicating the sync state of items in the workspace, compared to the items in the remote branch. The source control icon shows the number of items that are different between the workspace and the repository. To synchronize the workspace and repository: When you make workspace changes, synchronize them with the Git branch using the Changes selection in the Source control window. When new commits are made in the Git branch, synchronize them with your workspace using the Updates selection in the Source control window. Branching scenarios Changes that you make to a workspace when you're doing development work affect all other workspace users so it's a best practice to work in isolation outside of shared workspaces. To keep your development work isolated from shared workspaces, you can develop using: A separate, isolated workspace Client tools like Power BI Desktop for reports and semantic models or VS Code for Notebooks. In both scenarios, your feature development work should take place in a dedicated branch instead of the main code branch.  This makes it easy for multiple developers to work on a feature without affecting the main branch. Create a dedicated branch, issue pull requests and sync a workspace with Git Create a dedicated branch and issue pull requests to pull changes from your branch into the main branch by following these steps: For development using a separate, isolated workspace: Connect a development workspace to the main branch, following the instructions in the section on this page entitled \"Connect to a Git Repository\". If you're a developer who works in the Fabric web interface, create an isolated branch for your work from the development workspace that's connected to the main branch by selecting Source control and Branch out to new workspace . Name the branch and associate it with another workspace.  The new workspace syncs with the new branch you create and become an isolated work environment for your work. Makes changes in your branch, then commit them to your isolated branch via the Source control interface in Fabric. Then, in Git , create a Pull Request (PR) to pull the changes from your isolated branch into the main branch. The main branch in Git will be updated once the PR is merged to the main branch. When you open the shared development workspace, you'll be prompted to synchronize the new content from Git with the shared development workspace. When using client tools for development, the process is similar to that when developing in the Fabric web interface. Connect a development workspace to the main branch, following the instructions in the section on this page entitled \"Connect a Fabric workspace to a Git repository\". Clone the repository on your local machine. Push the changes to the remote repository when you're ready to test in Fabric. Test the changes by connecting your isolated branch to a separate workspace. Issue a PR in Git to merge your changes into the main branch. When you open the shared workspace associated with the main branch, you'll be prompted to sync the changes from the repository into the workspace.",
        "links": null,
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/implement-cicd-in-fabric/media/connect-git.png",
                "image_alt": "Screenshot of workspace to Git provider connection interface."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/implement-cicd-in-fabric/media/branch-selection.png",
                "image_alt": "Screenshot of branch selection in Git provider interface."
            },
            {
                "image_name": "image3",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/implement-cicd-in-fabric/media/git-status.png",
                "image_alt": "Screenshot of uncommitted changes in workspace."
            },
            {
                "image_name": "image4",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/implement-cicd-in-fabric/media/branch-out-to-new-workspace.png",
                "image_alt": "Screenshot of creating new workspace and branch."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Implement continuous integration and continuous delivery (CI/CD) in Microsoft Fabric",
        "topic": "Implement deployment pipelines",
        "text": "Pipelines enable a continuous integration/continuous deployment (CI/CD) approach that ensures content is updated, tested, and regularly refreshed. Pipelines are a way to automate the movement of content through the development, test, and production stages of the content development lifecycle. What are deployment pipelines? Fabric deployment pipelines help you deploy your Fabric items across different environments like development, test, and production. They let you develop and test content in Fabric before it reaches end users. Create a deployment pipeline Deployment pipelines can be created using two different methods: Using the Workspaces icon on the left navigation pane in Fabric. Using the Create deployment pipeline icon at the top of a workspace Follow these steps to create a deployment pipeline: Select the Workspaces icon on the left navigation pane, then Deployment pipelines. Select New pipeline . Then name the pipeline and select Next . Define, and name the stages in your pipeline. Then select Create and continue. Assign a workspace to a stage. Then select the green check mark next to the stage. Then you're ready to deploy content to the pipeline. Deploy content to a pipeline stage The deployment process lets you clone content from one stage in the pipeline to another, typically from development to test, and from test to production. To deploy content between stages, select the stage to deploy to, and then select the stage in the Deploy from drop-down box, and then select the Deploy button. The deployment process copies all of the workspace content into the target stage. In the following image, there's a data pipeline that only exists in the development stage that will be moved to the test stage when Deploy is selected in the development stage. Use deployment pipelines with Git Deployment pipelines can be used with Git branches. This would be used to promote content between development, test, and production environments when content for each environment resides in different Git repositories or branches. To use deployment pipelines with Git branches: Follow the instructions in the section on this page entitled \"Create a deployment pipeline\" to create a deployment pipeline and assign each stage to a workspace. Assign each workspace in the deployment pipeline to a Git repository and branch in Git integration in Workspace settings . Promote content between staging environments using the deploy button in the pipeline as described in the Deploy content to a pipeline stage section on this page. This moves content between environments in Fabric but the Git repository won't be updated until you manually update from the workspace. In the image below, the checkmark in the deployment stage box indicates that a data pipeline item exists in all three staging environments of the deployment pipeline in Fabric and that the Fabric stages are synchronized. When we select Source control from either the Test or Production workspaces that are part of the deployment pipeline, we see that the pipeline hasn't been synchronized with the Git repository. To synchronize the repository with the Test workspace, select the Commit button in the Source control window shown in the preceding image.",
        "links": null,
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/implement-cicd-in-fabric/media/create-stages.png",
                "image_alt": "Screenshot of pipeline stage selector."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/implement-cicd-in-fabric/media/assign-workspace.png",
                "image_alt": "Screenshot of workspace assignment interface."
            },
            {
                "image_name": "image3",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/implement-cicd-in-fabric/media/deploy-content.png",
                "image_alt": "Screenshot of content deployment interface."
            },
            {
                "image_name": "image4",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/implement-cicd-in-fabric/media/connect-git.png",
                "image_alt": "Screenshot of workspace to Git provider connection interface."
            },
            {
                "image_name": "image5",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/implement-cicd-in-fabric/media/pipeline-presync.png",
                "image_alt": "Screenshot of pipeline before files are synced with Git."
            },
            {
                "image_name": "image6",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/implement-cicd-in-fabric/media/git-not-updated.png",
                "image_alt": "Screenshot of workspace showing uncommitted items and showing the source control box where files aren't yet synced with Git."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Implement continuous integration and continuous delivery (CI/CD) in Microsoft Fabric",
        "topic": "Automate CI/CD using Fabric APIs",
        "text": "Fabric REST APIs allow you to automate Fabric procedures and processes, improving efficiency and productivity. REST API stands for Representational State Transfer Programming Application Interface. Azure REST APIs are used to manage and interact with various Azure services. Some of the advantages of using the Fabric REST APIs are: Automating repeat processes with consistency, making it easier to perform data processing on an ongoing basis. Seamless integration with other systems and applications, providing a streamlined and efficient data pipeline. Fabric CI/CD REST APIs are available for deployment pipelines and Git integration. Deployment pipelines REST APIs Git REST APIs Use the Fabric REST APIs for CI/CD to automate processes You can use the Fabric CI/CD REST APIs to: Commit the changes made in the workspace to the connected remote branch. Update the workspace with commits pushed to the connected branch. See which items have incoming changes and which items have changes that weren't yet committed to Git with the Git status API. List Deployment Pipeline Stage Items: Returns the supported items from the workspace assigned to the specified stage of the specified deployment pipeline. Deploy Stage Content: Deploys items from the specified stage of the specified deployment pipeline.",
        "links": [
            "https://learn.microsoft.com/en-us/rest/api/fabric/core/deployment-pipelines?azure-portal=true",
            "https://learn.microsoft.com/en-us/rest/api/fabric/core/git?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Implement continuous integration and continuous delivery (CI/CD) in Microsoft Fabric",
        "topic": "Exercise: Implement deployment pipelines in Microsoft Fabric",
        "text": "Now it's your chance to use CI/CD in Microsoft Fabric. In this exercise, you learn how to use Fabric deployment pipelines to automate the process of copying changes between environments like development, test, and production. Note You need a Microsoft Fabric trial license with the Fabric preview enabled in your tenant. See Getting started with Fabric to enable your Fabric trial license. Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://go.microsoft.com/fwlink/?linkid=2296918&azure-portal=true"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Implement continuous integration and continuous delivery (CI/CD) in Microsoft Fabric",
        "topic": "Summary",
        "text": "In this module, you learned how to work with version control in Fabric, how to use Fabric deployment pipelines and how Fabric REST APIs can be used for CI/CD automation. To learn more, see: Tutorial: End to end lifecycle management Exploring CI/CD Capabilities in Microsoft Fabric: A Focus on Data Pipelines Using the Microsoft Fabric REST APIs",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/cicd/cicd-tutorial?azure-portal=true",
            "https://blog.fabric.microsoft.com/en-us/blog/exploring-ci-cd-capabilities-in-microsoft-fabric-a-focus-on-data-pipelines/",
            "https://learn.microsoft.com/en-us/rest/api/fabric/articles/using-fabric-apis"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Monitor activities in Microsoft Fabric",
        "topic": "Introduction",
        "text": "In a data analytics solution, you create, manage, and optimize the frameworks for collecting, storing, processing, and analyzing data. Data in its raw form isn't meaningful for making business decisions. Data from different sources has to be combined and loaded or ingested into data stores, and transformed into formats that are useful for analysis. The ingestion and transformation process must be monitored so errors can be identified and remediated before they affect users. Microsoft Fabric provides capabilities for monitoring Fabric activities. In this module, we explore how to use Monitor Hub to monitor ingestion and transformation activities in your Fabric environment and how to use Activator to automatically take action when patterns or conditions are detected in changing data. By the end of this module, you'll understand the monitoring capabilities in Microsoft Fabric.  You'll also get to complete a practical exercise using Monitor Hub.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Monitor activities in Microsoft Fabric",
        "topic": "Understand monitoring",
        "text": "Monitoring is the process of collecting system data and metrics that determine if a system is healthy and operating as expected. Monitoring exposes errors that occurred and when they happened. To investigate issues and remediate errors, historical data is analyzed to get a picture of the health of a system or process. Monitoring Fabric activities In Fabric, you schedule activities and jobs that perform tasks like data movement, and transformation. Activities have dependencies on one another. You need to make sure that data arrives in its expected location on time and that system errors or delays don't affect users or downstream activities. End-to-end processes need to be managed to ensure they're reliable, performant, and resilient. One aspect of this monitoring is identifying and handling long-running operations and errors effectively. By doing this, you can minimize downtime and quickly address any underlying issues. The following activities in Fabric allow you to perform tasks that deliver data to users. These activities should be monitored: Data pipeline activity - A data pipeline is a group of activities that together perform a data ingestion task. Pipelines allow you to manage, extract, transform, and load (ETL) activities together instead of individually. Monitor the success or failure of jobs and pipeline activities. Look for errors if the pipeline failed. View job history to compare current activity performance to past job execution performance to gain insight into when errors were first introduced into a process. Dataflows - A dataflow is a tool for ingesting, loading, and transforming data using a low-code interface. Dataflows can be run manually or scheduled or run as part of pipeline orchestration. Monitor start and end times, status, duration, and table load activities. To investigate issues, drill down into activities and view information about errors. Semantic model refreshes - A semantic model is a visual representation of a data model that's ready for reporting and visualization. It contains transformations, calculations, and data relationships. Changes to the data model require the semantic model to be refreshed. Semantic models can be refreshed from data pipelines using the semantic model refresh activity. Monitor for refresh retries to help identify transient issues, before classifying an issue as a failure. Spark jobs, notebooks and lakehouses - Notebooks are an interface for developing Apache Spark jobs. Data can be loaded, or transformed for lakehouses using Spark and notebooks.  Monitor Spark job progress, task execution, resource usage, and review Spark logs. Microsoft Fabric Eventstreams - Events are observations about the state of an object, like a timestamp for weather sensors. Eventstreams in Fabric are set up to run perpetually to ingest real-time or streaming events into Fabric and transform them for analytics needs, and then route them to various destinations.  Monitor streaming event data, ingestion status, and ingestion performance. Monitoring best practices Continuously monitor the data ingestion, transformation, and load processes to ensure they're running smoothly. Monitoring best practices include: Identifying what to monitor and tracking metrics. Collecting and analyzing data on a regular basis to identify normal behavior so you can spot anomalies when they occur. Reviewing logs and metrics regularly to identify and establish parameters for normal system behavior. Taking action to resolve problems when metrics and logs show deviations from normal behavior. Optimizing performance by using monitoring data to identify bottlenecks or performance issues.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Monitor activities in Microsoft Fabric",
        "topic": "Use Microsoft Fabric Monitor Hub",
        "text": "Visualization tools make monitoring easier. They help you identify trends or anomalies. Monitor hub is the monitoring visualization tool in Microsoft Fabric. Monitor hub collects and aggregates data from selected Fabric items and processes. It stores Fabric activity data in a common interface so you can view the status of multiple different data integration, transformation, movement, and analysis activities in Fabric in one place, rather than monitor each separately. Activities displayed in the Monitor hub Some of the activities you can see monitoring metadata for in the Microsoft Fabric Monitor hub include: Data pipeline execution history Dataflow executions Datamart and semantic model refreshes Spark job and notebook execution history and job details View the Monitor Hub The Monitor hub can be opened by selecting Monitor from the Fabric navigation pane. View Fabric activity detail Each activity in Monitor hub can be selected and several actions can be performed for the selected activity. Actions vary by activity and include options such as: opening the activity, retrying it, viewing activity details or historical runs. To view this information, select the ellipsis that appears when you hover over an activity. When you select View detail , the screen that appears is customized for the activity you select and provides clarity about what happened during the activity. You can view metadata such as: Activity status Start and end time Duration Activity statistics Investigate what happened during a Fabric activity To investigate what happened during an activity some activities in Monitor hub include hyperlinks that allow you to drill down into execution details. Information is provided about any errors and the success or failure of an execution. Spark activities across items can be viewed. You can view Spark applications triggered from Notebooks, Spark Job Definitions, and Pipelines. For more information, see: Apache Spark monitoring overview",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/data-engineering/spark-monitoring-overview"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/monitor-fabric-items/media/monitor-hub.png",
                "image_alt": "Screenshot of the Microsoft Fabric Monitor hub interface."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/monitor-fabric-items/media/view-monitor-hub-details.png",
                "image_alt": "Screenshot of the Microsoft Fabric Monitor hub details interface."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Monitor activities in Microsoft Fabric",
        "topic": "Take action with Microsoft Fabric Activator",
        "text": "When monitoring surfaces changing data, anomalies, or critical events, alerts are generated or actions are triggered. Real-time data analytics is commonly based on the ingestion and processing of a data stream that consists of a perpetual series of data, typically related to specific point-in-time events. For example, a stream of data from an environmental IoT weather sensor. Real-Time Intelligence in Fabric contains a tool called Activator that can be used to trigger actions on streaming data. For example, a stream of data from an environmental IoT weather sensor might be used to trigger emails to sailors when wind thresholds are met. When certain conditions or logic is met, an action is taken, like alerting users, executing Fabric job items like a pipeline, or kicking off Power Automate workflows. The logic can be either a defined threshold, a pattern like events happening repeatedly over a time period, or the results of logic defined by a Kusto Query Language (KQL) query. What is Activator Activator is a technology in Microsoft Fabric that enables automated processing of events that trigger actions. For example, you can use Activator to notify you by email when a value in an eventstream deviates from a specific range or to run a notebook to perform some Spark-based data processing logic when a real-time dashboard is updated. Understand Activator key concepts Activator operates based on four core concepts: Events , *Objects, Properties , and Rules . Events - Each record in a stream of data represents an event that has occurred at a specific point in time. Objects - The data in an event record can be used to represent an object , such as a sales order, a sensor, or some other business entity. Properties - The fields in the event data can be mapped to properties of the business object, representing some aspect of its state. For example, a total_amount field might represent a sales order total, or a temperature field might represent the temperature measured by an environmental sensor. Rules - The key to using Activator to automate actions based on events is to define rules that set conditions under which an action is triggered based on the property values of objects referenced in events. For example, you might define a rule that sends an email to a maintenance manager if the temperature measured by a sensor exceeds a specific threshold. Use cases for Activator Activator can help you in various scenarios, such as dynamic inventory management, real-time customer engagement, and effective resource allocation in cloud environments. It's a potent tool for any circumstance that requires real-time data analysis and actions. Use Activator to: Initiate marketing actions when product sales drop. Send notifications when temperature changes could affect perishable goods. Flag real-time issues affecting the user experience on apps and websites. Trigger alerts when a shipment hasn't been updated within an expected time frame. Send alerts when a customer's account balance crosses a certain threshold. Respond to anomalies or failures in data processing workflows immediately. Run ads when same-store sales decline. Alert store managers to move food from failing grocery store freezers before it spoils. Tip For more information about working with Activator, see Tutorial: Create and activate an Activator rule .",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/data-activator/activator-tutorial"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/monitor-fabric-items/media/activator.png",
                "image_alt": "Screenshot of an Activator alert in Microsoft Fabric."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Monitor activities in Microsoft Fabric",
        "topic": "Exercise - Monitor Fabric activity in the Monitor hub",
        "text": "Now it's your turn to monitor Fabric activity in the Monitor hub. Note To complete this exercise, you need a Microsoft Fabric tenant. See Getting started with Fabric to find out how to enable a Fabric trial license. Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://go.microsoft.com/fwlink/?linkid=2260611&azure-portal=true"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Monitor activities in Microsoft Fabric",
        "topic": "Summary",
        "text": "In this module, you learned how monitoring is implemented in Microsoft Fabric. You learned how to use Monitor Hub to monitor Microsoft Fabric activities from a central location and how to use Activator to automatically take action when patterns or conditions are detected in changing data. Tip For more information about monitoring Fabric activities and responding to detected changes in data, see: Use the Monitor hub What is Activator?",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/admin/monitoring-hub",
            "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/data-activator/activator-introduction"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Secure data access in Microsoft Fabric",
        "topic": "Introduction",
        "text": "Security in Microsoft Fabric is optimized for securing data for specific use cases. Different users need the ability to perform various actions in Fabric to fulfill their job responsibilities. Fabric facilitates this by allowing you to grant users access to specific data workloads through workspace and item permissions, compute permissions, and OneLake data access roles (preview). Secure data by use case A security use case in Fabric refers to a set of users needing data access and accessing data in a specific way. Once a use case is identified, Fabric permissions associated with that use case can be configured. Suppose you work at a healthcare company with multiple systems that store data, such as electronic health records (EHR), insurance claims data, clinical trial data, patient and disease registries, and administrative data. Different users within your company or partner organizations need to view, transform, analyze, aggregate, and use this data to derive business insights. Users need access to different Fabric compute engines, items, and workspaces to perform their jobs effectively: Data engineers need access to data in a lakehouse to develop downstream data products. Business or data analysts need to query data to answer business questions. Data scientists need to access data in a lakehouse and consume it through Apache Spark to create models and experiments. Report creators need to build reports to share with report consumers. Report consumers need to view data in Power BI reports to make decisions. In this module, you'll learn how to use the Fabric access control features available to secure your data and provide your team with the necessary access within Fabric to perform their job duties. You'll explore Fabrics multi-layer security model and how to use it to manage data access. By the end of this module, you'll know how to configure security for an entire workspace, for individual Fabric data items, and how to apply granular permissions within Fabric data items.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Secure data access in Microsoft Fabric",
        "topic": "Understand the Fabric security model",
        "text": "Data access in organizations is often restricted by users' responsibilities, and roles and by an organization's Fabric deployment patterns, and data architecture. Fabric has a flexible, multi-layer security model that allows you to configure security to accommodate different data access requirements. Having the ability to control permissions at different layers means you can adhere to the principle of least privilege, restricting user permissions to only what's needed to perform job tasks. Fabric has three security levels and they're evaluated sequentially to determine whether a user has data access. The order of evaluation for access is: Microsoft Entra ID authentication: checks if the user can authenticate to the Azure identity and access management service, Microsoft Entra ID. Fabric access: checks if the user can access Fabric. Data security: checks if the user can perform the action they've requested on a table or file. The third level, data security, has several building blocks that can be configured individually or together to align with different access requirements. The primary access controls in Fabric are: Workspace roles Item permissions Compute or granular permissions OneLake data access controls (preview) It's helpful to envision these building blocks in a hierarchy to understand how access controls can be applied individually or together. A workspace in Fabric enables you to distribute ownership and access policies using workspace roles . Within a workspace, you can create Fabric data items like lakehouses, data warehouses, and semantic models. Item permissions can be inherited from a workspace role or set individually by sharing an item. When workspace roles provide too much access, items can be shared using item permissions to ensure proper security. Within each data item, granular engine permissions such as Read, ReadData, or ReadAll can be applied. Compute or granular permissions can be applied within a specific compute engine in Fabric, like the SQL Endpoint or semantic model. Fabric data items store their data in OneLake. Access to data in the lakehouse can be restricted to specific files or folders using the role-based-access control (RBAC) feature called OneLake data access controls (preview).",
        "links": null,
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/secure-data-access-in-fabric/media/data-access-controls.png",
                "image_alt": "Screenshot of Fabric access control hierarchy."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Secure data access in Microsoft Fabric",
        "topic": "Configure workspace and item permissions",
        "text": "Workspaces are environments where users can collaborate to create groups of items. Items are the resources you can work with in Fabric such as lakehouses, warehouses, and reports. Workspace roles are preconfigured sets of permissions that let you manage what users can do and access in a Fabric workspace. Item permissions control access to individual Fabric items within a workspace. Item permissions let you either adjust the permissions set by a workspace role or give a user access to one or more items within a workspace without adding the user to a workspace role. Let's consider some scenarios where you would need to configure data access using  workspace roles and item permissions. Understand workspace roles Suppose you work at a health care company as the Fabric security admin. You need to set up access for a new data engineer. The data engineer needs the ability to: Create Fabric items in an existing workspace Read all data in an existing lakehouse that's in the same workspace where they can create Fabric items Workspace roles control what users can do and access within a Fabric workspace. There are four workspace roles and they apply to all items within a workspace. Workspace roles can be assigned to individuals, security groups, Microsoft 365 groups, and distribution lists. Users can be assigned to the following roles: Admin - Can view, modify, share, and manage all content and data in the workspace, and manage permissions. Member - Can view, modify, and share all content and data in the workspace. Contributor - Can view and modify all content and data in the workspace. Viewer - Can view all content and data in the workspace, but can't modify it. Tip For a full list of the permissions associated with workspace roles, see: Roles in workspaces To meet the access requirements for the new data engineer, you can assign them the workspace Contributor role. This gives them access to modify content in the workspace, including creating Fabric items like lakehouses. The contributor role would also allow them to read data in the existing lakehouse. Assign workspace roles Users can be added to workspace roles from the Manage access button from within a workspace. Add a user by entering the user's name and selecting the workspace role to assign them in the Add people dialogue. Configure item permissions Item permissions control access to individual Fabric items within a workspace. Item permission can be used to give a user access to one or more items within a workspace without adding the user to a workspace role or can be used with workspace roles. Suppose that after a few months of having Contributor access on a workspace, a data engineer no longer needs to create Fabric items and now only needs to view a single lakehouse and read data in it. Since the engineer no longer needs to view all items in the workspace, the Contributor workspace role can be removed and item permissions on the lakehouse can be configured so the engineer will only be able to see the lakehouse metadata and data and nothing else in the workspace. This item access configuration helps you adhere to the principle of least privilege, where the engineer only has access to what's needed to perform their job duties. An item can be shared and item permissions can be configured by selecting on the ellipsis (...) next to a Fabric item in a workspace and then selecting Manage permissions . In the Grant people access window that appears after selecting Manage permissions , if you add the user and don't select any of the checkboxes under Additional permissions , the user will have read access to the lakehouse metadata. The user won't have access to the underlying data in the lakehouse. To grant the engineer the ability to read data and not just metadata, Read all SQL endpoint data or Read all Apache Spark can be selected. Tip Each Fabric data item has its own security model. To learn more about permissions that can be granted when a lakehouse or other Fabric data item is shared see: Warehouse Lakehouse Semantic model",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/roles-workspaces?azure-portal=true",
            "https://learn.microsoft.com/wwl-data-ai/secure-data-access-in-fabric/media/manage-access.png#lightbox",
            "https://learn.microsoft.com/wwl-data-ai/secure-data-access-in-fabric/media/manage-item-permissions.png#lightbox",
            "https://learn.microsoft.com/wwl-data-ai/secure-data-access-in-fabric/media/grant-people-access-lakehouse.png#lightbox",
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/share-warehouse-manage-permissions?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-sharing?azure-portal=true",
            "https://learn.microsoft.com/en-us/power-bi/connect-data/service-datasets-permissions?azure-portal=true"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/secure-data-access-in-fabric/media/manage-access.png",
                "image_alt": "Screenshot of clicking the manage access button."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/secure-data-access-in-fabric/media/manage-item-permissions.png",
                "image_alt": "Screenshot of configuring item permissions. "
            },
            {
                "image_name": "image3",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/secure-data-access-in-fabric/media/grant-people-access-lakehouse.png",
                "image_alt": "Screenshot of grant people lakehouse read all access."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Secure data access in Microsoft Fabric",
        "topic": "Apply granular permissions",
        "text": "When the permissions provided by workspace roles or item permissions are insufficient, granular permissions like table and row-level security and file and folder access can be set through the: SQL analytics endpoint OneLake data access roles (preview) Warehouse Semantic model Configure data access through the SQL analytics endpoint in a lakehouse Data in a lakehouse can be read through the SQL analytics endpoint. Each Lakehouse has an autogenerated SQL analytics endpoint that can be used to transition between the lake view of the lakehouse and the SQL view of the lakehouse. The lake view supports data engineering and Apache Spark and the SQL view of the same lakehouse allows you to create views, functions, stored procedures and to apply SQL security and object level permissions. Data in a Fabric lakehouse is stored with the following folder structure: /Files /Tables View the SQL analytics endpoint view of the lakehouse The SQL analytics endpoint is used to read data in the /Tables folder of the lakehouse using T-SQL. Apply granular permissions to the lakehouse using T-SQL Using the SQL analytics endpoint, granular T-SQL permissions can be applied to SQL objects using Data Control Language (DCL) commands such as: GRANT DENY REVOKE Row-level security, column-level security, and dynamic data masking can also be applied using the SQL analytics endpoint. See: Row-level security Column-level security Dynamic data masking Configure data access through the lake view of the lakehouse The lake view of the lakehouse is used to read data in the /Tables and /Files folder of the lakehouse. Use OneLake data access roles to secure data Workspace and item permissions provide coarse access to data in a lakehouse. To further refine data access, folders in the lake view of the lakehouse can be secured using OneLake data access roles (preview). You can create custom roles within a lakehouse and grant read permissions only to specific folders in OneLake. Folder security is inheritable to all subfolders. To create a custom OneLake data access role: Select Manage OneLake data access (preview) from the menu in the lake view of the lakehouse. In the New Role window, create a new role name and select the folders to grant access to. Once the role is created, assign a user or group to the role and select the permissions to assign. Tip For more information on how OneLake RBAC permissions are evaluated with workspace and item permissions, see: How OneLake RBAC permissions are evaluated with Fabric permissions Configure granular warehouse permissions Granular permissions can be applied to warehouses using the SQL analytics endpoint, similar to the way the endpoint is used for the lakehouse. The same permissions can be applied: GRANT, REVOKE, and DENY and row-level security, column-level security, and dynamic data masking. Configure Semantic model permissions A user's role in a workspace implicitly grants them permission on the semantic models in a workspace. Semantic models allow for security to be defined using DAX.  More granular permission can be applied using row-level security (RLS). To learn more about the managing RLS or permissions on the semantic model see: Semantic model permissions Row-level security (RLS) with Power BI",
        "links": [
            "https://learn.microsoft.com/wwl-data-ai/secure-data-access-in-fabric/media/analytics-endpoint.png#lightbox",
            "https://learn.microsoft.com/en-us/sql/t-sql/statements/grant-transact-sql?azure-portal=true",
            "https://learn.microsoft.com/en-us/sql/t-sql/statements/deny-transact-sql?azure-portal=true",
            "https://learn.microsoft.com/en-us/sql/t-sql/statements/revoke-database-permissions-transact-sql?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/row-level-security?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/column-level-security?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/data-warehouse/dynamic-data-masking?azure-portal=true",
            "https://learn.microsoft.com/wwl-data-ai/secure-data-access-in-fabric/media/lakehouse-files.png#lightbox",
            "https://learn.microsoft.com/wwl-data-ai/secure-data-access-in-fabric/media/manage-onelake-data-access.png#lightbox",
            "https://learn.microsoft.com/en-us/fabric/onelake/security/data-access-control-model#how-onelake-rbac-permissions-are-evaluated-with-fabric-permissions?azure-portal=true",
            "https://learn.microsoft.com/wwl-data-ai/secure-data-access-in-fabric/media/warehouse-granular-permissions.png#lightbox",
            "https://learn.microsoft.com/en-us/power-bi/connect-data/service-datasets-permissions?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/security/service-admin-row-level-security?azure-portal=true"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/secure-data-access-in-fabric/media/analytics-endpoint.png",
                "image_alt": "Screenshot of SQL analytics endpoint view."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/secure-data-access-in-fabric/media/lakehouse-files.png",
                "image_alt": "Screenshot of files in lakehouse."
            },
            {
                "image_name": "image3",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/secure-data-access-in-fabric/media/manage-onelake-data-access.png",
                "image_alt": "Screenshot of OneLake data access button."
            },
            {
                "image_name": "image4",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl-data-ai/secure-data-access-in-fabric/media/warehouse-granular-permissions.png",
                "image_alt": "Screenshot of warehouse granular permissions."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Secure data access in Microsoft Fabric",
        "topic": "Exercise: Secure data access in Microsoft Fabric",
        "text": "Now it's your chance to secure data access in Microsoft Fabric. In this exercise, you learn how to secure data access in Fabric using the concepts explored in this module. Note You need a Microsoft Fabric trial license with the Fabric preview enabled in your tenant. See Getting started with Fabric to enable your Fabric trial license. To complete the exercises in this lab, you'll need two users: one user should be assigned the Workspace Admin role, and the other will be assigned permissions throughout this lab. To assign roles to workspaces see Give access to your workspace . Launch the exercise and follow the instructions.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/get-started/give-access-workspaces"
        ],
        "images": null,
        "videos": null,
        "exercise": "https://go.microsoft.com/fwlink/?linkid=2293021&azure-portal=true"
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Secure data access in Microsoft Fabric",
        "topic": "Summary",
        "text": "In this module, you learned how to use the access control features available across various Fabric engines to secure your data and provide your team with the necessary access within Fabric to perform their job duties. You explored Fabrics multi-layer security model and how to use it to manage data access. The main takeaways from this module include understanding how to configure security for an entire workspace, and for individual Fabric data items, and how to apply granular permissions. For more reading, you can refer to the following URLs: Microsoft Fabric security white paper Microsoft Fabric permission model Build common data architectures with OneLake in Microsoft Fabric How to secure data for common Fabric data architectures End-to-end security scenario",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/security/white-paper-landing-page?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/security/permission-model?azure-portal=true",
            "https://blog.fabric.microsoft.com/en-us/blog/building-common-data-architectures-with-onelake-in-microsoft-fabric?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/onelake/security/how-to-common-data-architectures?azure-portal=true",
            "https://learn.microsoft.com/en-us/fabric/security/security-scenario?azure-portal=true"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Administer a Microsoft Fabric environment",
        "topic": "Introduction",
        "text": "Administering a Microsoft Fabric environment involves tasks that are essential for ensuring the efficient and effective use of the Fabric platform within an organization. As a Fabric administrator (admin), you need to know: Fabric architecture Security and governance features Analytics capabilities Deployment and licensing options You also need to be familiar with the Fabric admin portal and other administrative tools, and be able to configure and manage the Fabric environment to meet the needs of your organization. Fabric admins work with business users, data analysts, and IT professionals to deploy and use Fabric to meet business objectives and comply with organizational policies and standards. By the end of this module, you'll have an understanding of the Fabric administrator role and the tasks and tools involved in administering Fabric.",
        "links": null,
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Administer a Microsoft Fabric environment",
        "topic": "Understand the Fabric Architecture",
        "text": "Microsoft Fabric is a Software-as-a-Service platform, which provides a simple and integrated approach while reducing administrative overhead. Fabric provides an all-in-one analytics solution for enterprises that covers everything from data movement to data science, real-time analytics, and business intelligence. It offers a comprehensive suite of services, including: Data warehousing Data engineering Data integration Data science Real-time intelligence Business intelligence All data in Fabric is stored in OneLake, which is built on Azure Data Lake Storage (ADLS) gen2 architecture. OneLake is hierarchical in nature to simplify management across your organization. There's only one OneLake per tenant and it provides a single-pane-of-glass file-system namespace that spans across users, regions, and even clouds. Understand Fabric concepts Tenant is a dedicated space for organizations to create, store, and manage Fabric items. There's often a single instance of Fabric for an organization, which is aligned with Microsoft Entra ID. The Fabric tenant maps to the root of OneLake and is at the top level of the hierarchy. Capacity is a dedicated set of resources that is available at a given time to be used. A tenant can have one or more capacities associated with it. Capacity defines the ability of a resource to perform an activity or to produce output. Capacity needs vary by item and duration of use. Fabric offers capacity through the Fabric SKU and Trials. Domain is a logical grouping of workspaces. Domains are used to organize items in a way that makes sense for your organization. You can group things together in a way that makes it easier for the groups of people to have access to workspaces. For example, you might have a domain for sales, another for marketing, and another for finance. Workspace is a collection of items that brings together different functionality in a single tenant. It acts as a container that uses capacity for the work that is executed, and provides controls for who can access the items in it. For example, in a sales workspace, users associated with the sales organization can create a data warehouse, run notebooks, create datasets, create reports, and more. Items are the building blocks of the Fabric platform. They're the objects that you create and manage in Fabric. There are different types of items, such as data warehouses, data pipelines, datasets, reports, and dashboards. Understanding Fabric concepts is important for you as an admin, because it helps you understand how to manage the Fabric environment. Note For more information, see the Start a Fabric trial documentation.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/fabric-trial"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/administer-fabric/media/fabric-overview.png",
                "image_alt": "Diagram of Fabric architecture, show with OneLake as the foundation, with each experience built on top."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Administer a Microsoft Fabric environment",
        "topic": "Understand the Fabric administrator role",
        "text": "Now that you understand the Fabric architecture and what you and your team might use Fabric for, let's look at the admin role and the tools used to manage the platform. There are several roles that work together to administer Microsoft Fabric for your organization. If you're a Microsoft 365 admin, a Power Platform admin, or a Fabric capacity admin, you're involved in administering Fabric. The Fabric admin role was formerly known as Power BI admin. As a Fabric admin, you work primarily in the Fabric admin portal. You might also need to familiarize yourself with the following tools: Microsoft 365 admin center Microsoft 365 Security & Microsoft Purview compliance portal Microsoft Entra ID in the Azure portal PowerShell cmdlets Administrative APIs and SDK Note For specific details on the different admin roles and responsibilities, see the What is Microsoft Fabric administration? documentation. Describe admin tasks As an admin, you might be responsible for a wide range of tasks to keep the Fabric platform running smoothly. These tasks include: Security and access control : One of the most important aspects of Fabric administration is managing security and access control to ensure that only authorized users can access sensitive data. You can use role-based access control (RBAC) to: Define who can view and edit content. Set up data gateways to securely connect to on-premises data sources. Manage user access with Microsoft Entra ID. Data governance : Effective Fabric administration requires a solid understanding of data governance principles. You should know how to secure inbound and outbound connectivity in your tenant and how to monitor usage and performance metrics. You should also know how to apply data governance policies to ensure data within your tenant is only accessible to authorized users. Customization and configuration : Fabric administration also involves customizing and configuring the platform to meet the needs of your organization. You might configure private links to secure your tenant, define data classification policies, or adjust the look and feel of reports and dashboards. Monitoring and optimization : As a Fabric admin, you need to know how to monitor the performance and usage of the platform, optimize resources, and troubleshoot issues. Examples include configuring monitoring and alerting settings, optimizing query performance, managing capacity and scaling, and troubleshooting data refresh and connectivity issues. Specific tasks vary depending on the needs of your organization and the complexity of your Fabric implementation. Describe admin tools It's important to familiarize yourself with a few tools to effectively implement the tasks previously outlined. Fabric admins can perform most admin tasks using one or more of the following tools: the Fabric admin portal, PowerShell cmdlets, admin APIs and SDKs, and the admin monitoring workspace. Fabric admin portal Fabric's admin portal is a web-based portal where you can manage all aspects of the platform. You can centrally manage, review, and apply settings for the entire tenant or by capacity in the admin portal. You can also manage users, admins and groups, access audit logs, and monitor usage and performance. The admin portal enables you to turn settings on and off. There are many settings located in the admin portal. One noteworthy setting is the Fabric on/off switch, located in tenant settings that lets organizations that use Power BI opt into Fabric. Here, you can enable Fabric for your tenant or allow capacity admins to enable Fabric. PowerShell cmdlets Fabric provides a set of PowerShell cmdlets that you can use to automate common administrative tasks. A PowerShell cmdlet is a simple command that can be executed in PowerShell. For example, you can use cmdlets in Fabric to systematically create and manage groups, configure data sources and gateways, and monitor usage and performance. You can also use the cmdlets to manage the Fabric admin APIs and SDKs. Note See Microsoft Power BI Cmdlets for Windows PowerShell and PowerShell Core for more resources on PowerShell cmdlets that work with Fabric. Admin APIs and SDKs An admin API and SDK are tools that allow developers to interact with a software system programmatically. An API (Application Programming Interface) is a set of protocols and tools that enable communication between different software applications. An SDK (Software Development Kit) is a set of tools and libraries that helps developers create software applications that can interact with a specific system or platform. You can use APIs and SDKs to automate common administrative tasks and integrate Fabric with other systems. For example, you can use APIs and SDKs to create and manage groups, configure data sources and gateways, and monitor usage and performance. You can also use the APIs and SDKs to manage the Fabric admin APIs and SDKs. You can make these requests using any HTTP client library that supports OAuth 2.0 authentication, such as Postman, or you can use PowerShell scripts to automate the process. Admin monitoring workspace Fabric tenant admins have access to the admin monitoring workspace . You can choose to share access to the workspace or specific items within it with other users in your organization. The admin monitoring workspace includes the Feature Usage and Adoption dataset and report, which together provide insights on the usage and performance of your Fabric environment. You can use this information to identify trends and patterns, and troubleshoot issues. Note For more information about what is included in the Admin Monitoring Workspace, see What is the Fabric admin monitoring workspace .",
        "links": [
            "https://learn.microsoft.com/en-us/microsoft-365/admin/admin-overview/admin-center-overview",
            "https://learn.microsoft.com/en-us/microsoft-365/compliance/microsoft-365-compliance-center",
            "https://learn.microsoft.com/en-us/azure/active-directory/fundamentals/active-directory-whatis",
            "https://learn.microsoft.com/en-us/powershell/power-bi/overview",
            "https://learn.microsoft.com/en-us/rest/api/power-bi/admin",
            "https://learn.microsoft.com/en-us/fabric/admin/microsoft-fabric-admin",
            "https://learn.microsoft.com/en-us/powershell/scripting/powershell-commands",
            "https://learn.microsoft.com/en-us/powershell/power-bi/overview",
            "https://learn.microsoft.com/en-us/fabric/admin/monitoring-workspace"
        ],
        "images": [
            {
                "image_name": "image1",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/administer-fabric/media/admin-delegation.png",
                "image_alt": "Screenshot of Tenant settings in admin portal."
            },
            {
                "image_name": "image2",
                "image_src": "https://learn.microsoft.com/en-us/training/wwl/administer-fabric/media/admin-monitoring-report.png",
                "image_alt": "Screenshot of a report in the Admin monitoring workspace."
            }
        ],
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Administer a Microsoft Fabric environment",
        "topic": "Manage Fabric security",
        "text": "As a Fabric admin, part of your role is to manage security for the Fabric environment, including managing users and groups, and how users share and distribute content in Fabric. Manage users: assign and manage licenses User licenses control the level of user access and functionality within the Fabric environment. Administrators ensure licensed users have the access they need to data and analytics to do their jobs effectively. They also limit access to sensitive data and ensure compliance with data protection laws and regulations. Managing licenses allows administrators to monitor and control costs by ensuring that licenses are allocated efficiently and only to users who need them. This can help to prevent unnecessary expenses and ensure that the organization is utilizing its resources effectively. Having the appropriate procedures in place to assign and manage licenses helps to control access to data and analytics, ensure compliance with regulations, and optimize costs. License management for Fabric is handled in the Microsoft 365 admin center. For more information about managing licenses, see Assign licenses to users . Note The license type in workspace settings is related to the user licenses listed here. Users can see reports depending on the user license and the workspace license. For detailed information, see the Microsoft Fabric licenses documentation. Manage items and sharing As an admin, you can manage how users share and distribute content. You can manage how users share content with others, and how they distribute content to others. You can also manage how users interact with items, such as data warehouses, data pipelines, datasets, reports, and dashboards. Items in workspaces are best distributed through a workspace app or the workspace directly. Granting the least permissive rights is the first step in securing the data. Share the read only app for access to the reports or grant access to the workspaces for collaboration and development. Another aspect of managing and distributing items is enforcing these types of best practices. You can manage sharing and distribution both internally and outside of your organization, in compliance with your organization's policies and procedures. Note For more information, see the Security in Microsoft Fabric documentation.",
        "links": [
            "https://learn.microsoft.com/en-us/microsoft-365/admin/manage/assign-licenses-to-users?view=o365-worldwide&preserve-view=true",
            "https://learn.microsoft.com/en-us/fabric/enterprise/licenses#workspace",
            "https://learn.microsoft.com/en-us/fabric/security/security-overview"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Administer a Microsoft Fabric environment",
        "topic": "Govern data in Fabric",
        "text": "Fabric includes built-in governance features to help you manage and control your data. Endorsement is a way for you as an admin to designate specific Fabric items as trusted and approved for use across the organization. Admins can also make use of the scanner API to scan Fabric items for sensitive data, and the data lineage feature to track the flow of data through Fabric. Endorse Fabric content Endorsement is a key governance feature that builds trust in your data assets by marking Fabric items as reviewed and approved. Endorsed items display a badge, signaling to users that these assets are reliable. Endorsement helps users trust the data, and it also helps you as an admin manage the overall growth of items across your environment. Promoted Fabric content appears with a Promoted badge in the Fabric portal. Workspace members with the contributor or admin role can promote content within a workspace. The Fabric admin can promote content across the organization. Certified content requires a more formal process that involves a review of the content by a designated reviewer. Content appears with a Certified badge in the Fabric portal. Admins manage the certification process and can customize it to meet the needs of your organization. If you aren't an admin, you need to request item certification from an admin. You can perform request certification by selecting the item in the Fabric portal, and then selecting Request certification from the More menu. Note For more detailed information on the content endorsement process, see Promote or certify content . Scan for sensitive data Metadata scanning facilitates governance of data by enabling cataloging and reporting on all the metadata of your organization's Fabric items. The scanner API is a set of Admin REST APIs that allows you to scan Fabric items for sensitive data. Use the scanner API to scan data warehouses, data pipelines, datasets, reports, and dashboards for sensitive data. The scanner API can be used to scan both structured and unstructured data. Important Before metadata scanning can be run, it needs to be set up in your organization by an Admin. For more information, see the Metadata scanning overview . Track data lineage Data lineage is the ability to track the flow of data through Fabric, also known as impact analysis . Data lineage allows you to see where data comes from, how it's transformed, and where it goes. The lineage view in workspaces helps you understand the data that is available in Fabric, and how it's being used. Report on sensitive data With the Microsoft Purview hub (preview) in Fabric, you can manage and govern your organization's Fabric data estate. It contains reports that provide insights about sensitive data, item endorsement, and domains, and also serves as a gateway to more advanced capabilities in the Microsoft Purview portal such as Data Catalog, Information Protection, Data Loss Prevention, and Audit.",
        "links": [
            "https://learn.microsoft.com/en-us/fabric/get-started/endorsement-promote-certify",
            "https://learn.microsoft.com/en-us/fabric/governance/metadata-scanning-overview"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    },
    {
        "course": "Microsoft Fabric Data Engineer",
        "module": "Manage a Microsoft Fabric environment",
        "unit": "Administer a Microsoft Fabric environment",
        "topic": "Summary",
        "text": "In this module, you learned about the Fabric architecture and the role of an administrator for the Fabric platform. You also explored the different tools available for managing security and sharing, as well as the governance features that can be used to enforce standards and ensure compliance. Your understanding of how to manage a Fabric environment ensures that it's secure, compliant, and well-governed. With this knowledge, you're well-equipped to help your organization get the most out of Fabric and derive valuable insights from all your data. For more information about data governance, complete the Govern data in Microsoft Fabric with Purview module.",
        "links": [
            "https://learn.microsoft.com/en-us/training/modules/fabric-data-governance-purview/"
        ],
        "images": null,
        "videos": null,
        "exercise": null
    }
]