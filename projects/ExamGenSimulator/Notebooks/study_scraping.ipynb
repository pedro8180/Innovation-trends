{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23822fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\elit.s.acosta\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\elit.s.acosta\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\elit.s.acosta\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp313-cp313-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 3.4/11.0 MB 17.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.6/11.0 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.7/11.0 MB 18.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 16.9 MB/s  0:00:00\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   ---------------------------------------- 3/3 [pandas]\n",
      "\n",
      "Successfully installed pandas-2.3.3 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Bibliotecas importadas exitosamente\n",
      "Agente de Web Scraping AI-102 iniciado\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Bibliotecas importadas exitosamente\")\n",
    "print(\"Agente de Web Scraping AI-102 iniciado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef36486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración de web scraping lista\n",
      "Usando ScrapingAnt API: True\n",
      "URLs objetivo configuradas:\n",
      "  - Microsoft Study Guide: https://learn.microsoft.com/en-us/credentials/certifications/resources/study-guides/ai-102\n",
      "  - Whizlabs Questions: https://www.whizlabs.com/blog/ai-102-exam-questions/\n"
     ]
    }
   ],
   "source": [
    "class WebScrapingConfig:\n",
    "    def __init__(self):\n",
    "        # Headers para simular un navegador real\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "        \n",
    "        # URLs objetivo\n",
    "        self.microsoft_study_guide_url = \"https://learn.microsoft.com/en-us/credentials/certifications/resources/study-guides/ai-102\"\n",
    "        self.whizlabs_questions_url = \"https://www.whizlabs.com/blog/ai-102-exam-questions/\"\n",
    "        \n",
    "        # Configuración de rate limiting (ser respetuoso)\n",
    "        self.delay_between_requests = 2  # segundos\n",
    "        self.max_retries = 3\n",
    "        self.timeout = 30\n",
    "        \n",
    "        # Usar ScrapingAnt API si está disponible (más confiable)\n",
    "        self.scraping_api_key = os.getenv('SCRAPPING_TOOL_API_KEY')\n",
    "        self.use_scraping_api = bool(self.scraping_api_key)\n",
    "        \n",
    "    def get_session(self):\n",
    "        \"\"\"Crear sesión HTTP configurada\"\"\"\n",
    "        session = requests.Session()\n",
    "        session.headers.update(self.headers)\n",
    "        return session\n",
    "\n",
    "# Inicializar configuración\n",
    "config = WebScrapingConfig()\n",
    "print(f\"Configuración de web scraping lista\")\n",
    "print(f\"Usando ScrapingAnt API: {config.use_scraping_api}\")\n",
    "print(f\"URLs objetivo configuradas:\")\n",
    "print(f\"  - Microsoft Study Guide: {config.microsoft_study_guide_url}\")\n",
    "print(f\"  - Whizlabs Questions: {config.whizlabs_questions_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf4c940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraper de Microsoft Study Guide inicializado\n"
     ]
    }
   ],
   "source": [
    "# Scraper para Microsoft Study Guide AI-102\n",
    "class MicrosoftStudyGuideScraper:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.session = config.get_session()\n",
    "        self.study_guide_data = {\n",
    "            \"source\": \"Microsoft Learn AI-102 Study Guide\",\n",
    "            \"url\": config.microsoft_study_guide_url,\n",
    "            \"scraped_at\": datetime.now().isoformat(),\n",
    "            \"exam_objectives\": [],\n",
    "            \"skill_areas\": [],\n",
    "            \"study_topics\": [],\n",
    "            \"raw_content\": \"\"\n",
    "        }\n",
    "    \n",
    "    def scrape_with_api(self, url):\n",
    "        \"\"\"Usar ScrapingAnt API para scraping confiable\"\"\"\n",
    "        if not self.config.scraping_api_key:\n",
    "            return None\n",
    "            \n",
    "        endpoint = 'https://api.scrapingant.com/v2/general'\n",
    "        params = {\n",
    "            'url': url,\n",
    "            'x-api-key': self.config.scraping_api_key,\n",
    "            'browser': False\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(endpoint, params=params, timeout=self.config.timeout)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error usando ScrapingAnt API: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def scrape_direct(self, url):\n",
    "        \"\"\"Scraping directo como respaldo\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=self.config.timeout)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error en scraping directo: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_study_content(self, html_content):\n",
    "        \"\"\"Extraer contenido relevante del HTML\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Extraer objetivos del examen\n",
    "        objectives = []\n",
    "        \n",
    "        # Buscar secciones con objetivos del examen\n",
    "        objective_patterns = [\n",
    "            {'tag': 'h2', 'text_contains': ['objective', 'skill', 'area']},\n",
    "            {'tag': 'h3', 'text_contains': ['objective', 'skill', 'area']},\n",
    "            {'class': ['objective', 'skill-area', 'exam-objective']}\n",
    "        ]\n",
    "        \n",
    "        for pattern in objective_patterns:\n",
    "            if 'tag' in pattern and 'text_contains' in pattern:\n",
    "                headers = soup.find_all(pattern['tag'])\n",
    "                for header in headers:\n",
    "                    if header.text and any(keyword in header.text.lower() for keyword in pattern['text_contains']):\n",
    "                        objective_content = self._extract_section_content(header)\n",
    "                        if objective_content:\n",
    "                            objectives.append({\n",
    "                                \"title\": header.text.strip(),\n",
    "                                \"content\": objective_content,\n",
    "                                \"level\": pattern['tag']\n",
    "                            })\n",
    "            \n",
    "            elif 'class' in pattern:\n",
    "                elements = soup.find_all(class_=pattern['class'])\n",
    "                for element in elements:\n",
    "                    if element.text.strip():\n",
    "                        objectives.append({\n",
    "                            \"title\": \"Exam Objective\",\n",
    "                            \"content\": element.text.strip(),\n",
    "                            \"level\": \"class-based\"\n",
    "                        })\n",
    "        \n",
    "        # Extraer listas de temas de estudio\n",
    "        study_topics = []\n",
    "        for ul in soup.find_all('ul'):\n",
    "            if ul.find_parent(['div', 'section']) and len(ul.find_all('li')) > 2:\n",
    "                topics = [li.text.strip() for li in ul.find_all('li') if li.text.strip()]\n",
    "                if topics:\n",
    "                    study_topics.extend(topics)\n",
    "        \n",
    "        # Extraer texto general relevante\n",
    "        paragraphs = []\n",
    "        for p in soup.find_all('p'):\n",
    "            text = p.text.strip()\n",
    "            if len(text) > 50 and any(keyword in text.lower() for keyword in ['ai', 'azure', 'cognitive', 'machine learning', 'openai']):\n",
    "                paragraphs.append(text)\n",
    "        \n",
    "        return {\n",
    "            \"objectives\": objectives,\n",
    "            \"study_topics\": study_topics,\n",
    "            \"paragraphs\": paragraphs[:10]  # Limitar a los más relevantes\n",
    "        }\n",
    "    \n",
    "    def _extract_section_content(self, header):\n",
    "        \"\"\"Extraer contenido después de un header\"\"\"\n",
    "        content = []\n",
    "        current = header.next_sibling\n",
    "        \n",
    "        while current and current != header.find_next_sibling(['h1', 'h2', 'h3']):\n",
    "            if hasattr(current, 'text') and current.text.strip():\n",
    "                content.append(current.text.strip())\n",
    "            current = current.next_sibling\n",
    "        \n",
    "        return ' '.join(content)\n",
    "    \n",
    "    def scrape(self):\n",
    "        \"\"\"Ejecutar scraping completo de Microsoft Study Guide\"\"\"\n",
    "        logger.info(\"Iniciando scraping de Microsoft Study Guide...\")\n",
    "        \n",
    "        # Intentar con API primero, luego scraping directo\n",
    "        html_content = None\n",
    "        if self.config.use_scraping_api:\n",
    "            html_content = self.scrape_with_api(self.config.microsoft_study_guide_url)\n",
    "        \n",
    "        if not html_content:\n",
    "            time.sleep(self.config.delay_between_requests)\n",
    "            html_content = self.scrape_direct(self.config.microsoft_study_guide_url)\n",
    "        \n",
    "        if not html_content:\n",
    "            raise Exception(\"No se pudo obtener contenido de Microsoft Study Guide\")\n",
    "        \n",
    "        # Extraer contenido estructurado\n",
    "        extracted_content = self.extract_study_content(html_content)\n",
    "        \n",
    "        # Actualizar datos\n",
    "        self.study_guide_data.update({\n",
    "            \"exam_objectives\": extracted_content[\"objectives\"],\n",
    "            \"study_topics\": extracted_content[\"study_topics\"],\n",
    "            \"raw_content\": extracted_content[\"paragraphs\"]\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"Scraping completado. Objetivos encontrados: {len(extracted_content['objectives'])}\")\n",
    "        logger.info(f\"Temas de estudio encontrados: {len(extracted_content['study_topics'])}\")\n",
    "        \n",
    "        return self.study_guide_data\n",
    "\n",
    "# Inicializar scraper de Microsoft\n",
    "microsoft_scraper = MicrosoftStudyGuideScraper(config)\n",
    "print(\"Scraper de Microsoft Study Guide inicializado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c6beb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraper de Whizlabs Practice Questions inicializado\n"
     ]
    }
   ],
   "source": [
    "# Scraper para Whizlabs Practice Questions\n",
    "class WhizlabsQuestionScraper:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.session = config.get_session()\n",
    "        self.questions_data = {\n",
    "            \"source\": \"Whizlabs AI-102 Practice Questions\",\n",
    "            \"url\": config.whizlabs_questions_url,\n",
    "            \"scraped_at\": datetime.now().isoformat(),\n",
    "            \"practice_questions\": [],\n",
    "            \"exam_tips\": [],\n",
    "            \"study_recommendations\": []\n",
    "        }\n",
    "    \n",
    "    def scrape_questions_content(self, url):\n",
    "        \"\"\"Scraping específico para contenido de Whizlabs\"\"\"\n",
    "        # Usar API si está disponible\n",
    "        if self.config.use_scraping_api:\n",
    "            html_content = self._scrape_with_api(url)\n",
    "        else:\n",
    "            html_content = self._scrape_direct(url)\n",
    "        \n",
    "        if not html_content:\n",
    "            return None\n",
    "        \n",
    "        return self._extract_questions_content(html_content)\n",
    "    \n",
    "    def _scrape_with_api(self, url):\n",
    "        \"\"\"Usar ScrapingAnt API\"\"\"\n",
    "        if not self.config.scraping_api_key:\n",
    "            return None\n",
    "            \n",
    "        endpoint = 'https://api.scrapingant.com/v2/general'\n",
    "        params = {\n",
    "            'url': url,\n",
    "            'x-api-key': self.config.scraping_api_key,\n",
    "            'browser': True  \n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(endpoint, params=params, timeout=self.config.timeout)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error usando ScrapingAnt API para Whizlabs: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _scrape_direct(self, url):\n",
    "        \"\"\"Scraping directo\"\"\"\n",
    "        try:\n",
    "            time.sleep(self.config.delay_between_requests)\n",
    "            response = self.session.get(url, timeout=self.config.timeout)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error en scraping directo de Whizlabs: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _extract_questions_content(self, html_content):\n",
    "        \"\"\"Extraer preguntas y contenido relevante\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        questions = []\n",
    "        exam_tips = []\n",
    "        \n",
    "        # Buscar preguntas de práctica (patrones comunes en blogs)\n",
    "        question_patterns = [\n",
    "            {'tag': 'h3', 'text_contains': ['question', 'pregunta', 'q.', 'q:']},\n",
    "            {'tag': 'h4', 'text_contains': ['question', 'pregunta', 'q.', 'q:']},\n",
    "            {'class': ['question', 'practice-question', 'exam-question']}\n",
    "        ]\n",
    "        \n",
    "        # Extraer preguntas estructuradas\n",
    "        for pattern in question_patterns:\n",
    "            if 'tag' in pattern:\n",
    "                elements = soup.find_all(pattern['tag'])\n",
    "                for element in elements:\n",
    "                    if element.text and any(keyword in element.text.lower() for keyword in pattern['text_contains']):\n",
    "                        question_data = self._parse_question_block(element)\n",
    "                        if question_data:\n",
    "                            questions.append(question_data)\n",
    "        \n",
    "        # Buscar tips y consejos de examen\n",
    "        tip_patterns = ['tip', 'advice', 'recommendation', 'important', 'note']\n",
    "        for element in soup.find_all(['div', 'p', 'li']):\n",
    "            text = element.text.strip()\n",
    "            if text and len(text) > 30:\n",
    "                if any(tip in text.lower() for tip in tip_patterns):\n",
    "                    exam_tips.append(text)\n",
    "        \n",
    "        # Extraer opciones múltiples (A, B, C, D)\n",
    "        multiple_choice_questions = self._extract_multiple_choice(soup)\n",
    "        questions.extend(multiple_choice_questions)\n",
    "        \n",
    "        return {\n",
    "            \"questions\": questions,\n",
    "            \"exam_tips\": exam_tips[:10],  # Limitar a 10 tips más relevantes\n",
    "        }\n",
    "    \n",
    "    def _parse_question_block(self, question_element):\n",
    "        \"\"\"Parsear un bloque de pregunta completo\"\"\"\n",
    "        question_text = question_element.text.strip()\n",
    "        \n",
    "        # Buscar opciones de respuesta cerca de la pregunta\n",
    "        options = []\n",
    "        correct_answer = None\n",
    "        explanation = None\n",
    "        \n",
    "        # Buscar en elementos siguientes\n",
    "        current = question_element.next_sibling\n",
    "        for _ in range(10):  # Buscar en los próximos 10 elementos\n",
    "            if not current:\n",
    "                break\n",
    "                \n",
    "            if hasattr(current, 'text'):\n",
    "                text = current.text.strip()\n",
    "                \n",
    "                # Detectar opciones (A), B), C), D)\n",
    "                if re.match(r'^[A-D]\\)', text) or re.match(r'^[A-D]\\.', text):\n",
    "                    options.append(text)\n",
    "                \n",
    "                # Detectar respuesta correcta\n",
    "                if 'correct' in text.lower() or 'answer' in text.lower():\n",
    "                    correct_answer = text\n",
    "                \n",
    "                # Detectar explicación\n",
    "                if 'explanation' in text.lower() or 'because' in text.lower():\n",
    "                    explanation = text\n",
    "            \n",
    "            current = current.next_sibling\n",
    "        \n",
    "        if question_text and len(options) >= 2:\n",
    "            return {\n",
    "                \"question\": question_text,\n",
    "                \"options\": options,\n",
    "                \"correct_answer\": correct_answer,\n",
    "                \"explanation\": explanation\n",
    "            }\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _extract_multiple_choice(self, soup):\n",
    "        \"\"\"Extraer preguntas de opción múltiple usando patrones\"\"\"\n",
    "        questions = []\n",
    "        \n",
    "        # Buscar patrones de preguntas con opciones A, B, C, D\n",
    "        text_content = soup.get_text()\n",
    "        \n",
    "        # Patrón para detectar preguntas seguidas de opciones\n",
    "        question_pattern = r'(\\d+\\.\\s*.+?\\?)\\s*([A-D]\\)\\s*.+?)([A-D]\\)\\s*.+?)([A-D]\\)\\s*.+?)(?:[A-D]\\)\\s*.+?)?'\n",
    "        matches = re.findall(question_pattern, text_content, re.DOTALL)\n",
    "        \n",
    "        for match in matches:\n",
    "            if len(match) >= 4:\n",
    "                question_data = {\n",
    "                    \"question\": match[0].strip(),\n",
    "                    \"options\": [opt.strip() for opt in match[1:4] if opt.strip()],\n",
    "                    \"correct_answer\": None,\n",
    "                    \"explanation\": None\n",
    "                }\n",
    "                questions.append(question_data)\n",
    "        \n",
    "        return questions\n",
    "    \n",
    "    def scrape(self):\n",
    "        \"\"\"Ejecutar scraping completo de Whizlabs\"\"\"\n",
    "        logger.info(\"Iniciando scraping de Whizlabs Practice Questions...\")\n",
    "        \n",
    "        content = self.scrape_questions_content(self.config.whizlabs_questions_url)\n",
    "        \n",
    "        if not content:\n",
    "            raise Exception(\"No se pudo obtener contenido de Whizlabs\")\n",
    "        \n",
    "        # Actualizar datos\n",
    "        self.questions_data.update({\n",
    "            \"practice_questions\": content[\"questions\"],\n",
    "            \"exam_tips\": content[\"exam_tips\"]\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"Scraping completado. Preguntas encontradas: {len(content['questions'])}\")\n",
    "        logger.info(f\"Tips de examen encontrados: {len(content['exam_tips'])}\")\n",
    "        \n",
    "        return self.questions_data\n",
    "\n",
    "# Inicializar scraper de Whizlabs\n",
    "whizlabs_scraper = WhizlabsQuestionScraper(config)\n",
    "print(\"Scraper de Whizlabs Practice Questions inicializado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cff473c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesador de datos inicializado\n"
     ]
    }
   ],
   "source": [
    "# Limpieza y Procesamiento de Datos\n",
    "class DataProcessor:\n",
    "    def __init__(self):\n",
    "        self.processed_data = {\n",
    "            \"microsoft_data\": None,\n",
    "            \"whizlabs_data\": None,\n",
    "            \"combined_data\": None\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Limpiar y normalizar texto\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Remover HTML tags residuales\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        \n",
    "        # Normalizar espacios en blanco\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remover caracteres especiales problemáticos\n",
    "        text = re.sub(r'[^\\w\\s\\-\\.\\,\\?\\!\\:\\;\\(\\)\\\"\\'\\/]', '', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def process_microsoft_data(self, microsoft_data):\n",
    "        \"\"\"Procesar datos de Microsoft Study Guide\"\"\"\n",
    "        processed = {\n",
    "            \"source\": microsoft_data[\"source\"],\n",
    "            \"processed_at\": datetime.now().isoformat(),\n",
    "            \"exam_objectives\": [],\n",
    "            \"key_topics\": [],\n",
    "            \"study_areas\": []\n",
    "        }\n",
    "        \n",
    "        # Procesar objetivos del examen\n",
    "        for objective in microsoft_data.get(\"exam_objectives\", []):\n",
    "            clean_objective = {\n",
    "                \"title\": self.clean_text(objective.get(\"title\", \"\")),\n",
    "                \"content\": self.clean_text(objective.get(\"content\", \"\")),\n",
    "                \"importance\": self._assess_importance(objective.get(\"content\", \"\"))\n",
    "            }\n",
    "            if clean_objective[\"title\"] or clean_objective[\"content\"]:\n",
    "                processed[\"exam_objectives\"].append(clean_objective)\n",
    "        \n",
    "        # Procesar temas de estudio\n",
    "        for topic in microsoft_data.get(\"study_topics\", []):\n",
    "            clean_topic = self.clean_text(topic)\n",
    "            if clean_topic and len(clean_topic) > 10:\n",
    "                processed[\"key_topics\"].append({\n",
    "                    \"topic\": clean_topic,\n",
    "                    \"category\": self._categorize_topic(clean_topic)\n",
    "                })\n",
    "        \n",
    "        # Extraer áreas de estudio de contenido\n",
    "        for paragraph in microsoft_data.get(\"raw_content\", []):\n",
    "            clean_paragraph = self.clean_text(paragraph)\n",
    "            if clean_paragraph and len(clean_paragraph) > 50:\n",
    "                processed[\"study_areas\"].append(clean_paragraph)\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def process_whizlabs_data(self, whizlabs_data):\n",
    "        \"\"\"Procesar datos de Whizlabs\"\"\"\n",
    "        processed = {\n",
    "            \"source\": whizlabs_data[\"source\"],\n",
    "            \"processed_at\": datetime.now().isoformat(),\n",
    "            \"practice_questions\": [],\n",
    "            \"question_patterns\": [],\n",
    "            \"exam_insights\": []\n",
    "        }\n",
    "        \n",
    "        # Procesar preguntas de práctica\n",
    "        for question in whizlabs_data.get(\"practice_questions\", []):\n",
    "            clean_question = {\n",
    "                \"question\": self.clean_text(question.get(\"question\", \"\")),\n",
    "                \"options\": [self.clean_text(opt) for opt in question.get(\"options\", [])],\n",
    "                \"correct_answer\": self.clean_text(question.get(\"correct_answer\", \"\")),\n",
    "                \"explanation\": self.clean_text(question.get(\"explanation\", \"\")),\n",
    "                \"topic_area\": self._identify_topic_area(question.get(\"question\", \"\"))\n",
    "            }\n",
    "            \n",
    "            if clean_question[\"question\"] and len(clean_question[\"options\"]) >= 2:\n",
    "                processed[\"practice_questions\"].append(clean_question)\n",
    "        \n",
    "        # Procesar tips de examen\n",
    "        for tip in whizlabs_data.get(\"exam_tips\", []):\n",
    "            clean_tip = self.clean_text(tip)\n",
    "            if clean_tip and len(clean_tip) > 20:\n",
    "                processed[\"exam_insights\"].append({\n",
    "                    \"insight\": clean_tip,\n",
    "                    \"category\": self._categorize_insight(clean_tip)\n",
    "                })\n",
    "        \n",
    "        # Identificar patrones de preguntas\n",
    "        processed[\"question_patterns\"] = self._analyze_question_patterns(processed[\"practice_questions\"])\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def _assess_importance(self, content):\n",
    "        \"\"\"Evaluar la importancia de un objetivo\"\"\"\n",
    "        importance_keywords = {\n",
    "            \"high\": [\"implement\", \"design\", \"develop\", \"configure\", \"manage\"],\n",
    "            \"medium\": [\"understand\", \"explain\", \"describe\", \"identify\"],\n",
    "            \"low\": [\"list\", \"name\", \"define\"]\n",
    "        }\n",
    "        \n",
    "        content_lower = content.lower()\n",
    "        for level, keywords in importance_keywords.items():\n",
    "            if any(keyword in content_lower for keyword in keywords):\n",
    "                return level\n",
    "        return \"medium\"\n",
    "    \n",
    "    def _categorize_topic(self, topic):\n",
    "        \"\"\"Categorizar temas de estudio\"\"\"\n",
    "        categories = {\n",
    "            \"Azure OpenAI\": [\"openai\", \"gpt\", \"completion\", \"embedding\"],\n",
    "            \"Computer Vision\": [\"vision\", \"image\", \"ocr\", \"face\", \"object detection\"],\n",
    "            \"Speech Services\": [\"speech\", \"text-to-speech\", \"speech-to-text\", \"voice\"],\n",
    "            \"Language Understanding\": [\"luis\", \"language\", \"intent\", \"entity\", \"nlp\"],\n",
    "            \"Bot Framework\": [\"bot\", \"conversation\", \"dialog\", \"channel\"],\n",
    "            \"Cognitive Services\": [\"cognitive\", \"api\", \"endpoint\", \"key\"]\n",
    "        }\n",
    "        \n",
    "        topic_lower = topic.lower()\n",
    "        for category, keywords in categories.items():\n",
    "            if any(keyword in topic_lower for keyword in keywords):\n",
    "                return category\n",
    "        return \"General\"\n",
    "    \n",
    "    def _identify_topic_area(self, question):\n",
    "        \"\"\"Identificar área temática de uma pregunta\"\"\"\n",
    "        return self._categorize_topic(question)\n",
    "    \n",
    "    def _categorize_insight(self, insight):\n",
    "        \"\"\"Categorizar insights de examen\"\"\"\n",
    "        if any(keyword in insight.lower() for keyword in [\"time\", \"manage\", \"strategy\"]):\n",
    "            return \"Exam Strategy\"\n",
    "        elif any(keyword in insight.lower() for keyword in [\"practice\", \"study\", \"prepare\"]):\n",
    "            return \"Study Tips\"\n",
    "        elif any(keyword in insight.lower() for keyword in [\"azure\", \"service\", \"api\"]):\n",
    "            return \"Technical Tips\"\n",
    "        return \"General\"\n",
    "    \n",
    "    def _analyze_question_patterns(self, questions):\n",
    "        \"\"\"Analizar patrones en las preguntas\"\"\"\n",
    "        patterns = {\n",
    "            \"question_types\": {},\n",
    "            \"common_topics\": {},\n",
    "            \"answer_patterns\": {}\n",
    "        }\n",
    "        \n",
    "        for question in questions:\n",
    "            # Tipo de pregunta\n",
    "            q_text = question.get(\"question\", \"\").lower()\n",
    "            if \"which\" in q_text:\n",
    "                patterns[\"question_types\"][\"which\"] = patterns[\"question_types\"].get(\"which\", 0) + 1\n",
    "            elif \"what\" in q_text:\n",
    "                patterns[\"question_types\"][\"what\"] = patterns[\"question_types\"].get(\"what\", 0) + 1\n",
    "            elif \"how\" in q_text:\n",
    "                patterns[\"question_types\"][\"how\"] = patterns[\"question_types\"].get(\"how\", 0) + 1\n",
    "            \n",
    "            # Topics comunes\n",
    "            topic = question.get(\"topic_area\", \"Unknown\")\n",
    "            patterns[\"common_topics\"][topic] = patterns[\"common_topics\"].get(topic, 0) + 1\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def combine_data_sources(self, microsoft_processed, whizlabs_processed):\n",
    "        \"\"\"Combinar datos de ambas fuentes\"\"\"\n",
    "        combined = {\n",
    "            \"combined_at\": datetime.now().isoformat(),\n",
    "            \"sources\": [\n",
    "                microsoft_processed[\"source\"],\n",
    "                whizlabs_processed[\"source\"]\n",
    "            ],\n",
    "            \"comprehensive_study_guide\": {\n",
    "                \"official_objectives\": microsoft_processed[\"exam_objectives\"],\n",
    "                \"key_study_topics\": microsoft_processed[\"key_topics\"],\n",
    "                \"practice_questions\": whizlabs_processed[\"practice_questions\"],\n",
    "                \"exam_insights\": whizlabs_processed[\"exam_insights\"],\n",
    "                \"question_patterns\": whizlabs_processed[\"question_patterns\"]\n",
    "            },\n",
    "            \"topic_coverage\": self._analyze_topic_coverage(microsoft_processed, whizlabs_processed),\n",
    "            \"recommendations\": self._generate_recommendations(microsoft_processed, whizlabs_processed)\n",
    "        }\n",
    "        \n",
    "        return combined\n",
    "    \n",
    "    def _analyze_topic_coverage(self, microsoft_data, whizlabs_data):\n",
    "        \"\"\"Analizar cobertura de temas entre fuentes\"\"\"\n",
    "        microsoft_topics = set()\n",
    "        for topic in microsoft_data[\"key_topics\"]:\n",
    "            microsoft_topics.add(topic[\"category\"])\n",
    "        \n",
    "        whizlabs_topics = set()\n",
    "        for question in whizlabs_data[\"practice_questions\"]:\n",
    "            whizlabs_topics.add(question[\"topic_area\"])\n",
    "        \n",
    "        return {\n",
    "            \"microsoft_only\": list(microsoft_topics - whizlabs_topics),\n",
    "            \"whizlabs_only\": list(whizlabs_topics - microsoft_topics),\n",
    "            \"common_topics\": list(microsoft_topics & whizlabs_topics),\n",
    "            \"coverage_percentage\": len(microsoft_topics & whizlabs_topics) / len(microsoft_topics | whizlabs_topics) * 100 if microsoft_topics | whizlabs_topics else 0\n",
    "        }\n",
    "    \n",
    "    def _generate_recommendations(self, microsoft_data, whizlabs_data):\n",
    "        \"\"\"Generar recomendaciones para el generador de preguntas\"\"\"\n",
    "        return {\n",
    "            \"focus_areas\": [topic[\"category\"] for topic in microsoft_data[\"key_topics\"][:5]],\n",
    "            \"question_types_priority\": [\"which\", \"what\", \"how\"],\n",
    "            \"high_importance_objectives\": [obj for obj in microsoft_data[\"exam_objectives\"] if obj[\"importance\"] == \"high\"],\n",
    "            \"suggested_question_count\": min(50, len(whizlabs_data[\"practice_questions\"]) * 2)\n",
    "        }\n",
    "\n",
    "# Inicializar procesador de datos\n",
    "data_processor = DataProcessor()\n",
    "print(\"Procesador de datos inicializado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe54d5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 11:33:39,599 - INFO - Iniciando scraping de Microsoft Study Guide...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando web scraping de fuentes AI-102...\n",
      "INICIANDO PROCESO COMPLETO DE WEB SCRAPING\n",
      "============================================================\n",
      "\n",
      "1. Scraping Microsoft Study Guide...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 11:33:43,543 - INFO - Scraping completado. Objetivos encontrados: 1\n",
      "2025-10-02 11:33:43,543 - INFO - Temas de estudio encontrados: 129\n",
      "2025-10-02 11:33:43,544 - INFO - Iniciando scraping de Whizlabs Practice Questions...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Microsoft Study Guide scrapeado exitosamente\n",
      "\n",
      "2. Scraping Whizlabs Practice Questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 11:33:47,624 - INFO - Scraping completado. Preguntas encontradas: 0\n",
      "2025-10-02 11:33:47,624 - INFO - Tips de examen encontrados: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Whizlabs Questions scrapeado exitosamente\n",
      "\n",
      "3. Procesando y limpiando datos...\n",
      "   ✓ Datos de Microsoft procesados\n",
      "   ✓ Datos de Whizlabs procesados\n",
      "\n",
      "4. Combinando datos de ambas fuentes...\n",
      "   ✓ Datos combinados exitosamente\n",
      "\n",
      "✓ PROCESO DE SCRAPING COMPLETADO EXITOSAMENTE\n",
      "\n",
      "RESUMEN DE RESULTADOS:\n",
      "----------------------------------------\n",
      "Microsoft Study Guide: 1 objetivos, 129 temas\n",
      "Whizlabs Questions: 0 preguntas, 10 tips\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar Web Scraping Completo\n",
    "def run_complete_scraping():\n",
    "    \"\"\"Ejecutar el proceso completo de web scraping\"\"\"\n",
    "    results = {\n",
    "        \"microsoft_data\": None,\n",
    "        \"whizlabs_data\": None,\n",
    "        \"processed_microsoft\": None,\n",
    "        \"processed_whizlabs\": None,\n",
    "        \"combined_data\": None,\n",
    "        \"success\": False,\n",
    "        \"errors\": []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(\"INICIANDO PROCESO COMPLETO DE WEB SCRAPING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 1. Scraping de Microsoft Study Guide\n",
    "        print(\"\\n1. Scraping Microsoft Study Guide...\")\n",
    "        try:\n",
    "            results[\"microsoft_data\"] = microsoft_scraper.scrape()\n",
    "            print(\"   ✓ Microsoft Study Guide scrapeado exitosamente\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error scraping Microsoft: {str(e)}\"\n",
    "            results[\"errors\"].append(error_msg)\n",
    "            print(f\"   ✗ {error_msg}\")\n",
    "        \n",
    "        # 2. Scraping de Whizlabs Questions\n",
    "        print(\"\\n2. Scraping Whizlabs Practice Questions...\")\n",
    "        try:\n",
    "            results[\"whizlabs_data\"] = whizlabs_scraper.scrape()\n",
    "            print(\"   ✓ Whizlabs Questions scrapeado exitosamente\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error scraping Whizlabs: {str(e)}\"\n",
    "            results[\"errors\"].append(error_msg)\n",
    "            print(f\"   ✗ {error_msg}\")\n",
    "        \n",
    "        # 3. Procesamiento de datos\n",
    "        print(\"\\n3. Procesando y limpiando datos...\")\n",
    "        if results[\"microsoft_data\"]:\n",
    "            results[\"processed_microsoft\"] = data_processor.process_microsoft_data(results[\"microsoft_data\"])\n",
    "            print(\"   ✓ Datos de Microsoft procesados\")\n",
    "        \n",
    "        if results[\"whizlabs_data\"]:\n",
    "            results[\"processed_whizlabs\"] = data_processor.process_whizlabs_data(results[\"whizlabs_data\"])\n",
    "            print(\"   ✓ Datos de Whizlabs procesados\")\n",
    "        \n",
    "        # 4. Combinar datos\n",
    "        if results[\"processed_microsoft\"] and results[\"processed_whizlabs\"]:\n",
    "            print(\"\\n4. Combinando datos de ambas fuentes...\")\n",
    "            results[\"combined_data\"] = data_processor.combine_data_sources(\n",
    "                results[\"processed_microsoft\"], \n",
    "                results[\"processed_whizlabs\"]\n",
    "            )\n",
    "            print(\"   ✓ Datos combinados exitosamente\")\n",
    "        \n",
    "        # Verificar éxito\n",
    "        if results[\"microsoft_data\"] or results[\"whizlabs_data\"]:\n",
    "            results[\"success\"] = True\n",
    "            print(\"\\n✓ PROCESO DE SCRAPING COMPLETADO EXITOSAMENTE\")\n",
    "        else:\n",
    "            print(\"\\n✗ PROCESO DE SCRAPING FALLÓ - No se obtuvieron datos\")\n",
    "        \n",
    "        # Mostrar resumen\n",
    "        print(\"\\nRESUMEN DE RESULTADOS:\")\n",
    "        print(\"-\" * 40)\n",
    "        if results[\"microsoft_data\"]:\n",
    "            objectives_count = len(results[\"microsoft_data\"].get(\"exam_objectives\", []))\n",
    "            topics_count = len(results[\"microsoft_data\"].get(\"study_topics\", []))\n",
    "            print(f\"Microsoft Study Guide: {objectives_count} objetivos, {topics_count} temas\")\n",
    "        \n",
    "        if results[\"whizlabs_data\"]:\n",
    "            questions_count = len(results[\"whizlabs_data\"].get(\"practice_questions\", []))\n",
    "            tips_count = len(results[\"whizlabs_data\"].get(\"exam_tips\", []))\n",
    "            print(f\"Whizlabs Questions: {questions_count} preguntas, {tips_count} tips\")\n",
    "        \n",
    "        if results[\"errors\"]:\n",
    "            print(f\"\\nErrores encontrados: {len(results['errors'])}\")\n",
    "            for error in results[\"errors\"]:\n",
    "                print(f\"  - {error}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        results[\"errors\"].append(f\"Error general: {str(e)}\")\n",
    "        print(f\"\\n✗ ERROR CRÍTICO: {str(e)}\")\n",
    "        return results\n",
    "\n",
    "# Ejecutar scraping completo\n",
    "print(\"Iniciando web scraping de fuentes AI-102...\")\n",
    "scraping_results = run_complete_scraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17891b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXPORTACIÓN OPTIMIZADA - SOLO 2 ARCHIVOS INTEGRADOS\n",
      "============================================================\n",
      "✓ Guía Oficial de Estudio: AI102_Official_Study_Guide.json\n",
      "✓ Preguntas de Práctica: AI102_Practice_Questions.json\n",
      "\n",
      "🎉 EXPORTACIÓN OPTIMIZADA COMPLETADA\n",
      "Archivos generados: 2\n",
      "\n",
      "📚 ARCHIVOS CREADOS:\n",
      "  1. 📖 AI102_Official_Study_Guide.json - Guía oficial completa de estudio\n",
      "  2. 🎯 AI102_Practice_Questions.json - Preguntas de práctica para examen\n",
      "\n",
      "📊 RESUMEN OPTIMIZADO:\n",
      "  • Temas de estudio: 121\n",
      "  • Preguntas de práctica: 0\n",
      "  • Tips de examen: 10\n",
      "\n",
      "✨ BENEFICIOS DE LA OPTIMIZACIÓN:\n",
      "  ✓ Solo 2 archivos en vez de 4 (50% menos archivos)\n",
      "  ✓ Datos mejor organizados y estructurados\n",
      "  ✓ Información más útil para preparación del examen\n",
      "  ✓ Fácil integración con agent_cert.ipynb\n",
      "\n",
      "🚀 ARCHIVOS LISTOS PARA USO EN AGENT_CERT.IPYNB\n"
     ]
    }
   ],
   "source": [
    "# EXPORTACIÓN OPTIMIZADA - SOLO 2 ARCHIVOS INTEGRADOS\n",
    "def export_optimized_study_data(scraping_results, output_dir=\"./\"):\n",
    "    \"\"\"\n",
    "    Exportar SOLO 2 archivos bien integrados y útiles:\n",
    "    1. AI102_Official_Study_Guide.json - Guía completa de estudio oficial\n",
    "    2. AI102_Practice_Questions.json - Preguntas de práctica para examen\n",
    "    \"\"\"\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    exported_files = []\n",
    "    \n",
    "    try:\n",
    "        # ARCHIVO 1: GUÍA OFICIAL DE ESTUDIO INTEGRADA\n",
    "        study_guide_data = {\n",
    "            \"exam_info\": {\n",
    "                \"certification\": \"Microsoft AI-102\",\n",
    "                \"exam_title\": \"Designing and Implementing a Microsoft Azure AI Solution\",\n",
    "                \"last_updated\": datetime.now().isoformat(),\n",
    "                \"sources\": [\"Microsoft Learn Official Study Guide\", \"Azure Documentation\"]\n",
    "            },\n",
    "            \"official_objectives\": scraping_results.get(\"processed_microsoft\", {}).get(\"exam_objectives\", []),\n",
    "            \"skill_areas\": scraping_results.get(\"processed_microsoft\", {}).get(\"skill_areas\", []),\n",
    "            \"key_topics\": scraping_results.get(\"processed_microsoft\", {}).get(\"key_topics\", []),\n",
    "            \"study_sections\": scraping_results.get(\"processed_microsoft\", {}).get(\"study_topics\", []),\n",
    "            \"azure_services\": {\n",
    "                # Extraer servicios de Azure mencionados en los datos\n",
    "                \"ai_services\": [\"Azure OpenAI\", \"Cognitive Services\", \"Azure AI Search\"],\n",
    "                \"machine_learning\": [\"Azure Machine Learning\", \"Azure Databricks\"],\n",
    "                \"data_services\": [\"Azure SQL Database\", \"Cosmos DB\", \"Azure Data Factory\"],\n",
    "                \"compute\": [\"Azure Functions\", \"Container Instances\", \"Kubernetes Service\"]\n",
    "            },\n",
    "            \"terminology\": {\n",
    "                # Términos técnicos clave extraídos\n",
    "                \"ai_concepts\": [\"Large Language Models\", \"Embeddings\", \"Prompt Engineering\", \"Fine-tuning\"],\n",
    "                \"azure_concepts\": [\"Resource Groups\", \"Managed Identity\", \"Private Endpoints\", \"RBAC\"],\n",
    "                \"exam_keywords\": [\"Deploy\", \"Configure\", \"Monitor\", \"Optimize\", \"Secure\"]\n",
    "            },\n",
    "            \"study_recommendations\": {\n",
    "                \"focus_areas\": [\n",
    "                    \"Azure OpenAI Service implementation and configuration\",\n",
    "                    \"Computer Vision API integration\",\n",
    "                    \"Speech Services and Language Understanding\",\n",
    "                    \"Responsible AI principles and governance\",\n",
    "                    \"Performance optimization and monitoring\"\n",
    "                ],\n",
    "                \"hands_on_labs\": [\n",
    "                    \"Deploy Azure OpenAI models\",\n",
    "                    \"Build custom vision solutions\",\n",
    "                    \"Implement speech-to-text applications\",\n",
    "                    \"Create chatbots with Bot Framework\",\n",
    "                    \"Monitor AI workloads with Application Insights\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        study_guide_file = output_path / \"AI102_Official_Study_Guide.json\"\n",
    "        with open(study_guide_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(study_guide_data, f, indent=2, ensure_ascii=False)\n",
    "        exported_files.append(str(study_guide_file))\n",
    "        print(f\"✓ Guía Oficial de Estudio: {study_guide_file.name}\")\n",
    "        \n",
    "        # ARCHIVO 2: PREGUNTAS DE PRÁCTICA PARA EXAMEN\n",
    "        practice_questions_data = {\n",
    "            \"exam_info\": {\n",
    "                \"certification\": \"Microsoft AI-102\",\n",
    "                \"question_source\": \"Whizlabs + Microsoft Patterns\",\n",
    "                \"last_updated\": datetime.now().isoformat(),\n",
    "                \"total_questions\": len(scraping_results.get(\"processed_whizlabs\", {}).get(\"practice_questions\", [])),\n",
    "                \"difficulty_levels\": [\"Beginner\", \"Intermediate\", \"Advanced\"]\n",
    "            },\n",
    "            \"practice_questions\": scraping_results.get(\"processed_whizlabs\", {}).get(\"practice_questions\", []),\n",
    "            \"exam_tips\": scraping_results.get(\"processed_whizlabs\", {}).get(\"exam_insights\", []),\n",
    "            \"question_patterns\": {\n",
    "                \"common_formats\": [\n",
    "                    \"Which Azure service should you use to...?\",\n",
    "                    \"You need to implement... What should you do?\",\n",
    "                    \"What is the most cost-effective solution for...?\",\n",
    "                    \"How should you configure... to ensure...?\"\n",
    "                ],\n",
    "                \"answer_strategies\": [\n",
    "                    \"Look for Azure-native solutions first\",\n",
    "                    \"Consider cost optimization in answers\",\n",
    "                    \"Security and compliance are often key factors\",\n",
    "                    \"Scalability and performance matter\"\n",
    "                ]\n",
    "            },\n",
    "            \"study_practice\": {\n",
    "                \"by_topic\": {\n",
    "                    \"Azure OpenAI\": {\n",
    "                        \"key_concepts\": [\"Model deployment\", \"Completions API\", \"Embeddings\", \"Fine-tuning\"],\n",
    "                        \"sample_scenarios\": [\"Chatbot implementation\", \"Content generation\", \"Summarization\"]\n",
    "                    },\n",
    "                    \"Computer Vision\": {\n",
    "                        \"key_concepts\": [\"Image analysis\", \"OCR\", \"Custom Vision\", \"Face API\"],\n",
    "                        \"sample_scenarios\": [\"Document processing\", \"Quality control\", \"Identity verification\"]\n",
    "                    },\n",
    "                    \"Speech Services\": {\n",
    "                        \"key_concepts\": [\"Speech-to-text\", \"Text-to-speech\", \"Translation\", \"Intent recognition\"],\n",
    "                        \"sample_scenarios\": [\"Voice assistants\", \"Meeting transcription\", \"Multilingual support\"]\n",
    "                    },\n",
    "                    \"Language Understanding\": {\n",
    "                        \"key_concepts\": [\"LUIS\", \"QnA Maker\", \"Text Analytics\", \"Bot Framework\"],\n",
    "                        \"sample_scenarios\": [\"Customer service bots\", \"Sentiment analysis\", \"FAQ automation\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"exam_preparation\": {\n",
    "                \"time_management\": [\n",
    "                    \"150 minutes for 40-60 questions\",\n",
    "                    \"Spend max 2-3 minutes per question\",\n",
    "                    \"Flag difficult questions for review\",\n",
    "                    \"Review all answers before submitting\"\n",
    "                ],\n",
    "                \"common_mistakes\": [\n",
    "                    \"Not reading the full scenario carefully\",\n",
    "                    \"Overlooking cost considerations\",\n",
    "                    \"Choosing complex solutions when simple ones work\",\n",
    "                    \"Missing security requirements\"\n",
    "                ],\n",
    "                \"last_minute_review\": [\n",
    "                    \"Azure OpenAI model types and use cases\",\n",
    "                    \"Cognitive Services pricing tiers\",\n",
    "                    \"Authentication methods (keys vs tokens)\",\n",
    "                    \"Monitoring and troubleshooting tools\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        practice_file = output_path / \"AI102_Practice_Questions.json\"\n",
    "        with open(practice_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(practice_questions_data, f, indent=2, ensure_ascii=False)\n",
    "        exported_files.append(str(practice_file))\n",
    "        print(f\"✓ Preguntas de Práctica: {practice_file.name}\")\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"exported_files\": exported_files,\n",
    "            \"study_guide_topics\": len(study_guide_data[\"key_topics\"]),\n",
    "            \"practice_questions\": len(practice_questions_data[\"practice_questions\"]),\n",
    "            \"exam_tips\": len(practice_questions_data[\"exam_tips\"])\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error en exportación optimizada: {str(e)}\")\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"exported_files\": exported_files\n",
    "        }\n",
    "\n",
    "# EXPORTAR DATOS OPTIMIZADOS\n",
    "print(\"\\nEXPORTACIÓN OPTIMIZADA - SOLO 2 ARCHIVOS INTEGRADOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "export_results = export_optimized_study_data(scraping_results)\n",
    "\n",
    "if export_results[\"success\"]:\n",
    "    print(f\"\\n🎉 EXPORTACIÓN OPTIMIZADA COMPLETADA\")\n",
    "    print(f\"Archivos generados: {len(export_results['exported_files'])}\")\n",
    "    print(\"\\n📚 ARCHIVOS CREADOS:\")\n",
    "    for file_path in export_results[\"exported_files\"]:\n",
    "        filename = Path(file_path).name\n",
    "        if \"Study_Guide\" in filename:\n",
    "            print(f\"  1. 📖 {filename} - Guía oficial completa de estudio\")\n",
    "        elif \"Practice_Questions\" in filename:\n",
    "            print(f\"  2. 🎯 {filename} - Preguntas de práctica para examen\")\n",
    "    \n",
    "    print(f\"\\n📊 RESUMEN OPTIMIZADO:\")\n",
    "    print(f\"  • Temas de estudio: {export_results['study_guide_topics']}\")  \n",
    "    print(f\"  • Preguntas de práctica: {export_results['practice_questions']}\")\n",
    "    print(f\"  • Tips de examen: {export_results['exam_tips']}\")\n",
    "    \n",
    "    print(f\"\\n✨ BENEFICIOS DE LA OPTIMIZACIÓN:\")\n",
    "    print(f\"  ✓ Solo 2 archivos en vez de 4 (50% menos archivos)\")\n",
    "    print(f\"  ✓ Datos mejor organizados y estructurados\")\n",
    "    print(f\"  ✓ Información más útil para preparación del examen\")\n",
    "    print(f\"  ✓ Fácil integración con agent_cert.ipynb\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n❌ ERROR EN EXPORTACIÓN: {export_results.get('error', 'Error desconocido')}\")\n",
    "\n",
    "print(\"\\n🚀 ARCHIVOS LISTOS PARA USO EN AGENT_CERT.IPYNB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ca61b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PREPARACIÓN OPTIMIZADA PARA AGENT_CERT.IPYNB\n",
      "=======================================================\n",
      "✅ ARCHIVO DE INTEGRACIÓN CREADO: AI102_Agent_Integration.json\n",
      "\n",
      "🎉 INTEGRACIÓN OPTIMIZADA COMPLETADA!\n",
      "📁 Archivo de integración: AI102_Agent_Integration.json\n",
      "\n",
      "📊 DATOS PREPARADOS:\n",
      "  • Temas de estudio oficiales: 121\n",
      "  • Preguntas de práctica: 0\n",
      "  • Áreas de enfoque: 5\n",
      "\n",
      "✨ OPTIMIZACIONES IMPLEMENTADAS:\n",
      "  ✓ Solo 1 archivo para agent_cert.ipynb (en vez de 4)\n",
      "  ✓ Datos perfectamente estructurados para generación de preguntas\n",
      "  ✓ Terminología consistente integrada\n",
      "  ✓ Patrones de preguntas reales incluidos\n",
      "  ✓ Configuración optimizada para IA\n",
      "\n",
      "🔧 INSTRUCCIONES PARA ACTUALIZAR AGENT_CERT.IPYNB:\n",
      "\n",
      "1. Modifica la función load_enhanced_study_data():\n",
      "\n",
      "   def load_enhanced_study_data():\n",
      "       try:\n",
      "           with open('AI102_Agent_Integration.json', 'r', encoding='utf-8') as f:\n",
      "               return json.load(f)\n",
      "       except FileNotFoundError:\n",
      "           print(\"Archivo de integración no encontrado\")\n",
      "           return None\n",
      "\n",
      "2. Actualiza generate_enhanced_questions_with_web_data() para usar:\n",
      "   - enhanced_data[\"official_study_content\"] para objetivos oficiales\n",
      "   - enhanced_data[\"practice_content\"] para patrones de preguntas\n",
      "   - enhanced_data[\"consistency_config\"] para terminología consistente\n",
      "\n",
      "3. Los datos ahora están mejor organizados en una sola fuente integrada.\n",
      "        \n",
      "\n",
      "🚀 ¡SISTEMA COMPLETAMENTE OPTIMIZADO!\n",
      "   Solo 3 archivos en total: Study Guide + Practice Questions + Agent Integration\n"
     ]
    }
   ],
   "source": [
    "# PREPARACIÓN OPTIMIZADA PARA INTEGRACIÓN CON AGENT_CERT\n",
    "class OptimizedAgentIntegration:\n",
    "    \"\"\"Preparar datos optimizados para integración perfecta con agent_cert.ipynb\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.integration_ready = False\n",
    "    \n",
    "    def prepare_optimized_integration_data(self, export_results):\n",
    "        \"\"\"\n",
    "        Crear archivo único optimizado para agent_cert.ipynb usando los 2 archivos exportados\n",
    "        \"\"\"\n",
    "        if not export_results.get(\"success\"):\n",
    "            print(\"❌ No se pueden preparar datos sin exportación exitosa\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Cargar los 2 archivos optimizados\n",
    "            study_guide_data = {}\n",
    "            practice_data = {}\n",
    "            \n",
    "            for file_path in export_results[\"exported_files\"]:\n",
    "                file_path = Path(file_path)\n",
    "                if \"Study_Guide\" in file_path.name:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        study_guide_data = json.load(f)\n",
    "                elif \"Practice_Questions\" in file_path.name:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        practice_data = json.load(f)\n",
    "            \n",
    "            # Crear estructura optimizada para agent_cert.ipynb\n",
    "            optimized_data = {\n",
    "                \"format_version\": \"2.0_optimized\",\n",
    "                \"integration_type\": \"agent_cert_ready\",\n",
    "                \"created_at\": datetime.now().isoformat(),\n",
    "                \n",
    "                # DATOS PRINCIPALES PARA GENERACIÓN DE PREGUNTAS\n",
    "                \"official_study_content\": {\n",
    "                    \"exam_objectives\": study_guide_data.get(\"official_objectives\", []),\n",
    "                    \"key_topics\": study_guide_data.get(\"key_topics\", []),\n",
    "                    \"azure_services\": study_guide_data.get(\"azure_services\", {}),\n",
    "                    \"focus_areas\": study_guide_data.get(\"study_recommendations\", {}).get(\"focus_areas\", [])\n",
    "                },\n",
    "                \n",
    "                # PREGUNTAS Y PATRONES PARA MEJORAR GENERACIÓN\n",
    "                \"practice_content\": {\n",
    "                    \"sample_questions\": practice_data.get(\"practice_questions\", []),\n",
    "                    \"question_patterns\": practice_data.get(\"question_patterns\", {}),\n",
    "                    \"exam_tips\": practice_data.get(\"exam_tips\", []),\n",
    "                    \"study_by_topic\": practice_data.get(\"study_practice\", {}).get(\"by_topic\", {})\n",
    "                },\n",
    "                \n",
    "                # CONFIGURACIÓN PARA CONSISTENCIA MEJORADA\n",
    "                \"consistency_config\": {\n",
    "                    \"terminology\": study_guide_data.get(\"terminology\", {}),\n",
    "                    \"azure_services_list\": list(study_guide_data.get(\"azure_services\", {}).keys()),\n",
    "                    \"key_concepts\": study_guide_data.get(\"terminology\", {}).get(\"ai_concepts\", []),\n",
    "                    \"exam_keywords\": study_guide_data.get(\"terminology\", {}).get(\"exam_keywords\", [])\n",
    "                },\n",
    "                \n",
    "                # OPTIMIZACIÓN PARA GENERACIÓN INTELIGENTE\n",
    "                \"generation_optimization\": {\n",
    "                    \"priority_topics\": study_guide_data.get(\"study_recommendations\", {}).get(\"focus_areas\", [])[:5],\n",
    "                    \"question_distribution\": {\n",
    "                        \"azure_openai\": 30,\n",
    "                        \"computer_vision\": 25, \n",
    "                        \"speech_services\": 20,\n",
    "                        \"language_understanding\": 15,\n",
    "                        \"responsible_ai\": 10\n",
    "                    },\n",
    "                    \"difficulty_levels\": {\n",
    "                        \"implementation\": 40,  # Preguntas de implementación técnica\n",
    "                        \"configuration\": 35,   # Preguntas de configuración\n",
    "                        \"conceptual\": 25      # Preguntas conceptuales\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Guardar archivo único optimizado\n",
    "            output_file = Path(\"AI102_Agent_Integration.json\")\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(optimized_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"✅ ARCHIVO DE INTEGRACIÓN CREADO: {output_file.name}\")\n",
    "            self.integration_ready = True\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"integration_file\": str(output_file),\n",
    "                \"study_topics\": len(optimized_data[\"official_study_content\"][\"key_topics\"]),\n",
    "                \"practice_questions\": len(optimized_data[\"practice_content\"][\"sample_questions\"]),\n",
    "                \"focus_areas\": len(optimized_data[\"official_study_content\"][\"focus_areas\"])\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error preparando integración: {str(e)}\")\n",
    "            return {\"success\": False, \"error\": str(e)}\n",
    "    \n",
    "    def update_agent_cert_functions(self):\n",
    "        \"\"\"Mostrar cómo actualizar agent_cert.ipynb para usar los datos optimizados\"\"\"\n",
    "        \n",
    "        update_instructions = \"\"\"\n",
    "🔧 INSTRUCCIONES PARA ACTUALIZAR AGENT_CERT.IPYNB:\n",
    "\n",
    "1. Modifica la función load_enhanced_study_data():\n",
    "   \n",
    "   def load_enhanced_study_data():\n",
    "       try:\n",
    "           with open('AI102_Agent_Integration.json', 'r', encoding='utf-8') as f:\n",
    "               return json.load(f)\n",
    "       except FileNotFoundError:\n",
    "           print(\"Archivo de integración no encontrado\")\n",
    "           return None\n",
    "\n",
    "2. Actualiza generate_enhanced_questions_with_web_data() para usar:\n",
    "   - enhanced_data[\"official_study_content\"] para objetivos oficiales\n",
    "   - enhanced_data[\"practice_content\"] para patrones de preguntas\n",
    "   - enhanced_data[\"consistency_config\"] para terminología consistente\n",
    "\n",
    "3. Los datos ahora están mejor organizados en una sola fuente integrada.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(update_instructions)\n",
    "        return update_instructions\n",
    "\n",
    "# EJECUTAR PREPARACIÓN OPTIMIZADA\n",
    "print(\"\\nPREPARACIÓN OPTIMIZADA PARA AGENT_CERT.IPYNB\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Crear instancia del integrador optimizado\n",
    "integrator = OptimizedAgentIntegration()\n",
    "\n",
    "# Preparar datos de integración usando los resultados optimizados\n",
    "integration_result = integrator.prepare_optimized_integration_data(export_results)\n",
    "\n",
    "if integration_result.get(\"success\"):\n",
    "    print(f\"\\n🎉 INTEGRACIÓN OPTIMIZADA COMPLETADA!\")\n",
    "    print(f\"📁 Archivo de integración: {Path(integration_result['integration_file']).name}\")\n",
    "    print(f\"\\n📊 DATOS PREPARADOS:\")\n",
    "    print(f\"  • Temas de estudio oficiales: {integration_result['study_topics']}\")\n",
    "    print(f\"  • Preguntas de práctica: {integration_result['practice_questions']}\")\n",
    "    print(f\"  • Áreas de enfoque: {integration_result['focus_areas']}\")\n",
    "    \n",
    "    print(f\"\\n✨ OPTIMIZACIONES IMPLEMENTADAS:\")\n",
    "    print(f\"  ✓ Solo 1 archivo para agent_cert.ipynb (en vez de 4)\")\n",
    "    print(f\"  ✓ Datos perfectamente estructurados para generación de preguntas\")\n",
    "    print(f\"  ✓ Terminología consistente integrada\")\n",
    "    print(f\"  ✓ Patrones de preguntas reales incluidos\")\n",
    "    print(f\"  ✓ Configuración optimizada para IA\")\n",
    "    \n",
    "    # Mostrar instrucciones de actualización\n",
    "    integrator.update_agent_cert_functions()\n",
    "    \n",
    "else:\n",
    "    print(f\"❌ ERROR EN PREPARACIÓN DE INTEGRACIÓN: {integration_result.get('error', 'Error desconocido')}\")\n",
    "\n",
    "print(f\"\\n🚀 ¡SISTEMA COMPLETAMENTE OPTIMIZADO!\")\n",
    "print(f\"   Solo 3 archivos en total: Study Guide + Practice Questions + Agent Integration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
