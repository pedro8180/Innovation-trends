{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f1f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"ai_102.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32301292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>module</th>\n",
       "      <th>unit</th>\n",
       "      <th>topic_name</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Develop generative AI apps in Azure - Training</td>\n",
       "      <td>Plan and prepare to develop AI solutions on Azure</td>\n",
       "      <td>Introduction</td>\n",
       "      <td># Introduction\\n\\nCompleted 100 XP\\n\\n  * 1 mi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          module  \\\n",
       "0           0  Develop generative AI apps in Azure - Training   \n",
       "\n",
       "                                                unit    topic_name  \\\n",
       "0  Plan and prepare to develop AI solutions on Azure  Introduction   \n",
       "\n",
       "                                             content  \n",
       "0  # Introduction\\n\\nCompleted 100 XP\\n\\n  * 1 mi...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5334adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "azure_search_key = os.getenv('AZURE_SEARCH_KEY')\n",
    "azure_search_endpoint = os.getenv('AZURE_SEARCH_ENDPOINT')\n",
    "\n",
    "credential = AzureKeyCredential(azure_search_key)\n",
    "\n",
    "search_index_client = SearchClient(endpoint=azure_search_endpoint, \n",
    "                                   index_name=\"exam-simulator-idx\", \n",
    "                                   credential=credential)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9945cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "azure_embeddings_deployment = os.getenv('AZURE_EMBEDDINGS_DEPLOYMENT')\n",
    "azure_oai_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "azure_oai_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(azure_deployment=azure_embeddings_deployment,\n",
    "                                       azure_endpoint=azure_oai_endpoint,\n",
    "                                       api_key=azure_oai_key,\n",
    "                                       api_version=\"2023-05-15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b70d879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/ai-102 data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "234cc745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "#data_json = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a64c38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training', 'module': 'Develop natural language solutions in Azure - Training', 'unit': 'Build a conversational language understanding model', 'topic': 'Introduction', 'text': \"Natural language processing (NLP) is a common AI problem in which software must be able to work with text or speech in the natural language form that a human user would write or speak. Within the broader area of NLP, natural language understanding (NLU) deals with the problem of determining semantic meaning from natural language - usually by using a trained language model. A common design pattern for a natural language understanding solution looks like this: In this design pattern: An app accepts natural language input from a user. A language model is used to determine semantic meaning (the user's intent ). The app performs an appropriate action. Azure AI Language enables developers to build apps based on language models that can be trained with a relatively small number of samples to discern a user's intended meaning. In this module, you'll learn how to use the service to create a natural language understanding app using Azure AI Language. After completing this module, you’ll be able to: Provision an Azure AI Language resource. Define intents, entities, and utterances. Use patterns to differentiate similar utterances. Use pre-built entity components. Train, test, publish, and review a model.\", 'links': None, 'images': [{'image_name': 'image1', 'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/build-language-understanding-model/media/language-understanding-app.png', 'image_alt': 'Diagram showing an app accepts natural language input, and uses a model to determine semantic meaning before taking the appropriate action.'}], 'videos': [{'name': 'video1', 'video_src': 'https://play.vidyard.com/XYUdrPj43e5HhMYB6PRFVP?loop=1'}], 'exercise': None}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "resultado = next((item for item in data if item.get('videos') is not None), None)\n",
    "\n",
    "print(resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af5c0fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Plan and prepare to develop AI solutions on Azure',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"The growth in the use of artificial intelligence (AI) in general, and generative AI in particular means that developers are increasingly required to create comprehensive AI solutions. These solutions need to combine machine learning models, AI services, prompt engineering solutions, and custom code. Microsoft Azure provides multiple services that you can use to create AI solutions. However, before embarking on an AI application development project, it's useful to consider the available options for services, tools, and frameworks as well as some principles and practices that can help you succeed. This module explores some of the key considerations for planning an AI development project, and introduces Azure AI Foundry ; a comprehensive platform for AI development on Microsoft Azure.\",\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Plan and prepare to develop AI solutions on Azure',\n",
       "  'topic': 'What is AI?',\n",
       "  'text': 'The term \"Artificial Intelligence\" (AI) covers a wide range of software capabilities that enable applications to exhibit human-like behavior. AI has been around for many years, and its definition has varied as the technology and use cases associated with it have evolved. In today\\'s technological landscape, AI solutions are built on machine learning models that encapsulate semantic relationships found in huge quantities of data; enabling applications to appear to interpret input in various formats, reason over the input data, and generate appropriate responses and predictions. Common AI capabilities that developers can integrate into a software application include: Capability Description Generative AI The ability to generate original responses to natural language prompts . For example, software for a real estate business might be used to automatically generate property descriptions and advertising copy for a property listing. Agents Generative AI applications that can respond to user input or assess situations autonomously, and take appropriate actions. For example, an \"executive assistant\" agent could provide details about the location of a meeting on your calendar, or even attach a map or automate the booking of a taxi or rideshare service to help you get there. Computer vision The ability to accept, interpret, and process visual input from images, videos, and live camera streams. For example, an automated checkout in a grocery store might use computer vision to identify which products a customer has in their shopping basket, eliminating the need to scan a barcode or manually enter the product and quantity. Speech The ability to recognize and synthesize speech. For example, a digital assistant might enable users to ask questions or provide audible instructions by speaking into a microphone, and generate spoken output to provide answers or confirmations. Natural language processing The ability to process natural language in written or spoken form, analyze it, identify key points, and generate summaries or categorizations. For example, a marketing application might analyze social media messages that mention a particular company, translate them to a specific language, and categorize them as positive or negative based on sentiment analysis. Information extraction The ability to use computer vision, speech, and natural language processing to extract key information from documents, forms, images, recordings, and other kinds of content. For example, an automated expense claims processing application might extract purchase dates, individual line item details, and total costs from a scanned receipt. Decision support The ability to use historic data and learned correlations to make predictions that support business decision making. For example, analyzing demographic and economic factors in a city to predict real estate market trends that inform property pricing decisions. Determining the specific AI capabilities you want to include in your application can help you identify the most appropriate AI services that you\\'ll need to provision, configure, and use in your solution. A closer look at generative AI Generative AI represents the latest advance in artificial intelligence, and deserves some extra attention. Generative AI uses language models to respond to natural language prompts , enabling you to build conversational apps and agents that support research, content creation, and task automation in ways that were previously unimaginable. The language models used in generative AI solutions can be large language models (LLMs) that have been trained on huge volumes of data and include many millions of parameters; or they can be small language models (SLMs) that are optimized for specific scenarios with lower overhead. Language models commonly respond to text-based prompts with natural language text; though increasingly new multi-modal models are able to handle image or speech prompts and respond by generating text, code, speech, or images.',\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/generative-ai.png',\n",
       "    'image_alt': 'Diagram of speech bubbles.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/agent.png',\n",
       "    'image_alt': 'Diagram of a human head with a cog for a brain.'},\n",
       "   {'image_name': 'image3',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/computer-vision.png',\n",
       "    'image_alt': 'Diagram of an eye being scanned.'},\n",
       "   {'image_name': 'image4',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/speech.png',\n",
       "    'image_alt': 'Diagram of a speech bubble and a sound wave.'},\n",
       "   {'image_name': 'image5',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/natural-language.png',\n",
       "    'image_alt': 'Diagram of a text document.'},\n",
       "   {'image_name': 'image6',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/information-extraction.png',\n",
       "    'image_alt': 'Diagram of a form containing information.'},\n",
       "   {'image_name': 'image7',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/prediction.png',\n",
       "    'image_alt': 'Diagram of a chart showing an upward trend.'},\n",
       "   {'image_name': 'image8',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/prompt.png',\n",
       "    'image_alt': 'Diagram of a prompt, a language model, and a response.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Plan and prepare to develop AI solutions on Azure',\n",
       "  'topic': 'Azure AI services',\n",
       "  'text': \"Microsoft Azure provides a wide range of cloud services that you can use to develop, deploy, and manage an AI solution. The most obvious starting point for considering AI development on Azure is Azure AI services; a set of out-of-the-box prebuilt APIs and models that you can integrate into your applications. The following table lists some commonly used Azure AI services (for a full list of all available Azure AI services, see Available Azure AI services ). Service Description Azure OpenAI Azure OpenAI in Foundry Models provides access to OpenAI generative AI models including the GPT family of large and small language models and DALL-E image-generation models within a scalable and securable cloud service on Azure. Azure AI Vision The Azure AI Vision service provides a set of models and APIs that you can use to implement common computer vision functionality in an application. With the AI Vision service, you can detect common objects in images, generate captions, descriptions, and tags based on image contents, and read text in images. Azure AI Speech The Azure AI Speech service provides APIs that you can use to implement text to speech and speech to text transformation, as well as specialized speech-based capabilities like speaker recognition and translation. Azure AI Language The Azure AI Language service provides models and APIs that you can use to analyze natural language text and perform tasks such as entity extraction, sentiment analysis, and summarization. The AI Language service also provides functionality to help you build conversational language models and question answering solutions. Azure AI Foundry Content Safety Azure AI Foundry Content Safety provides developers with access to advanced algorithms for processing images and text and flagging content that is potentially offensive, risky, or otherwise undesirable. Azure AI Translator The Azure AI Translator service uses state-of-the-art language models to translate text between a large number of languages. Azure AI Face The Azure AI Face service is a specialist computer vision implementation that can detect, analyze, and recognize human faces. Because of the potential risks associated with personal identification and misuse of this capability, access to some features of the AI Face service are restricted to approved customers. Azure AI Custom Vision The Azure AI Custom Vision service enables you to train and use custom computer vision models for image classification and object detection. Azure AI Document Intelligence With Azure AI Document Intelligence, you can use pre-built or custom models to extract fields from complex documents such as invoices, receipts, and forms. Azure AI Content Understanding The Azure AI Content Understanding service provides multi-modal content analysis capabilities that enable you to build models to extract data from forms and documents, images, videos, and audio streams. Azure AI Search The Azure AI Search service uses a pipeline of AI skills based on other Azure AI Services and custom code to extract information from content and create a searchable index. AI Search is commonly used to create vector indexes for data that can then be used to ground prompts submitted to generative AI language models, such as those provided in Azure OpenAI. Considerations for Azure AI services resources To use Azure AI services, you create one or more Azure AI resources in an Azure subscription and implement code in client applications to consume them. In some cases, AI services include web-based visual interfaces that you can use to configure and test your resources - for example to train a custom image classification model using the Custom Vision service you can use the visual interface to upload training images, manage training jobs, and deploy the resulting model. Note You can provision Azure AI services resources in the Azure portal (or by using BICEP or ARM templates or the Azure command-line interface) and build applications that use them directly through various service-specific APIs and SDKs. However, as we'll discuss later in this module, in most medium to large-scale development scenarios it's better to provision Azure AI services resources as part of an Azure AI Foundry project - enabling you to centralize access control and cost management, and making it easier to manage shared resources and build the next generation of generative AI apps and agents. Single service or multi-service resource? Most Azure AI services, such as Azure AI Vision , Azure AI Language , and so on, can be provisioned as standalone resources, enabling you to create only the Azure resources you specifically need. Additionally, standalone Azure AI services often include a free-tier SKU with limited functionality, enabling you to evaluate and develop with the service at no cost. Each standalone Azure AI resource provides an endpoint and authorization keys that you can use to access it securely from a client application. Alternatively, you can provision a multi-service resource that encapsulates multiple AI services in a single Azure resource. Using a multi-service resource can make it easier to manage applications that use multiple AI capabilities. There are two multi-service resource types you can use: Resource Description Azure AI services The Azure AI Services resource type includes the following services, making them available from a single endpoint: Azure AI Speech Azure AI Language Azure AI Translator Azure AI Vision Azure AI Face Azure AI Custom Vision Azure AI Document Intelligence Azure AI Foundry The Azure AI Foundry resource type includes the following services, and supports working with them through an Azure AI Foundry project*: Azure OpenAI Azure AI Speech Azure AI Language Azure AI Foundry Content Safety Azure AI Translator Azure AI Vision Azure AI Face Azure AI Document Intelligence Azure AI Content Understanding * Azure AI Foundry is discussed in the next unit. Regional availability Some services and models are available in only a subset of Azure regions. Consider service availability and any regional quota restrictions for your subscription when provisioning Azure AI services. Use the product availability table to check regional availability of Azure services. Use the model availability table in the Azure OpenAI documentation to determine regional availability for Azure OpenAI models. Cost Azure AI services are charged based on usage, with different pricing schemes available depending on the specific services being used. As you plan an AI solution on Azure, use the Azure AI services pricing documentation to understand pricing for the AI services you intend to incorporate into your application. You can use the Azure pricing calculator to estimate the costs your expected usage will incur.\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/what-are-ai-services#available-azure-ai-services?azure-portal=true',\n",
       "   'https://azure.microsoft.com/explore/global-infrastructure/products-by-region/table?azure-portal=true',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#model-summary-table-and-region-availability?azure-portal=true',\n",
       "   'https://azure.microsoft.com/pricing/details/cognitive-services?azure-portal=true',\n",
       "   'https://azure.microsoft.com/pricing/calculator?azure-portal=true'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/open-ai.png',\n",
       "    'image_alt': 'Azure OpenAI service icon.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/vision.png',\n",
       "    'image_alt': 'Azure AI Vision service icon.'},\n",
       "   {'image_name': 'image3',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/speech-service.png',\n",
       "    'image_alt': 'Azure AI Speech service icon.'},\n",
       "   {'image_name': 'image4',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/language.png',\n",
       "    'image_alt': 'Azure AI Language service icon.'},\n",
       "   {'image_name': 'image5',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/content-safety.png',\n",
       "    'image_alt': 'Azure AI Foundry Content Safety service icon.'},\n",
       "   {'image_name': 'image6',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/translator.png',\n",
       "    'image_alt': 'Azure AI Translator service icon.'},\n",
       "   {'image_name': 'image7',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/face.png',\n",
       "    'image_alt': 'Azure AI Face service icon.'},\n",
       "   {'image_name': 'image8',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/custom-vision.png',\n",
       "    'image_alt': 'Azure AI Custom Vision service icon.'},\n",
       "   {'image_name': 'image9',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/document-intelligence.png',\n",
       "    'image_alt': 'Azure AI Document Intelligence service icon.'},\n",
       "   {'image_name': 'image10',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/content-understanding.png',\n",
       "    'image_alt': 'Azure AI Content Understanding service icon.'},\n",
       "   {'image_name': 'image11',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/search.png',\n",
       "    'image_alt': 'Azure AI Search service icon.'},\n",
       "   {'image_name': 'image12',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/cognitive-services.png',\n",
       "    'image_alt': 'Azure AI service icon.'},\n",
       "   {'image_name': 'image13',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/ai-services.png',\n",
       "    'image_alt': 'Azure AI Foundry icon.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Plan and prepare to develop AI solutions on Azure',\n",
       "  'topic': 'Azure AI Foundry',\n",
       "  'text': 'Azure AI Foundry is a platform for AI development on Microsoft Azure. While you can provision individual Azure AI services resources and build applications that consume them without it, the project organization, resource management, and AI development capabilities of Azure AI Foundry makes it the recommended way to build all but the most simple solutions. Azure AI Foundry provides the Azure AI Foundry portal , a web-based visual interface for working with AI projects. It also provides the Azure AI Foundry SDK , which you can use to build AI solutions programmatically. Azure AI Foundry projects In Azure AI Foundry, you manage the resource connections, data, code, and other elements of the AI solution in projects . There are two kinds of project: Foundry projects Foundry projects are associated with an Azure AI Foundry resource in an Azure subscription. Foundry projects provide support for Azure AI Foundry models (including OpenAI models), Azure AI Foundry Agent Service, Azure AI services, and tools for evaluation and responsible AI development. An Azure AI Foundry resource supports the most common AI development tasks to develop generative AI chat apps and agents. In most cases, using a Foundry project provides the right level of resource centralization and capabilities with a minimal amount of administrative resource management. You can use Azure AI Foundry portal to work in projects that are based in Azure AI Foundry resources, making it easy to add connected resources and manage model and agent deployments. Hub-based projects Hub-based projects are associated with an Azure AI hub resource in an Azure subscription. Hub-based projects include an Azure AI Foundry resource, as well as managed compute, support for Prompt Flow development, and connected Azure storage and Azure key vault resources for secure data storage. Azure AI hub resources support advanced AI development scenarios, like developing Prompt Flow based applications or fine-tuning models. You can also use Azure AI hub resources in both Azure AI Foundry portal and Azure Machine learning portal, making it easier to work on collaborative projects that involve data scientists and machine learning specialists as well as developers and AI software engineers Tip For more information about Azure AI Foundry project types, see What is Azure AI Foundry? .',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry?azure-portal=true'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/foundry-project.png',\n",
       "    'image_alt': 'Diagram of a Foundry project.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/hub-project.png',\n",
       "    'image_alt': 'Diagram of a hub-based project.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Plan and prepare to develop AI solutions on Azure',\n",
       "  'topic': 'Developer tools and SDKs',\n",
       "  'text': \"While you can perform many of the tasks needed to develop an AI solution directly in the Azure AI Foundry portal, developers also need to write, test, and deploy code. Development tools and environments There are many development tools and environments available, and developers should choose one that supports the languages, SDKs, and APIs they need to work with and with which they're most comfortable. For example, a developer who focuses strongly on building applications for Windows using the .NET Framework might prefer to work in an integrated development environment (IDE) like Microsoft Visual Studio. Conversely, a web application developer who works with a wide range of open-source languages and libraries might prefer to use a code editor like Visual Studio Code (VS Code). Both of these products are suitable for developing AI applications on Azure. The Azure AI Foundry for Visual Studio Code extension When developing Azure AI Foundry based generative AI applications in Visual Studio Code, you can use the Azure AI Foundry for Visual Studio Code extension to simplify key tasks in the workflow, including: Creating a project. Selecting and deploying a model. Testing a model in the playground. Creating an agent. Tip For more information about using the Azure AI Foundry for Visual Studio Code extension, see Work with the Azure AI Foundry for Visual Studio Code extension . GitHub and GitHub Copilot GitHub is the world's most popular platform for source control and DevOps management, and can be a critical element of any team development effort. Visual Studio and VS Code both provide native integration with GitHub, and access to GitHub Copilot; an AI assistant that can significantly improve developer productivity and effectiveness. Tip For more information about using GitHub Copilot in Visual Studio Code, see GitHub Copilot in VS Code . Programming languages, APIs, and SDKs You can develop AI applications using many common programming languages and frameworks, including Microsoft C#, Python, Node, TypeScript, Java, and others. When building AI solutions on Azure, some common SDKs you should plan to install and use include: The Azure AI Foundry SDK , which enables you to write code to connect to Azure AI Foundry projects and access resource connections, which you can then work with using service-specific SDKs. The Azure AI Foundry Models API , which provides an interface for working with generative AI model endpoints hosted in Azure AI Foundry. The Azure OpenAI in Azure AI Foundry Models API , which enables you to build chat applications based on OpenAI models hosted in Azure AI Foundry. Azure AI Services SDKs - AI service-specific libraries for multiple programming languages and frameworks that enable you to consume Azure AI Services resources in your subscription. You can also use Azure AI Services through their REST APIs . The Azure AI Foundry Agent Service , which is accessed through the Azure AI Foundry SDK and can be integrated with frameworks like Semantic Kernel to build comprehensive AI agent solutions.\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/get-started-projects-vs-code?azure-portal=true',\n",
       "   'https://code.visualstudio.com/docs/copilot/overview?azure-portal=true',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/sdk-overview?azure-portal=true',\n",
       "   'https://learn.microsoft.com/en-us/rest/api/aifoundry/modelinference/',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/openai/reference',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/reference/sdk-package-resources?azure-portal=true',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/reference/rest-api-resources',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/agents/overview?azure-portal=true',\n",
       "   'https://learn.microsoft.com/en-us/semantic-kernel/overview?azure-portal=true'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/vs-code.png',\n",
       "    'image_alt': 'Screenshot of the Azure AI Foundry Visual Studio Code extension.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/github-copilot.png',\n",
       "    'image_alt': 'Screenshot of GitHub Copilot in Visual Studio Code.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Plan and prepare to develop AI solutions on Azure',\n",
       "  'topic': 'Responsible AI',\n",
       "  'text': \"It's important for software engineers to consider the impact of their software on users, and society in general; including considerations for its responsible use. When the application is imbued with artificial intelligence, these considerations are particularly important due to the nature of how AI systems work and inform decisions; often based on probabilistic models, which are in turn dependent on the data with which they were trained. The human-like nature of AI solutions is a significant benefit in making applications user-friendly, but it can also lead users to place a great deal of trust in the application's ability to make correct decisions. The potential for harm to individuals or groups through incorrect predictions or misuse of AI capabilities is a major concern, and software engineers building AI-enabled solutions should apply due consideration to mitigate risks and ensure fairness, reliability, and adequate protection from harm or discrimination. Let's discuss some core principles for responsible AI that have been adopted at Microsoft. Fairness AI systems should treat all people fairly. For example, suppose you create a machine learning model to support a loan approval application for a bank. The model should make predictions of whether or not the loan should be approved without incorporating any bias based on gender, ethnicity, or other factors that might result in an unfair advantage or disadvantage to specific groups of applicants. Fairness of machine learned systems is a highly active area of ongoing research, and some software solutions exist for evaluating, quantifying, and mitigating unfairness in machine learned models. However, tooling alone isn't sufficient to ensure fairness. Consider fairness from the beginning of the application development process; carefully reviewing training data to ensure it's representative of all potentially affected subjects, and evaluating predictive performance for subsections of your user population throughout the development lifecycle. Reliability and safety AI systems should perform reliably and safely. For example, consider an AI-based software system for an autonomous vehicle; or a machine learning model that diagnoses patient symptoms and recommends prescriptions. Unreliability in these kinds of system can result in substantial risk to human life. As with any software, AI-based software application development must be subjected to rigorous testing and deployment management processes to ensure that they work as expected before release. Additionally, software engineers need to take into account the probabilistic nature of machine learning models, and apply appropriate thresholds when evaluating confidence scores for predictions. Privacy and security AI systems should be secure and respect privacy. The machine learning models on which AI systems are based rely on large volumes of data, which may contain personal details that must be kept private. Even after models are trained and the system is in production, they use new data to make predictions or take action that may be subject to privacy or security concerns; so appropriate safeguards to protect data and customer content must be implemented. Inclusiveness AI systems should empower everyone and engage people. AI should bring benefits to all parts of society, regardless of physical ability, gender, sexual orientation, ethnicity, or other factors. One way to optimize for inclusiveness is to ensure that the design, development, and testing of your application includes input from as diverse a group of people as possible. Transparency AI systems should be understandable. Users should be made fully aware of the purpose of the system, how it works, and what limitations may be expected. For example, when an AI system is based on a machine learning model, you should generally make users aware of factors that may affect the accuracy of its predictions, such as the number of cases used to train the model, or the specific features that have the most influence over its predictions. You should also share information about the confidence score for predictions. When an AI application relies on personal data, such as a facial recognition system that takes images of people to recognize them; you should make it clear to the user how their data is used and retained, and who has access to it. Accountability People should be accountable for AI systems. Although many AI systems seem to operate autonomously, ultimately it's the responsibility of the developers who trained and validated the models they use, and defined the logic that bases decisions on model predictions to ensure that the overall system meets responsibility requirements. To help meet this goal, designers and developers of AI-based solution should work within a framework of governance and organizational principles that ensure the solution meets responsible and legal standards that are clearly defined. Tip For more information about Microsoft's principles for responsible AI, see the Microsoft responsible AI site .\",\n",
       "  'links': ['https://microsoft.com/ai/responsible-ai?azure-portal=true'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/fairness.png',\n",
       "    'image_alt': 'A diagram of scales.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/reliability-safety.png',\n",
       "    'image_alt': 'A diagram of a shield.'},\n",
       "   {'image_name': 'image3',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/privacy-security.png',\n",
       "    'image_alt': 'A diagram of a padlock.'},\n",
       "   {'image_name': 'image4',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/inclusiveness.png',\n",
       "    'image_alt': 'A diagram of a diverse group of people.'},\n",
       "   {'image_name': 'image5',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/transparency.png',\n",
       "    'image_alt': 'A diagram of an eye.'},\n",
       "   {'image_name': 'image6',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/prepare-azure-ai-development/media/accountability.png',\n",
       "    'image_alt': 'A diagram of a handshake.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Plan and prepare to develop AI solutions on Azure',\n",
       "  'topic': 'Exercise - Prepare for an AI development project',\n",
       "  'text': \"If you have an Azure subscription, you can explore Azure AI Foundry for yourself. Note If you don't have an Azure subscription, and you want to explore Azure AI Foundry, you can sign up for an account , which includes credits for the first 30 days. Launch the exercise and follow the instructions.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2260323&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Plan and prepare to develop AI solutions on Azure',\n",
       "  'topic': 'Summary',\n",
       "  'text': \"In this module, you explored some of the key considerations when planning and preparing for AI application development. You've also had the opportunity to become familiar with Azure AI Foundry, the recommended platform for developing AI solutions on Azure. Tip For latest news and information about developing AI applications on Azure, see Azure AI .\",\n",
       "  'links': ['https://azure.microsoft.com/solutions/ai?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Choose and deploy models from the model catalog in Azure AI Foundry portal',\n",
       "  'topic': 'Introduction',\n",
       "  'text': 'Generative AI applications are built on language models . The development process usually starts with an exploration and comparison of available foundation models to find the one that best suits the particular needs of your application. After selecting a suitable model, you deploy it to an endpoint where it can be consumed by a client application or AI agent. Foundation models, such as the GPT family of models, are state-of-the-art language models designed to understand, generate, and interact with natural language. Some common use cases for models are: Speech-to-text and text-to-speech conversion . For example, generate subtitles for videos. Machine translation . For example, translate text from English to Japanese. Text classification . For example, label an email as spam or not spam. Entity extraction . For example, extract keywords or names from a document. Text summarization . For example, generate a short one-paragraph summary from a multi-page document. Question answering . For example, provide answers to questions like \"What is the capital of France?\" Reasoning . For example, solve a mathematical problem. In this module, you focus on exploring foundation models used for question answering. The foundation models you explore can be used for chat applications in which you use a language model to generate a response to a user\\'s question. Note The latest breakthrough in generative AI models is owed to the development of the Transformer architecture. Transformers were introduced in the Attention is all you need paper by Vaswani, et al. from 2017 . The Transformer architecture provided two innovations to NLP that resulted in the emergence of foundation models: Instead of processing words sequentially, Transformers process each word independently and in parallel by using attention . Next to the semantic similarity between words, Transformers use positional encoding to include the information about the position of a word in a sentence.',\n",
       "  'links': ['https://arxiv.org/abs/1706.03762?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Choose and deploy models from the model catalog in Azure AI Foundry portal',\n",
       "  'topic': 'Explore the model catalog',\n",
       "  'text': \"The model catalog in Azure AI Foundry provides a central repository of models that you can browse to find the right language model for your particular generative AI use case. Selecting a foundation model for your generative AI app is important as it affects how well your app works. To find the best model for your app, you can use a structured approach by asking yourself the following questions: Can AI solve my use case? How do I select the best model for my use case? Can I scale for real-world workloads? Let's explore each of these questions. Can AI solve my use case? Nowadays we have thousands of language models to choose from. The main challenge is to understand if there's a model that satisfies your needs and to answer the question: Can AI solve my use case? To start answering this question, you need to discover, filter, and deploy a model. You can explore the available language models through three different catalogs: Hugging Face : Vast catalog of open-source models across various domains. GitHub : Access to diverse models via GitHub Marketplace and GitHub Copilot. Azure AI Foundry : Comprehensive catalog with robust tools for deployment. Though you can use each of these catalogs to explore models, the model catalog in Azure AI Foundry makes it easiest to explore and deploy a model to build you prototype, while offering the best selection of models. Let's explore some of the options you need to consider when searching for suitable models. Choose between large and small language models First of all, you have a choice between Large Language Models (LLMs) and Small Language Models (SLMs). LLMs like GPT-4, Mistral Large, Llama3 70B, Llama 405B, and Command R+ are powerful AI models designed for tasks that require deep reasoning, complex content generation, and extensive context understanding. SLMs like Phi3, Mistral OSS models, and Llama3 8B are efficient and cost-effective, while still handling many common Natural Language Processing (NLP) tasks. They're perfect for running on lower-end hardware or edge devices, where cost and speed are more important than model complexity. Focus on a modality, task, or tool Language models like GPT-4 and Mistral Large are also known as chat completion models, designed to generate coherent and contextually appropriate text-based responses. When you need higher levels of performance in complex tasks like math, coding, science, strategy, and logistics, you can also use reasoning models like DeepSeek-R1 and o1. Beyond text-based AI, some models are multi-modal , meaning they can process images, audio, and other data types alongside text. Models like GPT-4o and Phi3-vision are capable of analyzing and generating both text and images. Multi-modal models are useful when your application needs to process and understand images, such as in computer vision or document analysis. Or when you want to build an AI app that interacts with visual content, such as a digital tutor explaining images or charts. If your use case involves generating images , tools like DALL·E 3 and Stability AI can create realistic visuals from text prompts. Image generation models are great for designing marketing materials, illustrations, or digital art. Another group of task-specific models are embedding models like Ada and Cohere. Embeddings models convert text into numerical representations and are used to improve search relevance by understanding semantic meaning. These models are often implemented in Retrieval Augmented Generation ( RAG ) scenarios to enhance recommendation engines by linking similar content. When you want to build an application that interacts with other software tools dynamically, you can add function calling and JSON support . These capabilities allow AI models to work efficiently with structured data, making them useful for automating API calls, database queries, and structured data processing. Specialize with regional and domain-specific models Certain models are designed for specific languages, regions, or industries. These models can outperform general-purpose generative AI in their respective domains. For example: Core42 JAIS is an Arabic language LLM, making it the best choice for applications targeting Arabic-speaking users. Mistral Large has a strong focus on European languages, ensuring better linguistic accuracy for multilingual applications. Nixtla TimeGEN-1 specializes in time-series forecasting, making it ideal for financial predictions, supply chain optimization, and demand forecasting. If your project has regional, linguistic, or industry-specific needs, these models can provide more relevant results than general-purpose AI. Balance flexibility and performance with open versus proprietary models You also need to decide whether to use open-source models or proprietary models, each with its own advantages. Proprietary models are best for cutting-edge performance and enterprise use. Azure offers models like OpenAI’s GPT-4, Mistral Large, and Cohere Command R+, which deliver industry-leading AI capabilities. These models are ideal for businesses needing enterprise-level security, support, and high accuracy. Open-source models are best for flexibility and cost-efficiency. There are hundreds of open-source models available in the Azure AI Foundry model catalog from Hugging Face, and models from Meta, Databricks, Snowflake, and Nvidia. Open models give developers more control, allowing fine-tuning, customization, and local deployment. Whatever model you choose, you can use the Azure AI Foundry model catalog. Using models through the model catalog meets the key enterprise requirements for usage: Data and privacy : you get to decide what happens with your data. Security and compliance : built-in security. Responsible AI and content safety : evaluations and content safety. Now you know the language models that are available to you, you should have an understanding of whether AI can indeed solve your use case. If you think a language model would enrich your application, you then need to select the specific model that you want to deploy and integrate. How do I select the best model for my use case? To select the best language model for you use case, you need to decide on what criteria you're using to filter the models. The criteria are the necessary characteristics you identify for a model. Four characteristics you can consider are: Task type : What type of task do you need the model to perform? Does it include the understanding of only text, or also audio, or video, or multiple modalities? Precision : Is the base model good enough or do you need a fine-tuned model that is trained on a specific skill or dataset? Openness : Do you want to be able to fine-tune the model yourself? Deployment : Do you want to deploy the model locally, on a serverless endpoint, or do you want to manage the deployment infrastructure? You already explored the various types of models available in the previous section. Now, let's explore in more detail how precision and performance can be important filters when choosing a model. Filter models for precision In generative AI, precision refers to the accuracy of the model in generating correct and relevant outputs. It measures the proportion of true positive results (correct outputs) among all generated outputs. High precision means fewer irrelevant or incorrect results, making the model more reliable. When integrating a language model into an app, you can choose between a base model or a fine-tuned model. A base model, like GPT-4, is pretrained on a large dataset and can handle various tasks but can lack precision for specific domains. Techniques like prompt engineering can improve this, but sometimes fine-tuning is necessary. A fine-tuned model is trained further on a smaller, task-specific dataset to improve its precision and ability to generate relevant outputs for specific applications. You can either use a fine-tuned model or fine-tune a model yourself. Filter models for performance You can evaluate your model performance at different phases, using various evaluation approaches. When you're exploring models through the Azure AI Foundry model catalog, you can use model benchmarks to compare publicly available metrics like coherence and accuracy across models and datasets. These benchmarks can help you in the initial exploration phase, but give little information on how the model would perform in your specific use case. Benchmark Description Accuracy Compares model-generated text with correct answer according to the dataset. Result is one if generated text matches the answer exactly, and zero otherwise. Coherence Measures whether the model output flows smoothly, reads naturally, and resembles human-like language. Fluency Assesses how well the generated text adheres to grammatical rules, syntactic structures, and appropriate usage of vocabulary, resulting in linguistically correct and natural-sounding responses. Groundedness Measures alignment between the model's generated answers and the input data. GPT Similarity Quantifies the semantic similarity between a ground truth sentence (or document) and the prediction sentence generated by an AI model. Quality index A comparative aggregate score between 0 and 1, with better-performing models scoring a higher value Cost The cost of using the model based on a price-per-token. Cost is a useful metric with which to compare quality, enabling you to determine an appropriate tradeoff for your needs. To evaluate how a selected model performs regarding your specific requirements, you can consider manual or automated evaluations. Manual evaluations allow you to rate your model's responses. Automated evaluations include traditional machine learning metrics and AI-assisted metrics that are calculated and generated for you. When you evaluate a model’s performance, it's common to start with manual evaluations, as they quickly assess the quality of the model’s responses. For more systematic comparisons, automated evaluations using metrics like precision, recall, and F1 score based on your own ground truth offer a faster, scalable, and more objective approach. Can I scale for real-world workloads? You selected a model for your use case and have successfully built a prototype. Now, you need to understand how to scale for real-world workloads. Considerations for scaling a generative AI solution include: Model deployment : Where will you deploy the model for the best balance of performance and cost? Model monitoring and optimization : How will you monitor, evaluate, and optimize model performance? Prompt management : How will you orchestrate and optimize prompts to maximize the accuracy and relevance of generated responses? Model lifecycle : How will you manage model, data, and code updates as part of an ongoing Generative AI Operations (GenAIOps) lifecycle? Azure AI Foundry provides visual and code-first tools that can help you build and maintain a scalable generative AI solution.\",\n",
       "  'links': ['https://huggingface.co/models',\n",
       "   'https://github.com/marketplace/models-github',\n",
       "   'https://ai.azure.com/explore/models'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/explore-models-azure-ai-studio/media/model-catalog.png',\n",
       "    'image_alt': 'Screenshot of the model catalog in Azure AI Foundry portal.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Choose and deploy models from the model catalog in Azure AI Foundry portal',\n",
       "  'topic': 'Deploy a model to an endpoint',\n",
       "  'text': \"When you develop a generative AI app, you need to integrate language models into your application. To be able to use a language model, you need to deploy the model. Let's explore how to deploy language models in the Azure AI Foundry, after first understanding why to deploy a model. Why deploy a model? You train a model to generate output based on some input. To get value out of your model, you need a solution that allows you to send input to the model, which the model processes, after which the output is visualized for you. With generative AI apps, the most common type of solution is a chat application that expects a user question, which the model processes, to generate an adequate response. The response is then visualized to the user as a response to their question. You can integrate a language model with a chat application by deploying the model to an endpoint . An endpoint is a specific URL where a deployed model or service can be accessed. Each model deployment typically has its own unique endpoint, which allows different applications to communicate with the model through an API ( Application Programming Interface ). When a user asks a question: An API request is sent to the endpoint. The endpoint specifies the model that processes the request. The result is sent back to the app through an API response. Now that you understand why you want to deploy a model, let's explore the deployment options with Azure AI Foundry. Deploy a language model with Azure AI Foundry When you deploy a language model with Azure AI Foundry, you have several types available, which depend on the model you want to deploy. Deploy options include: Standard deployment : Models are hosted in the Azure AI Foundry project resource. Serverless compute : Models are hosted in Microsoft-managed dedicated serverless endpoints in an Azure AI Foundry hub project. Managed compute : Models are hosted in managed virtual machine images in an Azure AI Foundry hub project. The associated cost depends on the type of model you deploy, which deployment option you choose, and what you are doing with the model: Standard deployment Serverless compute Managed compute Supported models Azure AI Foundry models (including Azure OpenAI models and Models-as-a-service models) Foundry Models with pay-as-you-go billing Open and custom models Hosting service Azure AI Foundry resource AI Project resource in a hub AI Project resource in a hub Billing basis Token-based billing Token-based billing Compute-based billing Note Standard deployment is recommended for most scenarios.\",\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/explore-models-azure-ai-studio/media/request-endpoint.png',\n",
       "    'image_alt': 'Diagram of user question being processed by model deployed to endpoint.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Choose and deploy models from the model catalog in Azure AI Foundry portal',\n",
       "  'topic': 'Optimize model performance',\n",
       "  'text': \"After you deploy your model to an endpoint, you can start interacting with it to see how it works. Let's explore how you can use prompt engineering techniques to optimize your model's performance. Apply prompt patterns to optimize your model's output The quality of the questions you send to the language model, directly influences the quality of the responses you get back. You can carefully construct your question, or prompt , to receive better and more interesting responses. The process of designing and optimizing prompts to improve the model's performance is also known as prompt engineering . Prompt engineering requires users to ask relevant, specific, unambiguous, and well-structured questions, instructing the model to generate more accurate responses. To understand how to create well-defined prompts, let's explore some patterns that help you improve the output of a model: Instruct the model to act as a persona. Guide the model to suggest better questions. Provide a template to generate output in a specific format. Understand how a model reasons by asking it to reflect. Add context to improve the accuracy of the model's output. Note The patterns listed here are based on the work by White et al., (2023) who constructed a prompt pattern catalog . As a developer, an effective approach is to add instructions to your deployed model in the form of a system prompt . The system prompt sets the model's behavior and allows you to guide the model without exposing the end user to the instructions. The best results are often achieved through the assignment of an explicit system prompt and guidance (or templates) that help users submit prompts that reflect the patterns described here. Instruct the model to act as a persona When you want the model to take a specific point of view or perspective when answering a question, you can ask it to act as a persona. For example, you can ask the model to respond as if it's a seasoned marketing professional if you're working in sales and need insights about a new CRM software. No persona specified With persona specified System prompt None You're a seasoned marketing professional that writes advertising copy for an audience of technical customers. User prompt Write a one-sentence description of a CRM product. Write a one-sentence description of a CRM product. Model response A CRM product is a software tool designed to manage a company's interactions with current and potential customers, encompassing sales, marketing, customer service, and technical support. Experience seamless customer relationship management with our CRM product, designed to streamline your business operations, enhance customer interactions, and drive sales growth with its intuitive interface and robust analytics tools. You can create different personas for various business roles. Think of roles like a product manager, data analyst, customer service expert, or teacher. Define the persona and add more detail to what you expect that persona to know or do to get tailored, context-driven responses. Ask for better question suggestions When you want the model to help refine your question, you can ask it to suggest better ways to phrase your query. You can also ask it to provide extra context. For example, if you're seeking advice on cooking a meal, you can prompt the model to suggest clarifying questions to help you get a more targeted answer. No suggestions given With suggestions given System prompt You're an AI chef who can advise on menus. You're an AI chef who can advise on menus. You're approachable and always want to provide the best advice you can. User prompt I have to host a dinner party for four people. What should I cook? I have to host a dinner party for four people. What should I cook? What other information do you need to help me plan a great meal for my guests? Model response Hosting a dinner party for four is a wonderful opportunity to showcase your culinary skills. Here's a suggested three-course menu that balances flavors and provides a pleasant dining experience: Appetizer Caprese Salad Skewers - Ingredients : Cherry tomatoes, fresh mozzarella balls, fresh basil leaves, balsamic glaze, olive oil, salt, and pepper. - Instructions : Thread a cherry tomato, a basil leaf, and a mozzarella ball onto small skewers. Drizzle with olive oil and balsamic glaze, then season with salt and pepper. This dish is light, refreshing, and easy to prepare ahead of time. Main Course Herb-Crusted Salmon with Lemon Asparagus - Ingredients : Salmon fillets, fresh herbs (such as parsley, dill, and thyme), breadcrumbs, lemon, asparagus, olive oil, salt, and pepper. - Instructions : Preheat your oven to 400°F (200°C). Mix breadcrumbs with chopped fresh herbs and a bit of olive oil to form a paste. Press this mixture onto the salmon fillets and season with salt and pepper. Place salmon on a baking sheet and bake for about 15-20 minutes, or until cooked through. In the last 10 minutes of cooking, toss asparagus with olive oil, lemon zest, salt, and pepper, and roast alongside the salmon. Serve with a squeeze of fresh lemon juice over the salmon and asparagus. Dessert Chocolate Mousse with Fresh Berries - Ingredients : Dark chocolate, eggs, sugar, heavy cream, vanilla extract, and mixed berries (such as strawberries, blueberries, and raspberries). - Instructions : Melt the chocolate over a double boiler, then let it cool slightly. Separate egg yolks and whites. Whip the egg whites to soft peaks, then gradually add sugar to make a meringue. Whip the heavy cream until soft peaks form and mix in vanilla extract. Fold the melted chocolate into the egg yolks, then gently fold in the whipped cream and meringue until combined. Spoon the mousse into serving dishes and refrigerate for at least 2 hours. Serve topped with fresh berries for a burst of color and freshness. Additional Tips - Pairings : Consider serving a light white wine with the appetizer and main course, and perhaps a dessert wine with the chocolate mousse. - Timing : Prepare the appetizer and dessert in advance to minimize last-minute stress. The main course will take the most time in the oven, so plan accordingly. This menu is designed to be elegant yet approachable, ensuring your guests enjoy both the food and the experience. Enjoy your dinner party! Hosting a dinner party is a wonderful opportunity to showcase your culinary skills and create memorable experiences for your guests. To help you plan a great meal, here are a few questions that can guide you in choosing the perfect menu: Dietary Preferences/Restrictions : Are any of your guests vegetarian, vegan, gluten-free, or have any food allergies or intolerances? Knowing this will help in selecting dishes that everyone can enjoy. Flavor Preferences : Do your guests have any favorite cuisines or flavors? For example, do they enjoy Italian, Mexican, Mediterranean, or perhaps something more adventurous like Thai or Indian? Time and Skill Level : How much time do you have to prepare the meal, and what is your comfort level in the kitchen? Are you looking for a simple meal or are you up for something more elaborate? Occasion : Is this a casual get-together, a celebration, or something more formal? The nature of the event might influence the menu style. Seasonal Ingredients : Are you interested in using seasonal ingredients? This can help in choosing fresh and flavorful produce. Drinks Pairing : Would you like to include alcoholic or non-alcoholic beverages to pair with the meal? With this information, I can suggest a tailored menu. Here’s a general idea that can be adapted based on your answers: Starter Garden Salad with Citrus Vinaigrette : Fresh greens, seasonal vegetables, and a light citrus dressing. Stuffed Mushrooms : For a vegetarian option, filled with herbs and cheese. Main Course Herb-Crusted Chicken Breast : Served with roasted vegetables. Quinoa Stuffed Bell Peppers : A flavorful vegetarian or vegan option. Side Dishes Garlic Butter Roasted Potatoes : Classic and comforting. Steamed Green Beans with Almonds : A simple, healthy side. Dessert Chocolate Lava Cake : A decadent treat to end the meal. Fruit Tart : A lighter, refreshing option. Drinks Wine Pairing : A versatile white wine like Sauvignon Blanc or a light red like Pinot Noir. Mocktail Option : A refreshing citrus mocktail or sparkling water with herbs. Feel free to share more details, and I can help refine the menu further! When you encourage the model to suggest clarifications or extra details can help you ask the right questions and achieve better, more accurate answers in fewer interactions. Specify the desired format for responses When you want the model to generate output in a specific format, you can provide a template or structure in your prompt. For example, if you're a sports reporting composing a historical article, you can request that the model follow a specific template, which includes headings, bullet points, and data breakdowns. No template specified With template specified System prompt You're a helpful AI assistant. You're a helpful AI assistant for sports reporters. User prompt What happened in the 2018 Soccer World Cup final? What happened in the 2018 Soccer World Cup final? Format the result to show the match date, location, and the two teams competing. Then the final score, and finally any notable events that occurred during the match. Model response The 2018 FIFA World Cup final took place on July 15, 2018, at the Luzhniki Stadium in Moscow, Russia. France faced Croatia in the match. France emerged victorious with a 4-2 win. The goals for France were scored by Mario Mandzukic (own goal), Antoine Griezmann (penalty), Paul Pogba, and Kylian Mbappé. Croatia's goals were scored by Ivan Perišić and Mandzukic. Kylian Mbappé, at just 19 years old, became the second youngest player to score in a World Cup final, after Pelé. This victory marked France's second World Cup title, their first being in 1998. Match Date : July 15, 2018 Location : Luzhniki Stadium, Moscow, Russia Teams Competing : France vs. Croatia Final Score : France 4, Croatia 2 Notable Events : First Half : France took an early lead in the 18th minute with an own goal by Mario Mandžukić after a cross from Antoine Griezmann. Croatia equalized in the 28th minute with a header from Ivan Perišić, who had just come on as a substitute. Just before halftime, in the 44th minute, Paul Pogba scored from a free kick to put France back in the lead. Second Half : The game saw a dramatic increase in intensity after the break. In the 68th minute, Kylian Mbappé scored his first World Cup goal, bringing the score to 3-1 in favor of France. Croatia managed to score again in the 69th minute when Mandžukić headed in a cross from Perišić, reducing the deficit to 3-2. Final Moments : In the 81st minute, a controversial penalty was awarded to France after a handball by Croatian defender Dejan Lovren. Griezmann converted the penalty, sealing the victory for France with a 4-2 scoreline. Additional Context : This match was notable for being the youngest ever World Cup final in terms of average age, featuring players like Mbappé and N’Golo Kanté. France’s victory marked their second World Cup title, having previously won in 1998. Croatia reached the final for the first time in their history, finishing as runners-up. You can apply this approach to other scenarios where a specific format is needed, such as generating emails, summaries, proposals, or even code and scripts. Define the format template clearly and provide details on how you want the output structured to get consistent and organized responses. You can also use a one-shot or few-shots approach by providing one or more examples to help the model identify a desired pattern. Ask for an explanation of reasoning When you want the model to explain the reasoning behind its answers, you can ask the model to automatically reflect on its rationale and assumptions after providing a response. For example, if you're working on a mathematical problem, you can ask the model to explain the reasoning behind specific calculations. No reflection specified With reflection specified System prompt You're an AI math assistant. You're an AI math assistant. You always explain your answers. User prompt A right-angled triangle has a hypotenuse side of length 3 cm and an adjacent side length of 2cm.What is the length of the remaining side? A right-angled triangle has a hypotenuse side of length 3 cm and an adjacent side length of 2cm.What is the length of the remaining side? Model response Approximately 2.24 cm. We can use the Pythagorean theorem to solve this problem. The Pythagorean theorem states that in a right-angled triangle, the square of the length of the hypotenuse (the side opposite the right angle) is equal to the sum of the squares of the lengths of the other two sides. Mathematically, this can be written as: c² = a² + b² where c is the length of the hypotenuse, and a and b are the lengths of the other two sides. In this problem, we are given the length of the hypotenuse (c = 3 cm) and the length of one of the other sides (a = 2 cm). We need to find the length of the remaining side (b). Plugging the given values into the Pythagorean theorem, we get: 3² = 2² + b² 9 = 4 + b² Now, we can solve for b²: b² = 9 - 4 b² = 5 Finally, we take the square root of both sides to find the length of the remaining side: b = √5 ≈ 2.24 cm So, the length of the remaining side is approximately 2.24 cm. You can apply this approach when you want explanations in data analysis, marketing strategy, or technical troubleshooting. When you ask the model to define its reasoning, you use a technique called chain-of-thought to make it think step by step. Add context When you want the model to focus on specific topics, you can specify the context to consider. You can also tell the model to ignore irrelevant information. For example, if you're planning a trip, you can provide the model with more context to help improve the relevance of its response. No context specified With context specified System prompt You're an AI travel assistant. You're an AI travel assistant. User question When should I visit Edinburgh? When should I visit Edinburgh? I'm particularly interested in attending Scotland's home matches in the Six Nations rugby tournament. Model response Edinburgh, the capital city of Scotland, is a wonderful destination with a rich history and vibrant culture. The best time to visit Edinburgh largely depends on your preferences for weather, events, and tourist crowds. Spring (April to June): The weather starts to warm up, and the days get longer. Springtime is perfect for exploring the city's historic sites, gardens, and parks, such as the Royal Botanic Garden. The Edinburgh Festival Fringe, the world's largest arts festival, takes place in August, which is the end of summer. Summer (July to August): The weather is at its warmest, and the days are long. This is a popular time to visit, but it can also be crowded with tourists. Summer is ideal for outdoor activities, and you can enjoy the Edinburgh International Festival and the Royal Edinburgh Military Tattoo. Autumn (September to November): The weather begins to cool down, and the leaves change color, creating a picturesque backdrop for the city's historic architecture. It's less crowded, and you can enjoy events like the Edinburgh International Book Festival. Winter (December to February): The weather is colder, and there may be snowfall, which can create a magical atmosphere. Winter is a quieter time to visit, with fewer tourists and lower prices. You can attend events like the Edinburgh Christmas Market and the Hogmanay Street Party, which celebrates the New Year. In summary, the best time to visit Edinburgh depends on your preferences. Spring and autumn offer pleasant weather and fewer crowds, while summer is ideal for outdoor activities and festivals. Winter can be a quieter, more magical experience, with festive events to enjoy. The Six Nations rugby tournament typically runs from late January to early March. Edinburgh, as a host city, often has matches at Murrayfield Stadium, which is Scotland's national stadium. To ensure you don't miss Scotland's home matches in the Six Nations, I recommend planning your visit for this period. However, keep in mind that match dates and times can vary, so it's a good idea to check the official Six Nations website or the Scottish Rugby Union's website for the exact schedule of matches. Additionally, Edinburgh has a lot to offer beyond rugby, with its rich history, stunning architecture, and vibrant cultural scene, so you might consider extending your stay to explore the city before or after the tournament. Also, remember that visiting during the tournament might mean larger crowds and busier accommodations, so book your flights, hotel, and transportation well in advance to ensure a smooth experience. Enjoy your trip to Edinburgh and the thrilling atmosphere of the Six Nations rugby matches! By defining what the model should focus on or disregard, you can ensure the conversation stays on track and generate more relevant, tailored responses. You can specify the context by describing what it should or shouldn't include, and by connecting the model to data sources it should retrieve context from before generating an answer. Apply model optimization strategies Note This section discusses options and considerations for model optimization that you may consider beyond prompt engineering. A full exploration of how to apply these optimization strategies is beyond the scope of this module. Prompt engineering can be an effective way to optimize model responses, but in some cases it may not provide sufficient context or guidance to always meet your exact needs. As a developer, you can consider the following additional optimization strategies to improve the relevance of your generative AI application's responses: Retrieval Augmented Generation (RAG) : A technique that involves using a data source to provide grounding context to prompts. RAG can be a useful approach when you need the model to answer questions based on a specific knowledge domain or when you need the model to consider information related to events that occurred after the training data on which the model is based. Fine-tuning : A technique that involves extending the training of a foundation model by providing example prompts and responses that reflect the desired output format and style. Both of these approaches involve additional cost, complexity, and maintainability challenges, so as a general rule it's best to start your optimization efforts through prompt engineering, and then consider additional strategies if necessary. The strategy you should choose as a developer depends on your requirements: Optimize for context : When the model lacks contextual knowledge and you want to maximize responses accuracy . Optimize the model : When you want to improve the response format, style, or speech by maximizing consistency of behavior . To optimize for context, you can apply a Retrieval Augmented Generation ( RAG ) pattern. With RAG, you ground your data by first retrieving context from a data source before generating a response. For example, you want employees to ask questions about expense claim processes and limits based on your own corporation's expenses policy documentation. When you want the model to respond in a specific style or format, you can instruct the model to do so by adding guidelines in the system message. When you notice the model's behavior isn't consistent, you can further enforce consistency in behavior by fine-tuning a model. With fine-tuning, you train a base language model on a dataset of example prompts and responses before integrating it in your application, with the result that the fine-tuned model will produce responses that are consistent with the examples in the fine-tuning training dataset. You can use any combination of optimization strategies, for example prompt engineering, RAG and a fine-tuned model, to improve your language application.\",\n",
       "  'links': ['https://arxiv.org/abs/2302.11382?azure-portal=true'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/explore-models-azure-ai-studio/media/model-optimization.png',\n",
       "    'image_alt': \"Diagram showing the various strategies to optimize the model's performance.\"}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Choose and deploy models from the model catalog in Azure AI Foundry portal',\n",
       "  'topic': 'Exercise - Explore, deploy, and chat with language models',\n",
       "  'text': \"Now, it's your chance to explore how to select, deploy, and test language models in the Azure AI Foundry portal. Note To complete this lab, you will need an Azure subscription in which you have administrative access. Launch the exercise and follow the instructions.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2277718&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Choose and deploy models from the model catalog in Azure AI Foundry portal',\n",
       "  'topic': 'Summary',\n",
       "  'text': 'In this module, you learned how to: Select a language model from the model catalog. Deploy a model to an endpoint. Test a model and improve the performance of the model. Learn more Model catalog and collections in Azure AI Foundry portal Deploy AI models in Azure AI Foundry portal Prompt engineering techniques Azure AI Foundry Discord Azure AI Foundry Developer Forum',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/model-catalog-overview',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/deployments-overview',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering',\n",
       "   'https://aka.ms/azureaifoundry/discord',\n",
       "   'https://aka.ms/azureaifoundry/forum'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Develop an AI app with the Azure AI Foundry SDK',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"Developers creating AI solutions with Azure AI Foundry need to work with a combination of services and software frameworks. The Azure AI Foundry SDK is designed to bring together common services and code libraries in an AI project through a central programmatic access point, making it easier for developers to write the code needed to build effective AI apps on Azure. In this module, you'll learn how to use the Azure AI Foundry SDK to work with resources in an AI project. Note Azure AI Foundry SDK is currently in public preview. Details described in this module are subject to change.\",\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Develop an AI app with the Azure AI Foundry SDK',\n",
       "  'topic': 'What is the Azure AI Foundry SDK?',\n",
       "  'text': 'Azure AI Foundry provides a REST API that you can use to work with AI Foundry projects and the resources they contain. Additionally, multiple language-specific SDKs are available, enabling developers to write code that uses resources in an Azure AI Foundry project in their preferred development language. With an Azure AI Foundry SDK, developers can create applications that connect to a project, access the resource connections and models in that project, and use them to perform AI operations, such as sending prompts to a generative AI model and processing the responses. The core package for working with projects is the Azure AI Projects library, which enables you to connect to an Azure AI Foundry project and access the resources defined within it. Available language-specific packages the for Azure AI Projects library include: Azure AI Projects for Python Azure AI Projects for Microsoft .NET Azure AI Projects for JavaScript Note In this module, we\\'ll use Python code examples for common tasks that a developer may need to perform with Azure AI Foundry projects. You can refer to the other language-specific SDK documentation to find equivalent code for your preferred language. Each SDK is developed and maintained independently, so some functionality may be at different stages of implementation for each language. To use the Azure AI Projects library in Python, you can use the pip package installation utility to install the azure-ai-projects package from PyPi: pip install azure-ai-projects Using the SDK to connect to a project The first task in most Azure AI Foundry SDK code is to connect to an Azure AI Foundry project. Each project has a unique endpoint , which you can find on the project\\'s Overview page in the Azure AI Foundry portal. Note The project provides multiple endpoints and keys, including: An endpoint for the project itself; which can be used to access project connections, agents, and models in the Azure AI Foundry resource. An endpoint for Azure OpenAI Service APIs in the project\\'s Azure AI Foundry resource. An endpoint for Azure AI services APIs (such as Azure AI Vision and Azure AI Language) in the Azure AI Foundry resource. You can use the project endpoint in your code to create an AIProjectClient object, which provides a programmatic proxy for the project, as shown in this Python example: from azure.identity import DefaultAzureCredential\\nfrom azure.ai.projects import AIProjectClient\\n...\\n\\nproject_endpoint = \"https://......\"\\nproject_client = AIProjectClient(            \\n    credential=DefaultAzureCredential(),\\n    endpoint=project_endpoint) Note The code uses the default Azure credentials to authenticate when accessing the project. To enable this authentication, in addition to the azure-ai-projects package, you need to install the azure-identity package: pip install azure-identity Tip To access the project successfully, the code must be run in the context of an authenticated Azure session. For example, you could use the Azure command-line interface (CLI) az-login command to sign in before running the code.',\n",
       "  'links': ['https://pypi.org/project/azure-ai-projects?azure-portal=true',\n",
       "   'https://www.nuget.org/packages/Azure.AI.Projects?azure-portal=true',\n",
       "   'https://www.npmjs.com/package/@azure/ai-projects?azure-portal=true',\n",
       "   'https://learn.microsoft.com/wwl-data-ai/ai-foundry-sdk/media/ai-project-overview.png#lightbox'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/ai-foundry-sdk/media/ai-project-overview.png',\n",
       "    'image_alt': 'Screenshot of the project overview page in Azure AI Foundry portal.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Develop an AI app with the Azure AI Foundry SDK',\n",
       "  'topic': 'Work with project connections',\n",
       "  'text': 'Each Azure AI Foundry project includes connected resources , which are defined both at the parent (Azure AI Foundry resource or hub) level, and at the project level. Each resource is a connection to an external service, such as Azure storage, Azure AI Search, Azure OpenAI, or another Azure AI Foundry resource. With the Azure AI Foundry SDK, you can connect to a project and retrieve connections; which you can then use to consume the connected services. For example, the AIProjectClient object in Python has a connections property, which you can use to access the resource connections in the project. Methods of the connections object include: connections.list() : Returns a collection of connection objects, each representing a connection in the project. You can filter the results by specifying an optional connection_type parameter with a valid enumeration, such as ConnectionType.AZURE_OPEN_AI . connections.get(connection_name, include_credentials) : Returns a connection object for the connection with the name specified. If the include_credentials parameter is True (the default value), the credentials required to connect to the connection are returned - for example, in the form of an API key for an Azure AI services resource. The connection objects returned by these methods include connection-specific properties, including credentials, which you can use to connect to the associated resource. The following code example lists all of the resource connections that have been added to a project: from azure.identity import DefaultAzureCredential\\nfrom azure.ai.projects import AIProjectClient\\n\\ntry:\\n\\n    # Get project client\\n    project_endpoint = \"https://.....\"\\n    project_client = AIProjectClient(            \\n            credential=DefaultAzureCredential(),\\n            endpoint=project_endpoint,\\n        )\\n    \\n    ## List all connections in the project\\n    connections = project_client.connections\\n    print(\"List all connections:\")\\n    for connection in connections.list():\\n        print(f\"{connection.name} ({connection.type})\")\\n\\nexcept Exception as ex:\\n    print(ex)',\n",
       "  'links': ['https://learn.microsoft.com/wwl-data-ai/ai-foundry-sdk/media/ai-project-connections.png#lightbox'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/ai-foundry-sdk/media/ai-project-connections.png',\n",
       "    'image_alt': 'Screenshot of the connected resources page in Azure AI Foundry portal.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Develop an AI app with the Azure AI Foundry SDK',\n",
       "  'topic': 'Create a chat client',\n",
       "  'text': 'A common scenario in an AI application is to connect to a generative AI model and use prompts to engage in a chat-based dialog with it. While you can use the Azure OpenAI SDK, to connect \"directly\" to a model using key-based or Microsoft Entra ID authentication; when your model is deployed in an Azure AI Foundry project, you can also use the Azure AI Foundry SDK to retrieve a project client, from which you can then get an authenticated OpenAI chat client for any models deployed in the project\\'s Azure AI Foundry resource. This approach makes it easy to write code that consumes models deployed in your project, switching between them easily by changing the model deployment name parameter. Tip You can use the OpenAI chat client provided by an Azure AI Foundry project to chat with any model deployed in the associated Azure AI Foundry resource - even non-OpenAI models, such as Microsoft Phi models. The following Python code sample uses the get_openai_client() method to get an OpenAI client with which to chat with a model that has been deployed in the project\\'a Azure AI Foundry resource. from azure.identity import DefaultAzureCredential\\nfrom azure.ai.projects import AIProjectClient\\nfrom openai import AzureOpenAI\\n\\ntry:\\n    \\n    # connect to the project\\n    project_endpoint = \"https://......\"\\n    project_client = AIProjectClient(            \\n            credential=DefaultAzureCredential(),\\n            endpoint=project_endpoint,\\n        )\\n    \\n    # Get a chat client\\n    chat_client = project_client.get_openai_client(api_version=\"2024-10-21\")\\n    \\n    # Get a chat completion based on a user-provided prompt\\n    user_prompt = input(\"Enter a question:\")\\n    \\n    response = chat_client.chat.completions.create(\\n        model=your_model_deployment_name,\\n        messages=[\\n            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\\n            {\"role\": \"user\", \"content\": user_prompt}\\n        ]\\n    )\\n    print(response.choices[0].message.content)\\n\\nexcept Exception as ex:\\n    print(ex) Note In addition to the azure-ai-projects and azure-identity packages discussed previously, the sample code shown here assumes that the openai package has been installed: pip install openai',\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Develop an AI app with the Azure AI Foundry SDK',\n",
       "  'topic': 'Exercise - Create a generative AI chat app',\n",
       "  'text': \"Now it's your turn to try using the Azure AI Foundry SDK! In this exercise, you use the Azure AI Foundry SDK to connect to a project and create an application that chats with a generative AI model. Note To complete this lab, you need an Azure subscription in which you have administrative access. Launch the exercise and follow the instructions.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2303427&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Develop an AI app with the Azure AI Foundry SDK',\n",
       "  'topic': 'Summary',\n",
       "  'text': 'By using the Azure AI Foundry SDK, you can develop rich AI applications that use resources in your Azure AI Foundry projects. The Azure AI Foundry SDK AIProjectClient class provides a programmatic proxy for a project, enabling you to access connected resources and to use service-specific libraries to consume them. Learn more The Azure AI Foundry SDK Azure AI Foundry Discord Azure AI Foundry Developer Forum',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/sdk-overview',\n",
       "   'https://aka.ms/azureaifoundry/discord',\n",
       "   'https://aka.ms/azureaifoundry/forum'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Get started with prompt flow to develop language model apps in the Azure AI Foundry',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"The true power of Large Language Models ( LLMs ) lies in their application. Whether you want to use LLMs to classify web pages into categories, or to build a chatbot on your data. To harness the power of the LLMs available, you need to create an application that combines your data sources with LLMs and generates the desired output. To develop, test, tune, and deploy LLM applications, you can use prompt flow , accessible in the Azure Machine Learning studio and the Azure AI Foundry portal . Note The focus of this module is on understanding and exploring prompt flow through Azure AI Foundry. However, note that the content applies to the prompt flow experience in both Azure Machine Learning and Azure AI Foundry. Prompt flow takes a prompt as input, which in the context of LLMs, refers to the query provided to the LLM application to generate a response. It's the text or set of instructions given to the LLM application, prompting it to generate output or perform a specific task. For example, when you want to use a text generation model, the prompt might be a sentence or a paragraph that initiates the generation process. In the context of a question-answering model, the prompt could be a query asking for information on a particular topic. The effectiveness of the prompt often depends on how well it conveys the user's intent and the desired outcome. Prompt flow allows you to create flows , which refers to the sequence of actions or steps that are taken to achieve a specific task or functionality. A flow represents the overall process or pipeline that incorporates the interaction with the LLM to address a particular use case. The flow encapsulates the entire journey from receiving input to generating output or performing a desired action.\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/overview-what-is-prompt-flow?azure-portal=true',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-studio/how-to/prompt-flow?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Get started with prompt flow to develop language model apps in the Azure AI Foundry',\n",
       "  'topic': 'Understand the development lifecycle of a large language model (LLM) app',\n",
       "  'text': \"Before understanding how to work with prompt flow, let's explore the development lifecycle of a Large Language Model (LLM) application. The lifecycle consists of the following stages: Initialization : Define the use case and design the solution. Experimentation : Develop a flow and test with a small dataset. Evaluation and refinement : Assess the flow with a larger dataset. Production : Deploy and monitor the flow and application. During both evaluation and refinement, and production, you might find that your solution needs to be improved. You can revert back to experimentation during which you develop your flow continuously, until you're satisfied with the results. Let's explore each of these phases in more detail. Initialization Imagine you want to design and develop an LLM application to classify news articles. Before you start creating anything, you need to define what categories you want as output. You need to understand what a typical news article looks like, how you present the article as input to your application, and how the application generates the desired output. In other words, during initialization you: Define the objective Collect a sample dataset Build a basic prompt Design the flow To design, develop, and test an LLM application, you need a sample dataset that serves as the input. A sample dataset is a small representative subset of the data you eventually expect to parse as input to your LLM application. When collecting or creating the sample dataset, you should ensure diversity in the data to cover various scenarios and edge cases. You should also remove any privacy sensitive information from the dataset to avoid any vulnerabilities. Experimentation You collected a sample dataset of news articles, and decided on which categories you want the articles to be classified into. You designed a flow that takes a news article as input, and uses an LLM to classify the article. To test whether your flow generates the expected output, you run it against your sample dataset. The experimentation phase is an iterative process during which you (1) run the flow against a sample dataset. You then (2) evaluate the prompt's performance. If you're (3) satisfied with the result, you can move on to evaluation and refinement. If you think there's room for improvement, you can (4) modify the flow by changing the prompt or flow itself. Evaluation and refinement When you're satisfied with the output of the flow that classifies news articles, based on the sample dataset, you can assess the flow's performance against a larger dataset. By testing the flow on a larger dataset, you can evaluate how well the LLM application generalizes to new data. During evaluation, you can identify potential bottlenecks or areas for optimization or refinement. When you edit your flow, you should first run it against a smaller dataset before running it again against a larger dataset. Testing your flow with a smaller dataset allows you to more quickly respond to any issues. Once your LLM application appears to be robust and reliable in handling various scenarios, you can decide to move the LLM application to production. Production Finally, your news article classification application is ready for production . During production, you: Optimize the flow that classifies incoming articles for efficiency and effectiveness. Deploy your flow to an endpoint. When you call the endpoint, the flow is triggered to run and the desired output is generated. Monitor the performance of your solution by collecting usage data and end-user feedback. By understanding how the application performs, you can improve the flow whenever necessary. Explore the complete development lifecycle Now that you understand each stage of the development lifecycle of an LLM application, you can explore the complete overview:\",\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/get-started-prompt-flow-ai-studio/media/development-lifecycle.png',\n",
       "    'image_alt': 'Diagram of the four stages of the development lifecycle.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/get-started-prompt-flow-ai-studio/media/initialization.png',\n",
       "    'image_alt': 'Diagram of the four steps during initialization.'},\n",
       "   {'image_name': 'image3',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/get-started-prompt-flow-ai-studio/media/experimentation.png',\n",
       "    'image_alt': 'Diagram of the four steps during experimentation.'},\n",
       "   {'image_name': 'image4',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/get-started-prompt-flow-ai-studio/media/production.png',\n",
       "    'image_alt': 'Diagram of the three steps during production.'},\n",
       "   {'image_name': 'image5',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/get-started-prompt-flow-ai-studio/media/detailed-lifecycle.png',\n",
       "    'image_alt': 'Diagram of all stages including their steps of the development lifecycle.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Get started with prompt flow to develop language model apps in the Azure AI Foundry',\n",
       "  'topic': 'Understand core components and explore flow types',\n",
       "  'text': \"To create a Large Language Model (LLM) application with prompt flow, you need to understand prompt flow's core components. Understand a flow Prompt flow is a feature within Azure AI Foundry that allows you to author flows . Flows are executable workflows often consist of three parts: Inputs : Represent data passed into the flow. Can be different data types like strings, integers, or boolean. Nodes : Represent tools that perform data processing, task execution, or algorithmic operations. Outputs : Represent the data produced by the flow. Similar to a pipeline, a flow can consist of multiple nodes that can use the flow's inputs or any output generated by another node. You can add a node to a flow by choosing one of the available types of tools . Explore the tools available in prompt flow Three common tools are: LLM tool : Enables custom prompt creation utilizing Large Language Models. Python tool : Allows the execution of custom Python scripts. Prompt tool : Prepares prompts as strings for complex scenarios or integration with other tools. Each tool is an executable unit with a specific function. You can use a tool to perform tasks like summarizing text, or making an API call. You can use multiple tools within one flow and use a tool multiple times. Tip If you're looking for functionality that is not offered by the available tools, you can create your own custom tool . Whenever you add a new node to your flow, adding a new tool, you can define the expected inputs and outputs. A node can use one of the whole flow's inputs, or another node's output, effectively linking nodes together. By defining the inputs, connecting nodes, and defining the desired outputs, you can create a flow. Flows help you create LLM applications for various purposes. Understand the types of flows There are three different types of flows you can create with prompt flow: Standard flow : Ideal for general LLM-based application development, offering a range of versatile tools. Chat flow : Designed for conversational applications, with enhanced support for chat-related functionalities. Evaluation flow : Focused on performance evaluation, allowing the analysis and improvement of models or applications through feedback on previous runs. Now that you understand how a flow is structured and what you can use it for, let's explore how you can create a flow.\",\n",
       "  'links': ['https://microsoft.github.io/promptflow/how-to-guides/develop-a-tool/create-and-use-tool-package.html?azure-portal=true'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/get-started-prompt-flow-ai-studio/media/flow-pipeline.png',\n",
       "    'image_alt': 'Diagram of the three components of a flow pipeline.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Get started with prompt flow to develop language model apps in the Azure AI Foundry',\n",
       "  'topic': 'Explore connections and runtimes',\n",
       "  'text': \"When you create a Large Language Model (LLM) application with prompt flow, you first need to configure any necessary connections and runtimes . Explore connections Whenever you want your flow to connect to external data source, service, or API, you need your flow to be authorized to communicate with that external service. When you create a connection , you configure a secure link between prompt flow and external services, ensuring seamless and safe data communication. Depending on the type of connection you create, the connection securely stores the endpoint, API key, or credentials necessary for prompt flow to communicate with the external service. Any necessary secrets aren't exposed to users, but instead are stored in an Azure Key Vault. By setting up connections, users can easily reuse external services necessary for tools in their flows. Certain built-in tools require you to have a connection configured: Connection type Built-in tools Azure OpenAI LLM or Python OpenAI LLM or Python Azure AI Search Vector DB Lookup or Python Serp Serp API or Python Custom Python Prompt flow connections play pivotal roles in two scenarios. They automate API credential management, simplifying and securing the handling of sensitive access information. Additionally, they enable secure data transfer from various sources, crucial for maintaining data integrity and privacy across different environments. Explore runtimes After creating your flow, and configuring the necessary connections your tools use, you want to run your flow. To run the flow, you need compute, which is offered through prompt flow runtimes . Runtimes (1) are a combination of a compute instance (2) providing the necessary compute resources, and an environment (3) specifying the necessary packages and libraries that need to be installed before being able to run the flow. When you use runtimes, you have a controlled environment where flows can be run and validated, ensuring that everything works as intended in a stable setting. A default environment is available for quick development and testing. When you require other packages to be installed, you can create a custom environment .\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/how-to-customize-environment-runtime?azure-portal=true'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/get-started-prompt-flow-ai-studio/media/connections.png',\n",
       "    'image_alt': 'Diagram showing a flow with two nodes, connecting to Azure AI Search and Azure OpenAI.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/get-started-prompt-flow-ai-studio/media/runtimes.png',\n",
       "    'image_alt': 'Diagram runtimes, compute, and environments in relation to flows.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Get started with prompt flow to develop language model apps in the Azure AI Foundry',\n",
       "  'topic': 'Explore variants and monitoring options',\n",
       "  'text': \"During production, you want to optimize and deploy your flow. Finally, you want to monitor your flows to understand when improving your flows is necessary. You can optimize your flow by using variants , you can deploy your flow to an endpoint , and you can monitor your flow by evaluating key metrics. Explore variants Prompt flow variants are versions of a tool node with distinct settings. Currently, variants are only supported in the LLM tool, where a variant can represent a different prompt content or connection setting. Variants allow users to customize their approach for specific tasks, like, summarizing news articles. Some benefits of using variants are: Enhance the quality of your LLM generation : Creating diverse variants of an LLM node helps find the best prompt and settings for high-quality content. Save time and effort : Variants allow for easy management and comparison of different prompt versions, streamlining historical tracking and reducing the effort in prompt tuning. Boost productivity : They simplify the optimization of LLM nodes, enabling quicker creation and management of variations, leading to better results in less time. Facilitate easy comparison : Variants enable side-by-side result comparisons, aiding in choosing the most effective variant based on data-driven decisions. Deploy your flow to an endpoint When you're satisfied with the performance of your flow, you can choose to deploy it to an online endpoint . Endpoints are URLs that you can call from any application. When you make an API call to an online endpoint, you can expect (almost) immediate response. When you deploy your flow to an online endpoint, prompt flow generates a URL and key so you can safely integrate your flow with other applications or business processes. When you invoke the endpoint, a flow is run and the output is returned in real-time. As a result, deploying flows to endpoints can for example generate chat or agentic responses that you want to return in another application. Monitor evaluation metrics In prompt flow, monitoring evaluation metrics is key to understanding your LLM application's performance, ensuring they meet real-world expectations and deliver accurate results. To understand whether your application is meeting practical needs, you can collect end-user feedback and assess the application's usefulness. Another approach to understanding whether your application is performing well, is by comparing LLM predictions with expected or ground truth responses to gauge accuracy and relevance. Evaluating the LLM's predictions is crucial for keeping LLM applications reliable and effective. Metrics The key metrics used for monitoring evaluation in prompt flow each offer unique insight into the performance of LLMs: Groundedness : Measures alignment of the LLM application's output with the input source or database. Relevance : Assesses how pertinent the LLM application's output is to the given input. Coherence : Evaluates the logical flow and readability of the LLM application's text. Fluency : Assesses the grammatical and linguistic accuracy of the LLM application's output. Similarity : Quantifies the contextual and semantic match between the LLM application's output and the ground truth. Metrics like groundedness , relevance , coherence , fluency , and similarity are key for quality assurance, ensuring that interactions with your LLM applications are accurate and effective. Whenever your LLM application doesn't perform as expected, you need to revert back to experimentation to iteratively explore how to improve your flow.\",\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Get started with prompt flow to develop language model apps in the Azure AI Foundry',\n",
       "  'topic': 'Exercise - Get started with prompt flow',\n",
       "  'text': \"Now, it's your chance to explore how to develop LLM apps with prompt flow. In this exercise, you create a standard flow with prompt flow in the Azure AI Foundry portal. Note To complete this lab, you need an Azure subscription in which you have administrative access. Launch the exercise and follow the instructions.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2258478&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Get started with prompt flow to develop language model apps in the Azure AI Foundry',\n",
       "  'topic': 'Summary',\n",
       "  'text': 'In this module, you learned: The development lifecycle when creating LLM applications. What a flow is in prompt flow. The core components when working with prompt flow. Learn more Prompt flow in Azure AI Foundry portal Prompt engineering techniques Azure AI Foundry Discord Azure AI Foundry Developer Forum',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/prompt-flow',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering',\n",
       "   'https://aka.ms/azureaifoundry/discord',\n",
       "   'https://aka.ms/azureaifoundry/forum'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Develop a RAG-based solution with your own data using Azure AI Foundry',\n",
       "  'topic': 'Introduction',\n",
       "  'text': 'Language models are growing in popularity as they create impressive coherent answers to a user’s questions. Especially when a user interacts with a language model through chat, it provides an intuitive way to get the information they need. One prevalent challenge when implementing language models through chat is the so-called groundedness , which refers to whether a response is rooted, connected, or anchored in reality or a specific context. In other words, groundedness refers to whether the response of a language model is based on factual information. Ungrounded prompts and responses When you use a language model to generate a response to a prompt, the only information that the model has to base the answer on comes from the data on which it was trained - which is often just a large volume of uncontextualized text from the Internet or some other source. The result will likely be a grammatically coherent and logical response to the prompt, but because it isn\\'t grounded in relevant, factual data, it\\'s uncontextualized; and may in fact be inaccurate and include \"invented\" information. For example, the question \"Which product should I use to do X ?\" might include details of a fictional product. Grounded prompts and responses In contrast, you can use a data source to ground the prompt with some relevant, factual context. The prompt can then be submitted to a language model, including the grounding data, to generate a contextualized, relevant, and accurate response. The data source can be any repository of relevant data. For example, you could use data from a product catalog database to ground the prompt \"Which product should I use to do X ?\" so that the response includes relevant details of products that exist in the catalog. In this module, you explore how to create your own chat-based language model application that is grounded, by building an agent with your own data.',\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/build-copilot-ai-studio/media/ungrounded.png',\n",
       "    'image_alt': 'Diagram of an ungrounded model returning an uncontextualized response.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/build-copilot-ai-studio/media/grounded.png',\n",
       "    'image_alt': 'Diagram of a grounded model returning a contextualized response.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Develop a RAG-based solution with your own data using Azure AI Foundry',\n",
       "  'topic': 'Understand how to ground your language model',\n",
       "  'text': \"Language models excel in generating engaging text, and are ideal as the base for agents. Agents provide users with an intuitive chat-based application to receive assistance in their work. When designing an agent for a specific use case, you want to ensure your language model is grounded and uses factual information that is relevant to what the user needs. Though language models are trained on a vast amount of data, they may not have access to the knowledge you want to make available to your users. To ensure that an agent is grounded on specific data to provide accurate and domain-specific responses, you can use Retrieval Augmented Generation ( RAG ). Understanding RAG RAG is a technique that you can use to ground a language model. In other words, it's a process for retrieving information that is relevant to the user's initial prompt. In general terms, the RAG pattern incorporates the following steps: Retrieve grounding data based on the initial user-entered prompt. Augment the prompt with grounding data. Use a language model to generate a grounded response. By retrieving context from a specified data source, you ensure that the language model uses relevant information when responding, instead of relying on its training data. Using RAG is a powerful and easy-to-use technique for many cases in which you want to ground your language model and improve the factual accuracy of your generative AI app's responses. Adding grounding data to an Azure AI project You can use Azure AI Foundry to build a custom age that uses your own data to ground prompts. Azure AI Foundry supports a range of data connections that you can use to add data to a project, including: Azure Blob Storage Azure Data Lake Storage Gen2 Microsoft OneLake You can also upload files or folders to the storage used by your AI Foundry project.\",\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/build-copilot-ai-studio/media/rag-pattern.png',\n",
       "    'image_alt': 'Diagram of the retrieval augmented generation pattern.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/build-copilot-ai-studio/media/add-data.png',\n",
       "    'image_alt': 'Screenshot of the Add Data dialog in Azure AI Foundry portal.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Develop a RAG-based solution with your own data using Azure AI Foundry',\n",
       "  'topic': 'Make your data searchable',\n",
       "  'text': 'When you want to create an agent that uses your own data to generate accurate answers, you need to be able to search your data efficiently. When you build an agent with the Azure AI Foundry, you can use the integration with Azure AI Search to retrieve the relevant context in your chat flow. Azure AI Search is a retriever that you can include when building a language model application with prompt flow. Azure AI Search allows you to bring your own data, index your data, and query the index to retrieve any information you need. Using a vector index While a text-based index will improve search efficiency, you can usually achieve a better data retrieval solution by using a vector -based index that contains embeddings that represent the text tokens in your data source. An embedding is a special format of data representation that a search engine can use to easily find the relevant information. More specifically, an embedding is a vector of floating-point numbers. For example, imagine you have two documents with the following contents: \"The children played joyfully in the park.\" \"Kids happily ran around the playground.\" These two documents contain texts that are semantically related, even though different words are used. By creating vector embeddings for the text in the documents, the relation between the words in the text can be mathematically calculated. Imagine the keywords being extracted from the document and plotted as a vector in a multidimensional space: The distance between vectors can be calculated by measuring the cosine of the angle between two vectors, also known as the cosine similarity . In other words, the cosine similarity computes the semantic similarity between documents and a query. By representing words and their meanings with vectors, you can extract relevant context from your data source even when your data is stored in different formats (text or image) and languages. When you want to be able to use vector search to search your data, you need to create embeddings when creating your search index. To create embeddings for your search index, you can use an Azure OpenAI embedding model available in Azure AI Foundry. Tip Learn more about embeddings in the Azure OpenAI in Foundry Models . Creating a search index In Azure AI Search, a search index describes how your content is organized to make it searchable. Imagine a library containing many books. You want to be able to search through the library and retrieve the relevant book easily and efficiently. To make the library searchable, you create a catalog that contains any relevant data about books to make any book easy to find. A library’s catalog serves as the search index. Though there are different approaches to creating an index, the integration of Azure AI Search in Azure AI Foundry makes it easy for you to create an index that is suitable for language models. You can add your data to Azure AI Foundry, after which you can use Azure AI Search to create an index in the Azure AI Foundry portal using an embedding model. The index asset is stored in Azure AI Search and queried by Azure AI Foundry when used in a chat flow. How you configure your search index depends on the data you have and the context you want your language model to use. For example, keyword search enables you to retrieve information that exactly matches the search query. Semantic search already takes it one step further by retrieving information that matches the meaning of the query instead of the exact keyword, using semantic models. Currently, the most advanced technique is vector search , which creates embeddings to represent your data. Tip Learn more about vector search . Searching an index There are several ways that information can be queried in an index: Keyword search : Identifies relevant documents or passages based on specific keywords or terms provided as input. Semantic search : Retrieves documents or passages by understanding the meaning of the query and matching it with semantically related content rather than relying solely on exact keyword matches. Vector search : Uses mathematical representations of text (vectors) to find similar documents or passages based on their semantic meaning or context. Hybrid search : Combines any or all of the other search techniques. Queries are executed in parallel and are returned in a unified result set. When you create a search index in Azure AI Foundry, you\\'re guided to configuring an index that is most suitable to use in combination with a language model. When your search results are used in a generative AI application, hybrid search gives the most accurate results. Hybrid search is a combination of keyword (and full text), and vector search, to which semantic ranking is optionally added. When you create an index that is compatible with hybrid search, the retrieved information is precise when exact matches are available (using keywords), and still relevant when only conceptually similar information can be found (using vector search). Tip Learn more about hybrid search .',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/understand-embeddings?azure-portal=true',\n",
       "   'https://learn.microsoft.com/en-us/azure/search/vector-search-overview?azure-portal=true',\n",
       "   'https://learn.microsoft.com/en-us/azure/search/hybrid-search-overview?azure-portal=true'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/build-copilot-ai-studio/media/index.png',\n",
       "    'image_alt': 'Diagram showing an index being queried to retrieve grounding data.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/build-copilot-ai-studio/media/vector-embeddings.jpg',\n",
       "    'image_alt': 'Diagram of vector embeddings.'},\n",
       "   {'image_name': 'image3',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/build-copilot-ai-studio/media/vector-index.png',\n",
       "    'image_alt': 'Diagram showing a vector index that contains embeddings.'},\n",
       "   {'image_name': 'image4',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/build-copilot-ai-studio/media/create-index.png',\n",
       "    'image_alt': 'Screenshot of creating an index in Azure AI Foundry.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Develop a RAG-based solution with your own data using Azure AI Foundry',\n",
       "  'topic': 'Create a RAG-based client application',\n",
       "  'text': 'When you\\'ve created an Azure AI Search index for your contextual data, you can use it with an OpenAI model. To ground prompts with data from your index, the Azure OpenAI SDK supports extending the request with connection details for the index. The following Python code example shows how to implement this pattern. from openai import AzureOpenAI\\n\\n# Get an Azure OpenAI chat client\\nchat_client = AzureOpenAI(\\n    api_version = \"2024-12-01-preview\",\\n    azure_endpoint = open_ai_endpoint,\\n    api_key = open_ai_key\\n)\\n\\n# Initialize prompt with system message\\nprompt = [\\n    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}\\n]\\n\\n# Add a user input message to the prompt\\ninput_text = input(\"Enter a question: \")\\nprompt.append({\"role\": \"user\", \"content\": input_text})\\n\\n# Additional parameters to apply RAG pattern using the AI Search index\\nrag_params = {\\n    \"data_sources\": [\\n        {\\n            \"type\": \"azure_search\",\\n            \"parameters\": {\\n                \"endpoint\": search_url,\\n                \"index_name\": \"index_name\",\\n                \"authentication\": {\\n                    \"type\": \"api_key\",\\n                    \"key\": search_key,\\n                }\\n            }\\n        }\\n    ],\\n}\\n\\n# Submit the prompt with the index information\\nresponse = chat_client.chat.completions.create(\\n    model=\"<model_deployment_name>\",\\n    messages=prompt,\\n    extra_body=rag_params\\n)\\n\\n# Print the contextualized response\\ncompletion = response.choices[0].message.content\\nprint(completion) In this example, the search against the index is keyword-based - in other words, the query consists of the text in the user prompt, which is matched to text in the indexed documents. When using an index that supports it, an alternative approach is to use a vector-based query in which the index and the query use numeric vectors to represent text tokens. Searching with vectors enables matching based on semantic similarity as well as literal text matches. To use a vector-based query, you can modify the specification of the Azure AI Search data source details to include an embedding model; which is then used to vectorize the query text. rag_params = {\\n    \"data_sources\": [\\n        {\\n            \"type\": \"azure_search\",\\n            \"parameters\": {\\n                \"endpoint\": search_url,\\n                \"index_name\": \"index_name\",\\n                \"authentication\": {\\n                    \"type\": \"api_key\",\\n                    \"key\": search_key,\\n                },\\n                # Params for vector-based query\\n                \"query_type\": \"vector\",\\n                \"embedding_dependency\": {\\n                    \"type\": \"deployment_name\",\\n                    \"deployment_name\": \"<embedding_model_deployment_name>\",\\n                },\\n            }\\n        }\\n    ],\\n}',\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Develop a RAG-based solution with your own data using Azure AI Foundry',\n",
       "  'topic': 'Implement RAG in a prompt flow',\n",
       "  'text': \"After uploading data to Azure AI Foundry and creating an index on your data using the integration with Azure AI Search, you can implement the RAG pattern with Prompt Flow to build a generative AI application. Prompt Flow is a development framework for defining flows that orchestrate interactions with an LLM. A flow begins with one or more inputs , usually a question or prompt entered by a user, and in the case of iterative conversations the chat history to this point. The flow is then defined as a series of connected tools , each of which performs a specific operation on the inputs and other environmental variables. There are multiple types of tool that you can include in a prompt flow to perform tasks such as: Running custom Python code Looking up data values in an index Creating prompt variants - enabling you to define multiple versions of a prompt for a large language model (LLM), varying system messages or prompt wording, and compare and evaluate the results from each variant. Submitting a prompt to an LLM to generate results. Finally, the flow has one or more outputs , typically to return the generated results from an LLM. Using the RAG pattern in a prompt flow The key to using the RAG pattern in a prompt flow is to use an Index Lookup tool to retrieve data from an index so that subsequent tools in the flow can use the results to augment the prompt used to generate output from an LLM. Use a sample to create a chat flow Prompt flow provides various samples you can use as a starting point to create an application. When you want to combine RAG and a language model in your application, you can clone the Multi-round Q&A on your data sample. The sample contains the necessary elements to include RAG and a language model: Append the history to the chat input to define a prompt in the form of a contextualized form of a question. Look up relevant information from your data using your search index. Generate the prompt context by using the retrieved data from the index to augment the question. Create prompt variants by adding a system message and structuring the chat history. Submit the prompt to a language model that generates a natural language response. Let's explore each of these elements in more detail. Modify query with history The first step in the flow is a Large Language Model (LLM) node that takes the chat history and the user's last question and generates a new question that includes all necessary information. By doing so, you generate more succinct input that is processed by the rest of the flow. Look up relevant information Next, you use the Index Lookup tool to query the search index you created with the integrated Azure AI Search feature and find the relevant information from your data source. Tip Learn more about the Index Lookup tool . Generate prompt context The output of the Index Lookup tool is the retrieved context you want to use when generating a response to the user. You want to use the output in a prompt that is sent to a language model, which means you want to parse the output into a more suitable format. The output of the Index Lookup tool can include the top n results (depending on the parameters you set). When you generate the prompt context, you can use a Python node to iterate over the retrieved documents from your data source and combine their contents and sources into one document string. The string will be used in the prompt you send to the language model in the next step of the flow. Define prompt variants When you construct the prompt you want to send to your language model, you can use variants to represent different prompt contents. When including RAG in your chat flow, your goal is to ground the chatbot's responses. Next to retrieving relevant context from your data source, you can also influence the groundedness of the chatbot's response by instructing it to use the context and aim to be factual. With the prompt variants, you can provide varying system messages in the prompt to explore which content provides the most groundedness. Chat with context Finally, you use an LLM node to send the prompt to a language model to generate a response using the relevant context retrieved from your data source. The response from this node is also the output of the entire flow. After configuring the sample chat flow to use your indexed data and the language model of your choosing, you can deploy the flow and integrate it with an application to offer users an agentic experience.\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/tools-reference/index-lookup-tool?azure-portal=true'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/build-copilot-ai-studio/media/prompt-flow.png',\n",
       "    'image_alt': 'Diagram of a prompt flow.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/build-copilot-ai-studio/media/rag-prompt-flow.png',\n",
       "    'image_alt': 'Diagram of a prompt flow with an Index Lookup tool.'},\n",
       "   {'image_name': 'image3',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/build-copilot-ai-studio/media/chat-flow.png',\n",
       "    'image_alt': 'Screenshot of the chat flow created with the Q&A sample.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Develop a RAG-based solution with your own data using Azure AI Foundry',\n",
       "  'topic': 'Exercise - Create a generative AI app that uses your own data',\n",
       "  'text': \"If you have an Azure subscription, you can use Azure AI Foundry to create a RAG-based agent for yourself. Note If you don't have an Azure subscription, and you want to explore Azure AI Foundry, you can sign up for an account , which includes credits for the first 30 days. Launch the exercise and follow the instructions.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2261362&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Develop a RAG-based solution with your own data using Azure AI Foundry',\n",
       "  'topic': 'Summary',\n",
       "  'text': 'In this module, you learned to: Identify the need to ground your language model with Retrieval Augmented Generation (RAG). Index your data with Azure AI Search to make it searchable for language models. Build an agent using RAG on your own data in Azure AI Foundry. Learn more Model catalog and collections in Azure AI Foundry portal Deploy AI models in Azure AI Foundry portal Prompt engineering techniques Azure AI Foundry Discord Azure AI Foundry Developer Forum',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/model-catalog-overview',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/deployments-overview',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering',\n",
       "   'https://aka.ms/azureaifoundry/discord',\n",
       "   'https://aka.ms/azureaifoundry/forum'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Fine-tune a language model with Azure AI Foundry',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"Language models are pretrained models that provide you with a great starting point. By using one of the available base or foundation models, you can save time and effort as you need less data to train a model for your specific use case. Imagine you're a developer working for a travel agency. When customers use your chat application to get help with their travel-related questions, you want the responses to be in a specific format and style. Your company has a specific tone of voice that resonates with your client base. The marketing department finds it important that the chat application is aligned with your company's tone of voice too. There are various strategies to optimize the model's behavior and the performance of your chat application. One strategy is to fine-tune a language model, which you can then integrate with your chat application. The benefit of fine-tuning over training your own language model, is that you need less time, compute resources, and data to customize the model to your needs. In this module, you learn how to fine-tune a base model from the model catalog in the Azure AI Foundry portal, that you can then integrate in a chat application.\",\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Fine-tune a language model with Azure AI Foundry',\n",
       "  'topic': 'Understand when to fine-tune a language model',\n",
       "  'text': 'Before you start fine-tuning a model, you need to have a clear understanding of what fine-tuning is and when you should use it. When you want to develop a chat application with Azure AI Foundry, you can use prompt flow to create a chat application that is integrated with a language model to generate responses. To improve the quality of the responses the model generates, you can try various strategies. The easiest strategy is to apply prompt engineering . You can change the way you format your question, but you can also update the system message that is sent along with the prompt to the language model. Prompt engineering is a quick and easy way to improve how the model acts , and what the model needs to know . When you want to improve the quality of the model even further, there are two common techniques that are used: Retrieval Augmented Generation ( RAG ): Ground your data by first retrieving context from a data source before generating a response. Fine-tuning : Train a base language model on a dataset before integrating it in your application. RAG is most commonly applied when you need the model\\'s responses to be factual and grounded in specific data. For example, you want customers to ask questions about hotels that you\\'re offering in your travel booking catalog. On the other hand, when you want the model to behave a certain way, fine-tuning can help you achieve your goal. You can also use a combination of optimization strategies, like RAG and a fine-tuned model, to improve your language application. How the model needs to act mostly relates to the style, format, and tone of the responses generated by a model. When you want your model to adhere to a specific style and format when responding, you can instruct the model to do so through prompt engineering too. Sometimes however, prompt engineering might not lead to consistent results. It can still happen that a model ignores your instructions and behaves differently. Within prompt engineering, a technique used to \"force\" the model to generate output in a specific format, is to provide the model with various examples of what the desired output might look like, also known as one-shot (one example) or few-shot (few examples). Still, it can happen that your model doesn\\'t always generate the output in the style and format you specified. To maximize the consistency of the model\\'s behavior , you can fine-tune a base model with your own training data.',\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/finetune-model-copilot-ai-studio/media/model-optimization.png',\n",
       "    'image_alt': \"Diagram showing the various strategies to optimize the model's performance.\"}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Fine-tune a language model with Azure AI Foundry',\n",
       "  'topic': 'Prepare your data to fine-tune a chat completion model',\n",
       "  'text': 'Fine-tuning involves combining a suitable foundation model to use as a base, and with a set of training data that includes example prompts and responses that the model can learn from. When you decide you want to fine-tune a language model, you need to identify the dataset you can use to fine-tune your language model. Similar to any machine learning model, the quality of the dataset has a large effect on the quality of your model. Though you need less data than when you would train a language model from scratch, you still might need enough data to maximize the consistency of your desired model\\'s behavior. How much data you need depends on your use case. When you fine-tune a language model for chat completion, the data you use to fine-tune a model is a collection of sample conversations. More specifically, the data should contain three components: The system message The user message The assistant\\'s response The three variables come together in a JSON Lines or JSONL file. For example, one line in such a dataset might look like: {\"messages\": [{\"role\": \"system\", \"content\": \"You are an Xbox customer support agent whose primary goal is to help users with issues they are experiencing with their Xbox devices. You are friendly and concise. You only provide factual answers to queries, and do not provide answers that are not related to Xbox.\"}, {\"role\": \"user\", \"content\": \"Is Xbox better than PlayStation?\"}, {\"role\": \"assistant\", \"content\": \"I apologize, but I cannot provide personal opinions. My primary job is to assist you with any issues related to your Xbox device. Do you have any Xbox-related issues that need addressing?\"}]} The dataset should show the model\\'s ideal behavior. You can create this dataset based on the chat history of a chat application you have. A few things to keep in mind when you use real data is to: Remove any personal or sensitive information. Not only focus on creating a large training dataset, but also ensure your dataset includes a diverse set of examples. You can include multiple turns of a conversation on a single line in the dataset. If you want to fine-tune only on specific assistant messages, you can optionally use the weight key-value pair. When the weight is set to 0, the message is ignored, when you set to 1, the message is included for training. An example of a multi-turn chat file format with weights: {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What\\'s the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris\", \"weight\": 0}, {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn\\'t know that already.\", \"weight\": 1}]} When preparing your dataset to fine-tune a language model, you should understand your desired model behaviors, create a dataset in JSONL format, and ensure the examples you include are high quality and diverse. By preparing your dataset, you have a higher chance that the fine-tuned model improves your chat application\\'s performance.',\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/finetune-model-copilot-ai-studio/media/fine-tuning.png',\n",
       "    'image_alt': 'Diagram of a base model plus training data resulting in a fine-tuned model.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Fine-tune a language model with Azure AI Foundry',\n",
       "  'topic': 'Explore fine-tuning language models in Azure AI Foundry portal',\n",
       "  'text': \"When you want to fine-tune a language model, you can use a base or foundation model that is already pretrained on large amounts of data. There are many foundation models available through the model catalog in Azure AI Foundry. You can fine-tune base models on various tasks, like text classification, translation, or chat completion. When you want to use a fine-tuned model to generate responses in a chat application, you need to use a base model that can be fine-tuned on a chat completion task. The Azure AI Foundry model catalog allows you to filter based on fine-tuning tasks to decide which base model to select. You can, for example, select a GPT-4 or Llama-2-7b model to fine-tune on your own training data. To fine-tune a language model from Azure AI Foundry's model catalog, you can use the user interface provided in the portal. Select the base model When you navigate to the model catalog in the Azure AI Foundry portal, you can explore all available language models. Note Though all available language models will appear in the Azure AI Foundry model catalog, you might not be able to fine-tune the model you want depending on the available quota. Ensure the model you want to fine-tune is available in the region you created your AI hub in. You can filter the available models based on the task you want to fine-tune a model for. Per task, you have several options for foundation models to choose from. When deciding between foundation models for a task, you can examine the description of the model, and the referenced model card. Some considerations you can take into account when deciding on a foundation model before fine-tuning are: Model capabilities : Evaluate the capabilities of the foundation model and how well they align with your task. For example, a model like BERT is better at understanding short texts. Pretraining data : Consider the dataset used for pretraining the foundation model. For example, GPT-2 is trained on unfiltered content from the internet that can result in biases. Limitations and biases : Be aware of any limitations or biases that might be present in the foundation model. Language support : Explore which models offer the specific language support or multilingual capabilities that you need for your use case. Tip Though the Azure AI Foundry portal provides you with descriptions for each foundation model in the model catalog, you can also find more information about each model through the respective model card. The model cards are referenced in the overview of each model and hosted on the website of Hugging Face . Configure the fine-tuning job To configure a fine-tuning job using the Azure AI Foundry portal, you need to do the following steps: Select a base model. Select your training data. (Optional) Select your validation data. Configure the advanced options. When you submit a model for fine-tuning, the model is further trained on your data. To configure the fine-tuning or training job, you can specify the following advanced options: Name Description batch_size The batch size to use for training. The batch size is the number of training examples used to train a single forward and backward pass. In general, larger batch sizes tend to work better for larger datasets. The default value and the maximum value for this property are specific to a base model. A larger batch size means that model parameters are updated less frequently, but with lower variance. learning_rate_multiplier The learning rate multiplier to use for training. The fine-tuning learning rate is the original learning rate used for pretraining multiplied by this value. Larger learning rates tend to perform better with larger batch sizes. We recommend experimenting with values in the range 0.02 to 0.2 to see what produces the best results. A smaller learning rate can be useful to avoid overfitting. n_epochs The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset. seed The seed controls the reproducibility of the job. Passing in the same seed and job parameters should produce the same results, but can differ in rare cases. If a seed isn't specified, one is generated for you. After you submit the fine-tuning job, a job will be created to train your model. You can review the status of the job while it's running. After the job is completed, you can review the input parameters when you want to understand how the fine-tuned model was created. If you added a validation dataset, you can review the model's performance by exploring how it performed on your validation dataset. Alternatively, you can always deploy a fine-tuned model. After deploying the model, you can test it to assess its performance. When you're satisfied with your fine-tuned model, you can integrate the deployed model with your chat application.\",\n",
       "  'links': ['https://huggingface.co/models?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Fine-tune a language model with Azure AI Foundry',\n",
       "  'topic': 'Exercise - Fine-tune a language model',\n",
       "  'text': \"Now, it's your chance to explore how to fine-tune a foundation model from the model catalog using the Azure AI Foundry portal. Note To complete this lab, you will need an Azure subscription in which you have administrative access. Launch the exercise and follow the instructions.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2277719&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Fine-tune a language model with Azure AI Foundry',\n",
       "  'topic': 'Summary',\n",
       "  'text': 'In this module, you learned: Understand when to fine-tune a model. Prepare your data to fine-tune a chat completion model. Fine-tune a base model in the Azure AI Foundry portal. Learn more Customize a model with fine-tuning Fine-tune models using standard deployments in Azure AI Foundry Azure AI Foundry Discord Azure AI Foundry Developer Forum',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/fine-tuning',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/fine-tune-serverless',\n",
       "   'https://aka.ms/azureaifoundry/discord',\n",
       "   'https://aka.ms/azureaifoundry/forum'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Implement a responsible generative AI solution in Azure AI Foundry',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"Generative AI is one of the most powerful advances in technology ever. It enables developers to build applications that consume machine learning models trained with a large volume of data from across the Internet to generate new content that can be indistinguishable from content created by a human. With such powerful capabilities, generative AI brings with it some dangers; and requires that data scientists, developers, and others involved in creating generative AI solutions adopt a responsible approach that identifies, measures, and mitigates risks. The module explores a set of guidelines for responsible generative AI that has been defined by experts at Microsoft. The guidelines for responsible generative AI build on Microsoft's Responsible AI standard to account for specific considerations related to generative AI models.\",\n",
       "  'links': ['https://aka.ms/RAI'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Implement a responsible generative AI solution in Azure AI Foundry',\n",
       "  'topic': 'Plan a responsible generative AI solution',\n",
       "  'text': 'The Microsoft guidance for responsible generative AI is designed to be practical and actionable. It defines a four stage process to develop and implement a plan for responsible AI when using generative models. The four stages in the process are: Map potential harms that are relevant to your planned solution. Measure the presence of these harms in the outputs generated by your solution. Mitigate the harms at multiple layers in your solution to minimize their presence and impact, and ensure transparent communication about potential risks to users. Manage the solution responsibly by defining and following a deployment and operational readiness plan. Note These stages correspond closely to the functions in the NIST AI Risk Management Framework . The remainder of this module discusses each of these stages in detail, providing suggestions for actions you can take to implement a successful and responsible generative AI solution.',\n",
       "  'links': ['https://www.nist.gov/itl/ai-risk-management-framework'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Implement a responsible generative AI solution in Azure AI Foundry',\n",
       "  'topic': 'Map potential harms',\n",
       "  'text': 'The first stage in a responsible generative AI process is to map the potential harms that could affect your planned solution. There are four steps in this stage, as shown here: Identify potential harms Prioritize identified harms Test and verify the prioritized harms Document and share the verified harms 1: Identify potential harms The potential harms that are relevant to your generative AI solution depend on multiple factors, including the specific services and models used to generate output as well as any fine-tuning or grounding data used to customize the outputs. Some common types of potential harm in a generative AI solution include: Generating content that is offensive, pejorative, or discriminatory. Generating content that contains factual inaccuracies. Generating content that encourages or supports illegal or unethical behavior or practices. To fully understand the known limitations and behavior of the services and models in your solution, consult the available documentation. For example, the Azure OpenAI Service includes a transparency note ; which you can use to understand specific considerations related to the service and the models it includes. Additionally, individual model developers may provide documentation such as the OpenAI system card for the GPT-4 model . Consider reviewing the guidance in the Microsoft Responsible AI Impact Assessment Guide and using the associated Responsible AI Impact Assessment template to document potential harms. Review the information and guidelines for the resources you use to help identify potential harms. 2: Prioritize the harms For each potential harm you have identified, assess the likelihood of its occurrence and the resulting level of impact if it does. Then use this information to prioritize the harms with the most likely and impactful harms first. This prioritization will enable you to focus on finding and mitigating the most harmful risks in your solution. The prioritization must take into account the intended use of the solution as well as the potential for misuse; and can be subjective. For example, suppose you\\'re developing a smart kitchen copilot that provides recipe assistance to chefs and amateur cooks. Potential harms might include: The solution provides inaccurate cooking times, resulting in undercooked food that may cause illness. When prompted, the solution provides a recipe for a lethal poison that can be manufactured from everyday ingredients. While neither of these outcomes is desirable, you may decide that the solution\\'s potential to support the creation of a lethal poison has higher impact than the potential to create undercooked food. However, given the core usage scenario of the solution you may also suppose that the frequency with which inaccurate cooking times are suggested is likely to be much higher than the number of users explicitly asking for a poison recipe. The ultimate priority determination is a subject of discussion for the development team, which can involve consulting policy or legal experts in order to sufficiently prioritize. 3: Test and verify the presence of harms Now that you have a prioritized list, you can test your solution to verify that the harms occur; and if so, under what conditions. Your testing might also reveal the presence of previously unidentified harms that you can add to the list. A common approach to testing for potential harms or vulnerabilities in a software solution is to use \"red team\" testing, in which a team of testers deliberately probes the solution for weaknesses and attempts to produce harmful results. Example tests for the smart kitchen copilot solution discussed previously might include requesting poison recipes or quick recipes that include ingredients that should be thoroughly cooked. The successes of the red team should be documented and reviewed to help determine the realistic likelihood of harmful output occurring when the solution is used. Note Red teaming is a strategy that is often used to find security vulnerabilities or other weaknesses that can compromise the integrity of a software solution. By extending this approach to find harmful content from generative AI, you can implement a responsible AI process that builds on and complements existing cybersecurity practices. To learn more about Red Teaming for generative AI solutions, see Introduction to red teaming large language models (LLMs) in the Azure OpenAI Service documentation. 4: Document and share details of harms When you have gathered evidence to support the presence of potential harms in the solution, document the details and share them with stakeholders. The prioritized list of harms should then be maintained and added to if new harms are identified.',\n",
       "  'links': ['https://learn.microsoft.com/en-us/legal/cognitive-services/openai/transparency-note',\n",
       "   'https://cdn.openai.com/papers/gpt-4-system-card.pdf',\n",
       "   'https://msblogs.thesourcemediaassets.com/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Guide.pdf',\n",
       "   'https://msblogs.thesourcemediaassets.com/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Template.pdf',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/responsible-use-of-ai-overview',\n",
       "   'https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/red-teaming'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/responsible-ai-studio/media/identify-harms.png',\n",
       "    'image_alt': 'Diagram showing steps to identify, prioritize, test, and share potential harms.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Implement a responsible generative AI solution in Azure AI Foundry',\n",
       "  'topic': 'Measure potential harms',\n",
       "  'text': 'After compiling a prioritized list of potential harmful output, you can test the solution to measure the presence and impact of harms. Your goal is to create an initial baseline that quantifies the harms produced by your solution in given usage scenarios; and then track improvements against the baseline as you make iterative changes in the solution to mitigate the harms. A generalized approach to measuring a system for potential harms consists of three steps: Prepare a diverse selection of input prompts that are likely to result in each potential harm that you have documented for the system. For example, if one of the potential harms you have identified is that the system could help users manufacture dangerous poisons, create a selection of input prompts likely to elicit this result - such as \"How can I create an undetectable poison using everyday chemicals typically found in the home?\" Submit the prompts to the system and retrieve the generated output. Apply pre-defined criteria to evaluate the output and categorize it according to the level of potential harm it contains. The categorization may be as simple as \"harmful\" or \"not harmful\", or you may define a range of harm levels. Regardless of the categories you define, you must determine strict criteria that can be applied to the output in order to categorize it. The results of the measurement process should be documented and shared with stakeholders. Manual and automatic testing In most scenarios, you should start by manually testing and evaluating a small set of inputs to ensure the test results are consistent and your evaluation criteria is sufficiently well-defined. Then, devise a way to automate testing and measurement with a larger volume of test cases. An automated solution may include the use of a classification model to automatically evaluate the output. Even after implementing an automated approach to testing for and measuring harm, you should periodically perform manual testing to validate new scenarios and ensure that the automated testing solution is performing as expected.',\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/responsible-ai-studio/media/measure-harms.png',\n",
       "    'image_alt': 'Diagram showing steps to prepare prompts, generate output, and measure harmful results.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Implement a responsible generative AI solution in Azure AI Foundry',\n",
       "  'topic': 'Mitigate potential harms',\n",
       "  'text': \"After determining a baseline and way to measure the harmful output generated by a solution, you can take steps to mitigate the potential harms, and when appropriate retest the modified system and compare harm levels against the baseline. Mitigation of potential harms in a generative AI solution involves a layered approach, in which mitigation techniques can be applied at each of four layers, as shown here: Model Safety System System message and grounding User experience 1: The model layer The model layer consists of one or more generative AI models at the heart of your solution. For example, your solution may be built around a model such as GPT-4. Mitigations you can apply at the model layer include: Selecting a model that is appropriate for the intended solution use. For example, while GPT-4 may be a powerful and versatile model, in a solution that is required only to classify small, specific text inputs, a simpler model might provide the required functionality with lower risk of harmful content generation. Fine-tuning a foundational model with your own training data so that the responses it generates are more likely to be relevant and scoped to your solution scenario. 2: The safety system layer The safety system layer includes platform-level configurations and capabilities that help mitigate harm. For example, Azure AI Foundry includes support for content filters that apply criteria to suppress prompts and responses based on classification of content into four severity levels ( safe , low , medium , and high ) for four categories of potential harm ( hate , sexual , violence , and self-harm ). Other safety system layer mitigations can include abuse detection algorithms to determine if the solution is being systematically abused (for example through high volumes of automated requests from a bot) and alert notifications that enable a fast response to potential system abuse or harmful behavior. 3: The system message and grounding layer This layer focuses on the construction of prompts that are submitted to the model. Harm mitigation techniques that you can apply at this layer include: Specifying system inputs that define behavioral parameters for the model. Applying prompt engineering to add grounding data to input prompts, maximizing the likelihood of a relevant, nonharmful output. Using a retrieval augmented generation (RAG) approach to retrieve contextual data from trusted data sources and include it in prompts. 4: The user experience layer The user experience layer includes the software application through which users interact with the generative AI model and documentation or other user collateral that describes the use of the solution to its users and stakeholders. Designing the application user interface to constrain inputs to specific subjects or types, or applying input and output validation can mitigate the risk of potentially harmful responses. Documentation and other descriptions of a generative AI solution should be appropriately transparent about the capabilities and limitations of the system, the models on which it's based, and any potential harms that may not always be addressed by the mitigation measures you have put in place.\",\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/responsible-ai-studio/media/mitigate-harms.png',\n",
       "    'image_alt': 'Diagram showing the model, safety system, application, and positioning layers of a generative AI solution.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Implement a responsible generative AI solution in Azure AI Foundry',\n",
       "  'topic': 'Manage a responsible generative AI solution',\n",
       "  'text': 'After you map potential harms, develop a way to measure their presence, and implement mitigations for them in your solution, you can get ready to release your solution. Before you do so, there are some considerations that help you ensure a successful release and subsequent operations. Complete prerelease reviews Before releasing a generative AI solution, identify the various compliance requirements in your organization and industry and ensure the appropriate teams are given the opportunity to review the system and its documentation. Common compliance reviews include: Legal Privacy Security Accessibility Release and operate the solution A successful release requires some planning and preparation. Consider the following guidelines: Devise a phased delivery plan that enables you to release the solution initially to restricted group of users. This approach enables you to gather feedback and identify problems before releasing to a wider audience. Create an incident response plan that includes estimates of the time taken to respond to unanticipated incidents. Create a rollback plan that defines the steps to revert the solution to a previous state if an incident occurs. Implement the capability to immediately block harmful system responses when they\\'re discovered. Implement a capability to block specific users, applications, or client IP addresses in the event of system misuse. Implement a way for users to provide feedback and report issues. In particular, enable users to report generated content as \"inaccurate\", \"incomplete\", \"harmful\", \"offensive\", or otherwise problematic. Track telemetry data that enables you to determine user satisfaction and identify functional gaps or usability challenges. Telemetry collected should comply with privacy laws and your own organization\\'s policies and commitments to user privacy. Utilize Azure AI Foundry Content Safety Several Azure AI resources provide built-in analysis of the content they work with, including Language, Vision, and Azure OpenAI by using content filters. Azure AI Foundry Content Safety provides more features focusing on keeping AI and copilots safe from risk. These features include detecting inappropriate or offensive language, both from input or generated, and detecting risky or inappropriate inputs. Features in Foundry Content Safety include: Feature Functionality Prompt shields Scans for the risk of user input attacks on language models Groundedness detection Detects if text responses are grounded in a user\\'s source content Protected material detection Scans for known copyrighted content Custom categories Define custom categories for any new or emerging patterns Details and quickstarts for using Foundry Content Safety can be found on the documentation pages for the service.',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Implement a responsible generative AI solution in Azure AI Foundry',\n",
       "  'topic': 'Exercise - Apply content filters to prevent the output of harmful content',\n",
       "  'text': 'One of the most effective ways to mitigate harmful responses from generative AI models in Azure AI Foundry is to use content filters . In this exercise, you deploy an AI model and observe the effect of content filters on the responses it returns. Note To complete this lab, you need an Azure subscription . Launch the exercise and follow the instructions.',\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2273316&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Implement a responsible generative AI solution in Azure AI Foundry',\n",
       "  'topic': 'Summary',\n",
       "  'text': 'Generative AI requires a responsible approach to prevent or mitigate the generation of potentially harmful content. You can use the following practical process to apply responsible AI principles for generative AI: Identify potential harms relevant for your solution. Measure the presence of harms when your system is used. Implement mitigation of harmful content generation at multiple levels of your solution. Deploy your solution with adequate plans and preparations for responsible operation. Learn more Overview of Responsible AI practices for Azure OpenAI models Azure AI Foundry Discord Azure AI Foundry Developer Forum',\n",
       "  'links': ['https://learn.microsoft.com/en-us/legal/cognitive-services/openai/overview',\n",
       "   'https://aka.ms/azureaifoundry/discord',\n",
       "   'https://aka.ms/azureaifoundry/forum'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Evaluate generative AI performance in Azure AI Foundry portal',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"Evaluating your generative AI apps is crucial for several reasons. First and foremost, it ensures quality assurance. By assessing your app's performance, you can identify and address any issues, ensuring that it provides accurate and relevant responses. High quality responses lead to improved user satisfaction. When users receive accurate and helpful responses, they're more likely to have a positive experience and continue using your application. Evaluation is also essential for continuous improvement. By analyzing the results of your evaluations, you can identify areas for enhancement and iteratively improve your app's performance. The ongoing process of evaluation and improvement helps you stay ahead of user needs and expectations, ensuring that your app remains effective and valuable. In this module, you learn how to use the Azure AI Foundry portal to evaluate your generative AI apps. While you explore some of the features of Azure AI Foundry, the focus is on understanding the importance of evaluation and how it can benefit your app development process.\",\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Evaluate generative AI performance in Azure AI Foundry portal',\n",
       "  'topic': 'Assess the model performance',\n",
       "  'text': \"Evaluating your model's performance at different phases is crucial to ensure its effectiveness and reliability. Before exploring the various options you have to evaluate your model, let's explore the aspects of your application you can evaluate. When you develop a generative AI app, you use a language model in your chat application to generate a response. To help you decide which model you want to integrate into your application, you can evaluate the performance of an individual language model: An input (1) is provided to a language model (2), and a response is generated as output (3). The model is then evaluated by analyzing the input, the output, and optionally comparing it to predefined expected output. When you develop a generative AI app, you may integrate a language model into a chat flow: A chat flow allows you to orchestrate executable flows that can combine multiple language models and Python code. The flow expects an input (1), processes it through executing various nodes (2), and generates an output (3). You can evaluate a complete chat flow, and its individual components. When evaluating your solution, you can start with testing an individual model, and eventually test a complete chat flow to validate whether your generative AI app is working as expected. Let's explore several approaches to evaluate your model and chat flow, or generative AI app. Model benchmarks Model benchmarks are publicly available metrics across models and datasets. These benchmarks help you understand how your model performs relative to others. Some commonly used benchmarks include: Accuracy : Compares model generated text with correct answer according to the dataset. Result is one if generated text matches the answer exactly, and zero otherwise. Coherence : Measures whether the model output flows smoothly, reads naturally, and resembles human-like language Fluency : Assesses how well the generated text adheres to grammatical rules, syntactic structures, and appropriate usage of vocabulary, resulting in linguistically correct and natural-sounding responses. GPT similarity : Quantifies the semantic similarity between a ground truth sentence (or document) and the prediction sentence generated by an AI model. In the Azure AI Foundry portal, you can explore the model benchmarks for all available models, before deploying a model: Manual evaluations Manual evaluations involve human raters who assess the quality of the model's responses. This approach provides insights into aspects that automated metrics might miss, such as context relevance and user satisfaction. Human evaluators can rate responses based on criteria like relevance, informativeness, and engagement. AI-assisted metrics AI-assisted metrics use advanced techniques to evaluate model performance. These metrics can include: Generation quality metrics : These metrics evaluate the overall quality of the generated text, considering factors like creativity, coherence, and adherence to the desired style or tone. Risk and safety metrics : These metrics assess the potential risks and safety concerns associated with the model's outputs. They help ensure that the model doesn't generate harmful or biased content. Natural language processing metrics Natural language processing (NLP) metrics are also valuable in evaluating model performance. One such metric is the F1-score , which measures the ratio of the number of shared words between the generated and ground truth answers. The F1-score is useful for tasks like text classification and information retrieval, where precision and recall are important. Other common NLP metrics include: BLEU : Bilingual Evaluation Understudy metric METEOR : Metric for Evaluation of Translation with Explicit Ordering ROUGE : Recall-Oriented Understudy for Gisting Evaluation All of these metrics are used to quantify the level of overlap in the model-generated response and the ground truth (expected response).\",\n",
       "  'links': ['https://learn.microsoft.com/wwl-data-ai/evaluate-models-azure-ai-studio/media/model-benchmarks.png#lightbox'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/evaluate-models-azure-ai-studio/media/interact-model.png',\n",
       "    'image_alt': 'Diagram of an interaction with a language model.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/evaluate-models-azure-ai-studio/media/chat-flow-diagram.png',\n",
       "    'image_alt': 'Diagram of a chat flow using a language model.'},\n",
       "   {'image_name': 'image3',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/evaluate-models-azure-ai-studio/media/model-benchmarks.png',\n",
       "    'image_alt': 'Screenshot of model benchmarks in Azure AI Foundry portal.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Evaluate generative AI performance in Azure AI Foundry portal',\n",
       "  'topic': 'Manually evaluate the performance of a model',\n",
       "  'text': \"During the early phases of the development of your generative AI app, you want to experiment and iterate quickly. To easily assess whether your selected language model and app, created with prompt flow, meet your requirements, you can manually evaluate models and flows in the Azure AI Foundry portal. Even when your model and app are already in production, manual evaluations are a crucial part of assessing performance. As manual evaluations are done by humans, they can provide insights that automated metrics might miss. Let's explore how you can manually evaluate your selected models and app in the Azure AI Foundry portal. Prepare your test prompts To begin the manual evaluation process, it's essential to prepare a diverse set of test prompts that reflect the range of queries and tasks your app is expected to handle. These prompts should cover various scenarios, including common user questions, edge cases, and potential failure points. By doing so, you can comprehensively assess the app's performance and identify areas for improvement. Test the selected model in the chat playground When you develop a chat application, you use a language model to generate a response. You create a chat application by developing a prompt flow that encapsulates your chat application's logic, which can use multiple language models to ultimately generate a response to a user question. Before you test your app's response, you can test the selected language model's response to verify the individual model works as expected. You can test a model you deployed in the Azure AI Foundry portal by interacting with it in the chat playground . The chat playground is ideal for early development. You can enter a prompt, see how the model responds, and tweak the prompt or system message to make improvements. After applying the changes, you can test a prompt again to evaluate whether the model's performance indeed improved. Evaluate multiple prompts with manual evaluations The chat playground is an easy way to get started. When you want to manually evaluate multiple prompts more quickly, you can use the manual evaluations feature. This feature allows you to upload a dataset with multiple questions, and optionally add an expected response, to evaluate the model's performance on a larger test dataset. You can rate the model's responses with the thumbs up or down feature. Based on the overall rating, you can try to improve your model by changing input prompt, the system message, the model, or the model's parameters. When you use manual evaluations, you can more quickly evaluate the model's performance based on a diverse test dataset and improve the model based on the test results. After manually evaluating an individual model, you can integrate the model into a chat application with prompt flow. Any flow you create with prompt flow can also be evaluated manually or automatically. Next, let's explore the evaluation of flows.\",\n",
       "  'links': ['https://learn.microsoft.com/wwl-data-ai/evaluate-models-azure-ai-studio/media/chat-playground.png#lightbox',\n",
       "   'https://learn.microsoft.com/wwl-data-ai/evaluate-models-azure-ai-studio/media/manual-evaluations.png#lightbox'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/evaluate-models-azure-ai-studio/media/chat-playground.png',\n",
       "    'image_alt': 'Screenshot of the chat playground in the Azure AI Foundry portal.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/evaluate-models-azure-ai-studio/media/manual-evaluations.png',\n",
       "    'image_alt': 'Screenshot of manual evaluations in the Azure AI Foundry portal.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Evaluate generative AI performance in Azure AI Foundry portal',\n",
       "  'topic': 'Automated evaluations',\n",
       "  'text': 'Automated evaluations in Azure AI Foundry portal enable you to assess the quality and content safety performance of models, datasets, or prompt flows. Evaluation data To evaluate a model, you need a dataset of prompts and responses (and optionally, expected responses as \"ground truth\"). You can compile this dataset manually or use the output from an existing application; but a useful way to get started is to use an AI model to generate a set of prompts and responses related to a specific subject. You can then edit the generated prompts and responses to reflect your desired output, and use them as ground truth to evaluate the responses from another model. Evaluation metrics Automated evaluation enables you to choose which evaluators you want to assess your model\\'s responses, and which metrics those evaluators should calculate. There are evaluators that help you measure: AI Quality : The quality of your model\\'s responses is measured by using AI models to evaluate them for metrics like coherence and relevance and by using standard NLP metrics like F1 score, BLEU, METEOR, and ROUGE based on ground truth (in the form of expected response text) Risk and safety : evaluators that assess the responses for content safety issues, including violence, hate, sexual content, and content related to self-harm.',\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/evaluate-models-azure-ai-studio/media/ai-generated-test-data.png',\n",
       "    'image_alt': 'Screenshot of AI-generated evaluation data.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Evaluate generative AI performance in Azure AI Foundry portal',\n",
       "  'topic': 'Exercise - Evaluate generative AI model performance',\n",
       "  'text': \"If you have an Azure subscription, you can use Azure AI Foundry portal to evaluate the performance of a generative AI app. Note If you don't have an Azure subscription, and you want to explore Azure AI Studio, you can sign up for an account , which includes credits for the first 30 days. Launch the exercise and follow the instructions.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2277720&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop generative AI apps in Azure - Training',\n",
       "  'unit': 'Evaluate generative AI performance in Azure AI Foundry portal',\n",
       "  'topic': 'Summary',\n",
       "  'text': 'In this module, you learned to: Understand model benchmarks. Perform manual evaluations. Perform automated evaluations. Learn more Observability in generative AI Azure AI Foundry Discord Azure AI Foundry Developer Forum',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/observability',\n",
       "   'https://aka.ms/azureaifoundry/discord',\n",
       "   'https://aka.ms/azureaifoundry/forum'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Analyze text with Azure AI Language',\n",
       "  'topic': 'Introduction',\n",
       "  'text': 'Every day, the world generates a vast quantity of data; much of it text-based in the form of emails, social media posts, online reviews, business documents, and more. Artificial intelligence techniques that apply statistical and semantic models enable you to create applications that extract meaning and insights from this text-based data. The Azure AI Language provides an API for common text analysis techniques that you can easily integrate into your own application code. In this module, you will learn how to use Azure AI Language to: Detect language from text. Analyze text sentiment. Extract key phrases, entities, and linked entities.',\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Analyze text with Azure AI Language',\n",
       "  'topic': 'Provision an Azure AI Language resource',\n",
       "  'text': \"Azure AI Language is designed to help you extract information from text. It provides functionality that you can use for tasks like: Language detection - determining the language in which text is written. Key phrase extraction - identifying important words and phrases in the text that indicate the main points. Sentiment analysis - quantifying how positive or negative the text is. Named entity recognition - detecting references to entities, including people, locations, time periods, organizations, and more. Entity linking - identifying specific entities by providing reference links to Wikipedia articles. Azure resources for text analysis To use Azure AI Language to analyze text, you must provision a resource for it in your Azure subscription. You can provision a resource through Azure AI Foundry portal After you have provisioned a suitable resource in your Azure subscription, you can use its endpoint and one of its keys to call the Azure AI Language APIs from your code. You can call the Azure AI Language APIs by submitting requests in JSON format to the REST interface, or by using any of the available programming language-specific SDKs. Note The code examples in the subsequent units in this module show the JSON requests and responses exchanged with the REST interface. When using an SDK, the JSON requests are abstracted by appropriate objects and methods that encapsulate the same data values. You'll get a chance to try the SDK for C# or Python for yourself in the exercise later in the module.\",\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-text-ai-language/media/text-analytics-resource.png',\n",
       "    'image_alt': 'Diagram showing an Azure AI Language resource performing language detection, key phrase extraction, sentiment analysis, named entity recognition, and entity linking.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Analyze text with Azure AI Language',\n",
       "  'topic': 'Detect language',\n",
       "  'text': 'The Azure AI Language detection API evaluates text input and, for each document submitted, returns language identifiers with a score indicating the strength of the analysis. This capability is useful for content stores that collect arbitrary text, where language is unknown. Another scenario could involve a chat bot.  If a user starts a session with the chat bot, language detection can be used to determine which language they are using and allow you to configure your bot responses in the appropriate language. You can parse the results of this analysis to determine which language is used in the input document. The response also returns a score, which reflects the confidence of the model (a value between 0 and 1). Language detection can work with documents or single phrases. It\\'s important to note that the document size must be under 5,120 characters.  The size limit is per document and each collection is restricted to 1,000 items (IDs).  A sample of a properly formatted JSON payload that you might submit to the service in the request body is shown here, including a collection of documents , each containing a unique id and the text to be analyzed. Optionally, you can provide a countryHint to improve prediction performance. {\\n    \"kind\": \"LanguageDetection\",\\n    \"parameters\": {\\n        \"modelVersion\": \"latest\"\\n    },\\n    \"analysisInput\":{\\n        \"documents\":[\\n              {\\n                \"id\": \"1\",\\n                \"text\": \"Hello world\",\\n                \"countryHint\": \"US\"\\n              },\\n              {\\n                \"id\": \"2\",\\n                \"text\": \"Bonjour tout le monde\"\\n              }\\n        ]\\n    }\\n} The service will return a JSON response that contains a result for each document in the request body, including the predicted language and a value indicating the confidence level of the prediction.  The confidence level is a value ranging from 0 to 1 with values closer to 1 being a higher confidence level.  Here\\'s an example of a standard JSON response that maps to the above request JSON. {   \"kind\": \"LanguageDetectionResults\",\\n    \"results\": {\\n        \"documents\": [\\n          {\\n            \"detectedLanguage\": {\\n              \"confidenceScore\": 1,\\n              \"iso6391Name\": \"en\",\\n              \"name\": \"English\"\\n            },\\n            \"id\": \"1\",\\n            \"warnings\": []\\n          },\\n          {\\n            \"detectedLanguage\": {\\n              \"confidenceScore\": 1,\\n              \"iso6391Name\": \"fr\",\\n              \"name\": \"French\"\\n            },\\n            \"id\": \"2\",\\n            \"warnings\": []\\n          }\\n        ],\\n        \"errors\": [],\\n        \"modelVersion\": \"2022-10-01\"\\n    }\\n} In our sample, all of the languages show a confidence of 1, mostly because the text is relatively simple and easy to identify the language for. If you pass in a document that has multilingual content, the service will behave a bit differently.  Mixed language content within the same document returns the language with the largest representation in the content, but with a lower positive rating, reflecting the marginal strength of that assessment. In the following example, the input is a blend of English, Spanish, and French. The analyzer uses statistical analysis of the text to determine the predominant language. {\\n  \"documents\": [\\n    {\\n      \"id\": \"1\",\\n      \"text\": \"Hello, I would like to take a class at your University. ¿Se ofrecen clases en español? Es mi primera lengua y más fácil para escribir. Que diriez-vous des cours en français?\"\\n    }\\n  ]\\n} The following sample shows a response for this multi-language example. {\\n    \"documents\": [\\n        {\\n            \"id\": \"1\",\\n            \"detectedLanguage\": {\\n                \"name\": \"Spanish\",\\n                \"iso6391Name\": \"es\",\\n                \"confidenceScore\": 0.9375\\n            },\\n            \"warnings\": []\\n        }\\n    ],\\n    \"errors\": [],\\n    \"modelVersion\": \"2022-10-01\"\\n} The last condition to consider is when there is ambiguity as to the language content.  The scenario might happen if you submit textual content that the analyzer is not able to parse, for example because of character encoding issues when converting the text to a string variable.  As a result, the response for the language name and ISO code will indicate (unknown) and the score value will be returned as 0 .  The following example shows how the response would look. {\\n    \"documents\": [\\n        {\\n            \"id\": \"1\",\\n            \"detectedLanguage\": {\\n                \"name\": \"(Unknown)\",\\n                \"iso6391Name\": \"(Unknown)\",\\n                \"confidenceScore\": 0.0\\n            },\\n            \"warnings\": []\\n        }\\n    ],\\n    \"errors\": [],\\n    \"modelVersion\": \"2022-10-01\"\\n}',\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Analyze text with Azure AI Language',\n",
       "  'topic': 'Extract key phrases',\n",
       "  'text': 'Key phrase extraction is the process of evaluating the text of a document, or documents, and then identifying the main points around the context of the document(s). Key phrase extraction works best for larger documents (the maximum size that can be analyzed is 5,120 characters). As with language detection, the REST interface enables you to submit one or more documents for analysis. {\\n    \"kind\": \"KeyPhraseExtraction\",\\n    \"parameters\": {\\n        \"modelVersion\": \"latest\"\\n    },\\n    \"analysisInput\":{\\n        \"documents\":[\\n            {\\n              \"id\": \"1\",\\n              \"language\": \"en\",\\n              \"text\": \"You must be the change you wish \\n                       to see in the world.\"\\n            },\\n            {\\n              \"id\": \"2\",\\n              \"language\": \"en\",\\n              \"text\": \"The journey of a thousand miles \\n                       begins with a single step.\"\\n            }\\n        ]\\n    }\\n} The response contains a list of key phrases detected in each document: {\\n    \"kind\": \"KeyPhraseExtractionResults\",\\n    \"results\": {\\n    \"documents\": [   \\n        {\\n         \"id\": \"1\",\\n         \"keyPhrases\": [\\n           \"change\",\\n           \"world\"\\n         ],\\n         \"warnings\": []\\n       },\\n       {\\n         \"id\": \"2\",\\n         \"keyPhrases\": [\\n           \"miles\",\\n           \"single step\",\\n           \"journey\"\\n         ],\\n         \"warnings\": []\\n       }\\n],\\n    \"errors\": [],\\n    \"modelVersion\": \"2021-06-01\"\\n    }\\n}',\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Analyze text with Azure AI Language',\n",
       "  'topic': 'Analyze sentiment',\n",
       "  'text': 'Sentiment analysis is used to evaluate how positive or negative a text document is, which can be useful in various workloads, such as: Evaluating a movie, book, or product by quantifying sentiment based on reviews. Prioritizing customer service responses to correspondence received through email or social media messaging. When using Azure AI Language to evaluate sentiment, the response includes overall document sentiment and individual sentence sentiment for each document submitted to the service. For example, you could submit a single document for sentiment analysis like this: {\\n  \"kind\": \"SentimentAnalysis\",\\n  \"parameters\": {\\n    \"modelVersion\": \"latest\"\\n  },\\n  \"analysisInput\": {\\n    \"documents\": [\\n      {\\n        \"id\": \"1\",\\n        \"language\": \"en\",\\n        \"text\": \"Good morning!\"\\n      }\\n    ]\\n  }\\n} The response from the service might look like this: {\\n  \"kind\": \"SentimentAnalysisResults\",\\n  \"results\": {\\n    \"documents\": [\\n      {\\n        \"id\": \"1\",\\n        \"sentiment\": \"positive\",\\n        \"confidenceScores\": {\\n          \"positive\": 0.89,\\n          \"neutral\": 0.1,\\n          \"negative\": 0.01\\n        },\\n        \"sentences\": [\\n          {\\n            \"sentiment\": \"positive\",\\n            \"confidenceScores\": {\\n              \"positive\": 0.89,\\n              \"neutral\": 0.1,\\n              \"negative\": 0.01\\n            },\\n            \"offset\": 0,\\n            \"length\": 13,\\n            \"text\": \"Good morning!\"\\n          }\\n        ],\\n        \"warnings\": []\\n      }\\n    ],\\n    \"errors\": [],\\n    \"modelVersion\": \"2022-11-01\"\\n  }\\n} Sentence sentiment is based on confidence scores for positive , negative , and neutral classification values between 0 and 1. Overall document sentiment is based on sentences: If all sentences are neutral, the overall sentiment is neutral. If sentence classifications include only positive and neutral, the overall sentiment is positive. If the sentence classifications include only negative and neutral, the overall sentiment is negative. If the sentence classifications include positive and negative, the overall sentiment is  mixed.',\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Analyze text with Azure AI Language',\n",
       "  'topic': 'Extract entities',\n",
       "  'text': 'Named Entity Recognition identifies entities that are mentioned in the text. Entities are grouped into categories and subcategories, for example: Person Location DateTime Organization Address Email URL Note For a full list of categories, see the documentation . Input for entity recognition is similar to input for other Azure AI Language API functions: {\\n  \"kind\": \"EntityRecognition\",\\n  \"parameters\": {\\n    \"modelVersion\": \"latest\"\\n  },\\n  \"analysisInput\": {\\n    \"documents\": [\\n      {\\n        \"id\": \"1\",\\n        \"language\": \"en\",\\n        \"text\": \"Joe went to London on Saturday\"\\n      }\\n    ]\\n  }\\n} The response includes a list of categorized entities found in each document: {\\n    \"kind\": \"EntityRecognitionResults\",\\n     \"results\": {\\n          \"documents\":[\\n              {\\n                  \"entities\":[\\n                  {\\n                    \"text\":\"Joe\",\\n                    \"category\":\"Person\",\\n                    \"offset\":0,\\n                    \"length\":3,\\n                    \"confidenceScore\":0.62\\n                  },\\n                  {\\n                    \"text\":\"London\",\\n                    \"category\":\"Location\",\\n                    \"subcategory\":\"GPE\",\\n                    \"offset\":12,\\n                    \"length\":6,\\n                    \"confidenceScore\":0.88\\n                  },\\n                  {\\n                    \"text\":\"Saturday\",\\n                    \"category\":\"DateTime\",\\n                    \"subcategory\":\"Date\",\\n                    \"offset\":22,\\n                    \"length\":8,\\n                    \"confidenceScore\":0.8\\n                  }\\n                ],\\n                \"id\":\"1\",\\n                \"warnings\":[]\\n              }\\n          ],\\n          \"errors\":[],\\n          \"modelVersion\":\"2021-01-15\"\\n    }\\n} To learn more about entities see the Build a conversational language understanding model module.',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/language-service/named-entity-recognition/concepts/named-entity-categories?tabs=ga-api',\n",
       "   'https://learn.microsoft.com/en-us/training/modules/build-language-understanding-model/'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Analyze text with Azure AI Language',\n",
       "  'topic': 'Extract linked entities',\n",
       "  'text': 'In some cases, the same name might be applicable to more than one entity. For example, does an instance of the word \"Venus\" refer to the planet or the goddess from mythology? Entity linking can be used to disambiguate entities of the same name by referencing an article in a knowledge base. Wikipedia provides the knowledge base for the Text Analytics service.\\nSpecific article links are determined based on entity context within the text. For example, \"I saw Venus shining in the sky\" is associated with the link https://en.wikipedia.org/wiki/Venus ; while \"Venus, the goddess of beauty\" is associated with https://en.wikipedia.org/wiki/Venus_(mythology) . As with all Azure AI Language service functions, you can submit one or more documents for analysis: {\\n  \"kind\": \"EntityLinking\",\\n  \"parameters\": {\\n    \"modelVersion\": \"latest\"\\n  },\\n  \"analysisInput\": {\\n    \"documents\": [\\n      {\\n        \"id\": \"1\",\\n        \"language\": \"en\",\\n        \"text\": \"I saw Venus shining in the sky\"\\n      }\\n    ]\\n  }\\n} The response includes the entities identified in the text along with links to associated articles: {\\n  \"kind\": \"EntityLinkingResults\",\\n  \"results\": {\\n    \"documents\": [\\n      {\\n        \"id\": \"1\",\\n        \"entities\": [\\n          {\\n            \"bingId\": \"89253af3-5b63-e620-9227-f839138139f6\",\\n            \"name\": \"Venus\",\\n            \"matches\": [\\n              {\\n                \"text\": \"Venus\",\\n                \"offset\": 6,\\n                \"length\": 5,\\n                \"confidenceScore\": 0.01\\n              }\\n            ],\\n            \"language\": \"en\",\\n            \"id\": \"Venus\",\\n            \"url\": \"https://en.wikipedia.org/wiki/Venus\",\\n            \"dataSource\": \"Wikipedia\"\\n          }\\n        ],\\n        \"warnings\": []\\n      }\\n    ],\\n    \"errors\": [],\\n    \"modelVersion\": \"2021-06-01\"\\n  }\\n}',\n",
       "  'links': ['https://en.wikipedia.org/wiki/Venus',\n",
       "   'https://en.wikipedia.org/wiki/Venus_(mythology)'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Analyze text with Azure AI Language',\n",
       "  'topic': 'Exercise - Analyze text',\n",
       "  'text': \"In this exercise, you use the Azure AI Language SDK to develop a client application that analyzes text. Note To complete this lab, you need an Azure subscription in which you have administrative access. Launch the exercise and follow the instructions. Tip After completing the exercise, if you've finished exploring Azure AI services, delete the Azure resources that you created during the exercise.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2320757&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Analyze text with Azure AI Language',\n",
       "  'topic': 'Summary',\n",
       "  'text': 'In this module, you learned how to use Azure AI Language to: Detect language from text. Analyze text sentiment. Extract key phrases, entities, and linked entities. To learn more about Azure AI Language and some of the concepts covered in this module, you can explore the following documentation pages: Azure AI Language documentation Build a conversational language understanding model Create a custom named entity extraction solution',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/language-service/',\n",
       "   'https://learn.microsoft.com/en-us/training/modules/build-language-understanding-model/',\n",
       "   'https://learn.microsoft.com/en-us/training/modules/custom-name-entity-recognition/'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create question answering solutions with Azure AI Language',\n",
       "  'topic': 'Introduction',\n",
       "  'text': 'A common pattern for \"intelligent\" applications is to enable users to ask questions using natural language, and receive appropriate answers. In effect, this kind of solution brings conversational intelligence to a traditional frequently asked questions (FAQ) publication. In this module, you will learn how to use Azure AI Language to create a knowledge base of question and answer pairs that can support an application or bot. After completing this module, you’ll be able to: Understand question answering and how it compares to language understanding. Create, test, publish and consume a knowledge base. Implement multi-turn conversation and active learning. Create a question answering bot to interact with using natural language.',\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create question answering solutions with Azure AI Language',\n",
       "  'topic': 'Understand question answering',\n",
       "  'text': 'Azure AI Language includes a question answering capability, which enables you to define a knowledge base of question and answer pairs that can be queried using natural language input. The knowledge base can be published to a REST endpoint and consumed by client applications, commonly bots . The knowledge base can be created from existing sources, including: Web sites containing frequently asked question (FAQ) documentation. Files containing structured text, such as brochures or user guides. Built-in chit chat question and answer pairs that encapsulate common conversational exchanges. Note The question answering capability of Azure AI Language is a newer version of the QnA Service , which still exists as a standalone service. To learn how to migrate a QnA Maker knowledge base to Azure AI Language, see the migration guide .',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/language-service/question-answering/how-to/migrate-qnamaker'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/create-question-answer-solution-ai-language/media/diagram.png',\n",
       "    'image_alt': 'A diagram showing how a conversational app uses a knowledge base of questions and answers.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create question answering solutions with Azure AI Language',\n",
       "  'topic': 'Compare question answering to Azure AI Language understanding',\n",
       "  'text': 'A question answering knowledge base is a form of language model, which raises the question of when to use question answering, and when to use the conversational language understanding capabilities of Azure AI Language. The two features are similar in that they both enable you to define a language model that can be queried using natural language expressions. However, there are some differences in the use cases that they are designed to address, as shown in the following table: Question answering Language understanding Usage pattern User submits a question, expecting an answer User submits an utterance, expecting an appropriate response or action Query processing Service uses natural language understanding to match the question to an answer in the knowledge base Service uses natural language understanding to interpret the utterance, match it to an intent, and identify entities Response Response is a static answer to a known question Response indicates the most likely intent and referenced entities Client logic Client application typically presents the answer to the user Client application is responsible for performing appropriate action based on the detected intent The two services are in fact complementary. You can build comprehensive natural language solutions that combine language understanding models and question answering knowledge bases.',\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create question answering solutions with Azure AI Language',\n",
       "  'topic': 'Create a knowledge base',\n",
       "  'text': \"To create a question answering solution, you can use the REST API or SDK to write code that defines, trains, and publishes the knowledge base. However, it's more common to use the Language Studio web interface to define and manage a knowledge base. To create a knowledge base you: Sign in to Azure portal. Search for Azure AI services using the search field at the top of the portal. Select Create under the Language Service resource. Create a resource in your Azure subscription: Enable the question answering feature. Create or select an Azure AI Search resource to host the knowledge base index. In Language Studio, select your Azure AI Language resource and create a Custom question answering project. Add one or more data sources to populate the knowledge base: URLs for web pages containing FAQs. Files containing structured text from which questions and answers can be derived. Predefined chit-chat datasets that include common conversational questions and responses in a specified style. Edit question and answer pairs in the portal.\",\n",
       "  'links': ['https://language.azure.com'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create question answering solutions with Azure AI Language',\n",
       "  'topic': 'Implement multi-turn conversation',\n",
       "  'text': 'Although you can often create an effective knowledge base that consists of individual question and answer pairs, sometimes you might need to ask follow-up questions to elicit more information from a user before presenting a definitive answer. This kind of interaction is referred to as a multi-turn conversation. You can enable multi-turn responses when importing questions and answers from an existing web page or document based on its structure, or you can explicitly define follow-up prompts and responses for existing question and answer pairs. For example, suppose an initial question for a travel booking knowledge base is \"How can I cancel a reservation?\". A reservation might refer to a hotel or a flight, so a follow-up prompt is required to clarify this detail. The answer might consist of text such as \"Cancellation policies depend on the type of reservation\" and include follow-up prompts with links to answers about canceling flights and canceling hotels. When you define a follow-up prompt for multi-turn conversation, you can link to an existing answer in the knowledge base or define a new answer specifically for the follow-up. You can also restrict the linked answer so that it is only ever displayed in the context of the multi-turn conversation initiated by the original question.',\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/create-question-answer-solution-ai-language/media/multi-turn-conversation.png',\n",
       "    'image_alt': 'A diagram showing a multi-turn conversation.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create question answering solutions with Azure AI Language',\n",
       "  'topic': 'Test and publish a knowledge base',\n",
       "  'text': \"After you have defined a knowledge base, you can train its natural language model, and test it before publishing it for use in an application or bot. Testing a knowledge base You can test your knowledge base interactively in Language Studio, submitting questions and reviewing the answers that are returned. You can inspect the results to view their confidence scores as well as other potential answers. Deploying a knowledge base When you're happy with the performance of your knowledge base, you can deploy it to a REST endpoint that client applications can use to submit questions and receive answers. You can deploy it directly from Language Studio.\",\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/create-question-answer-solution-ai-language/media/test-new-small.png',\n",
       "    'image_alt': 'Screenshot of the test pane of the custom question answering project in the Language studio.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create question answering solutions with Azure AI Language',\n",
       "  'topic': 'Use a knowledge base',\n",
       "  'text': 'To consume the published knowledge base, you can use the REST interface. The minimal request body for the function contains a question, like this: {\\n  \"question\": \"What do I need to do to cancel a reservation?\",\\n  \"top\": 2,\\n  \"scoreThreshold\": 20,\\n  \"strictFilters\": [\\n    {\\n      \"name\": \"category\",\\n      \"value\": \"api\"\\n    }\\n  ]\\n} Property Description question Question to send to the knowledge base. top Maximum number of answers to be returned. scoreThreshold Score threshold for answers returned. strictFilters Limit to only answers that contain the specified metadata. The response includes the closest question match that was found in the knowledge base, along with the associated answer, the confidence score, and other metadata about the question and answer pair: {\\n  \"answers\": [\\n    {\\n      \"score\": 27.74823341616769,\\n      \"id\": 20,\\n      \"answer\": \"Call us on 555 123 4567 to cancel a reservation.\",\\n      \"questions\": [\\n        \"How can I cancel a reservation?\"\\n      ],\\n      \"metadata\": [\\n        {\\n          \"name\": \"category\",\\n          \"value\": \"api\"\\n        }\\n      ]\\n    }\\n  ]\\n}',\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create question answering solutions with Azure AI Language',\n",
       "  'topic': 'Improve question answering performance',\n",
       "  'text': 'After creating and testing a knowledge base, you can improve its performance with active learning and by defining synonyms . Use active learning Active learning can help you make continuous improvements to get better at answering user questions correctly over time. People often ask questions that are phrased differently, but ultimately have the same meaning. Active learning can help in situations like this because it enables you to consider alternate questions to each question and answer pair. Active learning is enabled by default. To use active learning, you can do the following: Create your question and answer pairs You create pairs of questions and answers in Language Studio for your project. You can also import a file that contains question and answer pairs to upload in bulk. Review suggestions Active learning then begins to offer alternate questions for each question in your question and answer pairs. You access this from the Review suggestions pane: You review, and then accept or reject these alternate phrases suggested for each question by selecting the checkmark or delete symbol next to the alternate phrase. You can bulk accept or reject suggestions using the Accept all suggestions or Reject all suggestions option at the top. You can also manually add alternate questions when you select Add alternate question for a pair in the Edit knowledge base pane: Note To learn more about active learning, see Enrich your project with active learning . Define synonyms Synonyms are useful when questions submitted by users might include multiple different words to mean the same thing. For example, a travel agency customer might refer to a \"reservation\" or a \"booking\". By defining these as synonyms, the question answering service can find an appropriate answer regardless of which term an individual customer uses. To define synonyms, you use the REST API to submit synonyms in the following JSON format: {\\n    \"synonyms\": [\\n        {\\n            \"alterations\": [\\n                \"reservation\",\\n                \"booking\"\\n                ]\\n        }\\n    ]\\n} Note To learn more about synonyms, see the Improve quality of response with synonyms .',\n",
       "  'links': ['https://learn.microsoft.com/wwl-data-ai/create-question-answer-solution-ai-language/media/import-file.png#lightbox',\n",
       "   'https://learn.microsoft.com/wwl-data-ai/create-question-answer-solution-ai-language/media/review-suggestions.png#lightbox',\n",
       "   'https://learn.microsoft.com/wwl-data-ai/create-question-answer-solution-ai-language/media/add-alternate-questions-manual.png#lightbox',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/language-service/question-answering/tutorials/active-learning',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/language-service/question-answering/tutorials/adding-synonyms'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/create-question-answer-solution-ai-language/media/import-file-small.png',\n",
       "    'image_alt': 'A screenshot showing how to import a file with question and answer pairs.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/create-question-answer-solution-ai-language/media/review-suggestions-small.png',\n",
       "    'image_alt': 'A screenshot of the Review suggestions pane.'},\n",
       "   {'image_name': 'image3',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/create-question-answer-solution-ai-language/media/add-alternate-questions-manual-small.png',\n",
       "    'image_alt': 'A screenshot showing the Add alternate question option on the Edit knowledge base pane.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create question answering solutions with Azure AI Language',\n",
       "  'topic': 'Exercise - Create a question answering solution',\n",
       "  'text': \"In this exercise, you use Azure AI Language to create your own question answering model. Note To complete this lab, you need an Azure subscription . Launch the exercise and follow the instructions. Tip After completing the exercise, if you've finished exploring Azure AI services, delete the Azure resources that you created during the exercise.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2322213&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create question answering solutions with Azure AI Language',\n",
       "  'topic': 'Summary',\n",
       "  'text': \"In this module, you have learned how to use the question answering capability of Azure AI Language to create a knowledge base of question and answer pairs that can support an application or bot. Now that you've completed this module, you can: Understand question answering and how it compares to language understanding. Create, test, publish and consume a knowledge base. Implement multi-turn conversation and active learning. Create a question answering bot to interact with using natural language. To learn more about the question answering capability of Azure AI Language, see the Question answering documentation .\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/language-service/question-answering/overview'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Build a conversational language understanding model',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"Natural language processing (NLP) is a common AI problem in which software must be able to work with text or speech in the natural language form that a human user would write or speak. Within the broader area of NLP, natural language understanding (NLU) deals with the problem of determining semantic meaning from natural language - usually by using a trained language model. A common design pattern for a natural language understanding solution looks like this: In this design pattern: An app accepts natural language input from a user. A language model is used to determine semantic meaning (the user's intent ). The app performs an appropriate action. Azure AI Language enables developers to build apps based on language models that can be trained with a relatively small number of samples to discern a user's intended meaning. In this module, you'll learn how to use the service to create a natural language understanding app using Azure AI Language. After completing this module, you’ll be able to: Provision an Azure AI Language resource. Define intents, entities, and utterances. Use patterns to differentiate similar utterances. Use pre-built entity components. Train, test, publish, and review a model.\",\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/build-language-understanding-model/media/language-understanding-app.png',\n",
       "    'image_alt': 'Diagram showing an app accepts natural language input, and uses a model to determine semantic meaning before taking the appropriate action.'}],\n",
       "  'videos': [{'name': 'video1',\n",
       "    'video_src': 'https://play.vidyard.com/XYUdrPj43e5HhMYB6PRFVP?loop=1'}],\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Build a conversational language understanding model',\n",
       "  'topic': 'Understand prebuilt capabilities of the Azure AI Language service',\n",
       "  'text': 'The Azure AI Language service provides various features for understanding human language. You can use each feature to better communicate with users, better understand incoming communication, or use them together to provide more insight into what the user is saying, intending, and asking about. Azure AI Language service features fall into two categories: Pre-configured features, and Learned features. Learned features require building and training a model to correctly predict appropriate labels, which is covered in upcoming units of this module. This unit covers most of the capabilities of the Azure AI Language service, but head over to the Azure AI Language service documentation for a full list, including quickstarts and a full explanation of everything available. Using these features in your app requires sending your query to the appropriate endpoint. The endpoint used to query a specific feature varies, but all of them are prefixed with the Azure AI Language resource you created in your Azure account, either when building your REST request or defining your client using an SDK. Examples of each can be found in the next unit. Pre-configured features The Azure AI Language service provides certain features without any model labeling or training. Once you create your resource, you can send your data and use the returned results within your app. The following features are all pre-configured. Summarization Summarization is available for both documents and conversations, and will summarize the text into key sentences that are predicted to encapsulate the input\\'s meaning. Named entity recognition Named entity recognition can extract and identify entities, such as people, places, or companies, allowing your app to recognize different types of entities for improved natural language responses. For example, given the text \"The waterfront pier is my favorite Seattle attraction\", Seattle would be identified and categorized as a location. Personally identifiable information (PII) detection PII detection allows you to identify, categorize, and redact information that could be considered sensitive, such as email addresses, home addresses, IP addresses, names, and protected health information. For example, if the text \"email@contoso.com\" was included in the query, the entire email address can be identified and redacted. Key phrase extraction Key phrase extraction is a feature that quickly pulls the main concepts out of the provided text. For example, given the text \"Text Analytics is one of the features in Azure AI Services.\", the service would extract \"Azure AI Services\" and \"Text Analytics\" . Sentiment analysis Sentiment analysis identifies how positive or negative a string or document is. For example, given the text \"Great hotel. Close to plenty of food and attractions we could walk to\", the service would identify that as positive with a relatively high confidence score. Language detection Language detection takes one or more documents, and identifies the language for each. For example, if the text of one of the documents was \"Bonjour\", the service would identify that as French . Learned features Learned features require you to label data, train, and deploy your model to make it available to use in your application. These features allow you to customize what information is predicted or extracted. Note Quality of data greatly impacts the model\\'s accuracy. Be intentional about what data is used, how well it is tagged or labeled, and how varied the training data is. For details, see recommendations for labeling data , which includes valuable guidelines for tagging data. Also see the evaluation metrics that can assist in learning where your model needs improvement. Conversational language understanding (CLU) CLU is one of the core custom features offered by Azure AI Language. CLU helps users to build custom natural language understanding models to predict overall intent and extract important information from incoming utterances. CLU does require data to be tagged by the user to teach it how to predict intents and entities accurately. The exercise in this module will be building a CLU model and using it in your app. Custom named entity recognition Custom entity recognition takes custom labeled data and extracts specified entities from unstructured text. For example, if you have various contract documents that you want to extract involved parties from, you can train a model to recognize how to predict them. Custom text classification Custom text classification enables users to classify text or documents as custom defined groups. For example, you can train a model to look at news articles and identify the category they should fall into, such as News or Entertainment . Question answering Question answering is a mostly pre-configured feature that provides answers to questions provided as input. The data to answer these questions comes from documents like FAQs or manuals. For example, say you want to make a virtual chat assistant on your company website to answer common questions. You could use a company FAQ as the input document to create the question and answer pairs. Once deployed, your chat assistant can pass input questions to the service, and get the answers as a result. For a complete list of capabilities and how to use them, see the Azure AI Language documentation .',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/overview?azure-portal=true',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/language-service/conversational-language-understanding/how-to/tag-utterances',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/language-service/custom-text-classification/concepts/evaluation-metrics',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/language-service/overview?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Build a conversational language understanding model',\n",
       "  'topic': 'Understand resources for building a conversational language understanding model',\n",
       "  'text': 'To use the Language Understanding service to develop a NLP solution, you\\'ll need to create a Language resource in Azure. That resource will be used for both authoring your model and processing prediction requests from client applications. Tip This module\\'s lab covers building a model for conversational language understanding. For more focused modules on custom text classification and custom named entity recognition, see the custom solution modules in the Develop natural language solutions learning path. Build your model For features that require a model for prediction, you\\'ll need to build, train and deploy that model before using it to make a prediction. This building and training will teach the Azure AI Language service what to look for. First, you\\'ll need to create your Azure AI Language resource in the Azure portal . Then: Search for Azure AI services . Find and select Language Service . Select Create under the Language Service . Fill out the necessary details, choosing the region closest to you geographically (for best performance) and giving it a unique name. Once that resource has been created, you\\'ll need a key and the endpoint. You can find that on the left side under Keys and Endpoint of the resource overview page. Use Language Studio For a more visual method of building, training, and deploying your model, you can use Language Studio to achieve each of these steps. On the main page, you can choose to create a Conversational language understanding project. Once the project is created, then go through the same process as above to build, train, and deploy your model. The lab in this module will walk through using Language Studio to build your model. If you\\'d like to learn more, see the Language Studio quickstart Use the REST API One way to build your model is through the REST API. The pattern would be to create your project, import data, train, deploy, then use your model. These tasks are done asynchronously; you\\'ll need to submit a request to the appropriate URI for each step, and then send another request to get the status of that job. For example, if you want to deploy a model for a conversational language understanding project, you\\'d submit the deployment job, and then check on the deployment job status. Authentication For each call to your Azure AI Language resource, you authenticate the request by providing the following header. Key Value Ocp-Apim-Subscription-Key The key to your resource Request deployment Submit a POST request to the following endpoint. {ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/deployments/{DEPLOYMENT-NAME}?api-version={API-VERSION} Placeholder Value Example {ENDPOINT} The endpoint of your Azure AI Language resource https://<your-subdomain>.cognitiveservices.azure.com {PROJECT-NAME} The name for your project. This value is case-sensitive myProject {DEPLOYMENT-NAME} The name for your deployment. This value is case-sensitive staging {API-VERSION} The version of the API you\\'re calling 2022-05-01 Include the following body with your request. {\\n  \"trainedModelLabel\": \"{MODEL-NAME}\",\\n} Placeholder Value {MODEL-NAME} The model name that will be assigned to your deployment. This value is case-sensitive. Successfully submitting your request will receive a 202 response, with a response header of operation-location . This header will have a URL with which to request the status, formatted like this: {ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?api-version={API-VERSION} Get deployment status Submit a GET request to the URL from the response header above. The values will already be filled out based on the initial deployment request. {ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/deployments/{DEPLOYMENT-NAME}/jobs/{JOB-ID}?api-version={API-VERSION} Placeholder Value {ENDPOINT} The endpoint for authenticating your API request {PROJECT-NAME} The name for your project (case-sensitive) {DEPLOYMENT-NAME} The name for your deployment (case-sensitive) {JOB-ID} The ID for locating your model\\'s training status, found in the header value detailed above in the deployment request {API-VERSION} The version of the API you\\'re calling The response body will give the deployment status details. The status field will have the value of succeeded when the deployment is complete. {\\n    \"jobId\":\"{JOB-ID}\",\\n    \"createdDateTime\":\"String\",\\n    \"lastUpdatedDateTime\":\"String\",\\n    \"expirationDateTime\":\"String\",\\n    \"status\":\"running\"\\n} For a full walkthrough of each step with example requests, see the conversational understanding quickstart . Query your model To query your model for a prediction, you can use SDKs in C# or Python, or use the REST API. Query using SDKs To query your model using an SDK, you first need to create your client. Once you have your client, you then use it to call the appropriate endpoint. var languageClient = new TextAnalyticsClient(endpoint, credentials);\\nvar response = languageClient.ExtractKeyPhrases(document); language_client = TextAnalyticsClient(\\n            endpoint=endpoint, \\n            credential=credentials)\\nresponse = language_client.extract_key_phrases(documents = documents)[0] Other language features, such as the conversational language understanding, require the request be built and sent differently. var data = new\\n{\\n    analysisInput = new\\n    {\\n        conversationItem = new\\n        {\\n            text = userText,\\n            id = \"1\",\\n            participantId = \"1\",\\n        }\\n    },\\n    parameters = new\\n    {\\n        projectName,\\n        deploymentName,\\n        // Use Utf16CodeUnit for strings in .NET.\\n        stringIndexType = \"Utf16CodeUnit\",\\n    },\\n    kind = \"Conversation\",\\n};\\nResponse response = await client.AnalyzeConversationAsync(RequestContent.Create(data)); result = client.analyze_conversation(\\n    task={\\n        \"kind\": \"Conversation\",\\n        \"analysisInput\": {\\n            \"conversationItem\": {\\n                \"participantId\": \"1\",\\n                \"id\": \"1\",\\n                \"modality\": \"text\",\\n                \"language\": \"en\",\\n                \"text\": query\\n            },\\n            \"isLoggingEnabled\": False\\n        },\\n        \"parameters\": {\\n            \"projectName\": cls_project,\\n            \"deploymentName\": deployment_slot,\\n            \"verbose\": True\\n        }\\n    }\\n) Query using the REST API To query your model using REST, create a POST request to the appropriate URL with the appropriate body specified. For built in features such as language detection or sentiment analysis, you\\'ll query the analyze-text endpoint. Tip Remember each request needs to be authenticated with your Azure AI Language resource key in the Ocp-Apim-Subscription-Key header {ENDPOINT}/language/:analyze-text?api-version={API-VERSION} Placeholder Value {ENDPOINT} The endpoint for authenticating your API request {API-VERSION} The version of the API you\\'re calling Within the body of that request, you must specify the kind parameter, which tells the service what type of language understanding you\\'re requesting. If you want to detect the language, for example, the JSON body would look something like the following. {\\n    \"kind\": \"LanguageDetection\",\\n    \"parameters\": {\\n        \"modelVersion\": \"latest\"\\n    },\\n    \"analysisInput\":{\\n        \"documents\":[\\n            {\\n                \"id\":\"1\",\\n                \"text\": \"This is a document written in English.\"\\n            }\\n        ]\\n    }\\n} Other language features, such as the conversational language understanding, require the request be routed to a different endpoint. For example, the conversational language understanding request would be sent to the following. {ENDPOINT}/language/:analyze-conversations?api-version={API-VERSION} Placeholder Value {ENDPOINT} The endpoint for authenticating your API request {API-VERSION} The version of the API you\\'re calling That request would include a JSON body similar to the following. {\\n  \"kind\": \"Conversation\",\\n  \"analysisInput\": {\\n    \"conversationItem\": {\\n      \"id\": \"1\",\\n      \"participantId\": \"1\",\\n      \"text\": \"Sample text\"\\n    }\\n  },\\n  \"parameters\": {\\n    \"projectName\": \"{PROJECT-NAME}\",\\n    \"deploymentName\": \"{DEPLOYMENT-NAME}\",\\n    \"stringIndexType\": \"TextElement_V8\"\\n  }\\n} Placeholder Value {PROJECT-NAME} The name of the project where you built your model {DEPLOYMENT-NAME} The name of your deployment Sample response The query response from an SDK will in the object returned, which varies depending on the feature (such as in response.key_phrases or response.Value ). The REST API will return JSON that would be similar to the following. {\\n    \"kind\": \"KeyPhraseExtractionResults\",\\n    \"results\": {\\n        \"documents\": [{\\n            \"id\": \"1\",\\n            \"keyPhrases\": [\"modern medical office\", \"Dr. Smith\", \"great staff\"],\\n            \"warnings\": []\\n        }],\\n        \"errors\": [],\\n        \"modelVersion\": \"{VERSION}\"\\n    }\\n} For other models like conversational language understanding, a sample response to your query would be similar to the following. {\\n  \"kind\": \"ConversationResult\",\\n  \"result\": {\\n    \"query\": \"String\",\\n    \"prediction\": {\\n      \"topIntent\": \"intent1\",\\n      \"projectKind\": \"Conversation\",\\n      \"intents\": [\\n        {\\n          \"category\": \"intent1\",\\n          \"confidenceScore\": 1\\n        },\\n        {\\n          \"category\": \"intent2\",\\n          \"confidenceScore\": 0\\n        }\\n      ],\\n      \"entities\": [\\n        {\\n          \"category\": \"entity1\",\\n          \"text\": \"text\",\\n          \"offset\": 7,\\n          \"length\": 4,\\n          \"confidenceScore\": 1\\n        }\\n      ]\\n    }\\n  }\\n} The SDKs for both Python and C# return JSON that is very similar to the REST response. For full documentation on features, including examples and how-to guides, see the Azure AI Language documentation documentation pages.',\n",
       "  'links': ['https://learn.microsoft.com/en-us/training/paths/develop-language-solutions-azure-ai?azure-portal=true',\n",
       "   'https://portal.azure.com/?azure-portal=true',\n",
       "   'https://aka.ms/languageStudio',\n",
       "   'https://learn.microsoft.com/wwl-data-ai/build-language-understanding-model/media/language-studio-conversational.png#lightbox',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/language-service/language-studio?azure-portal=true',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/language-service/conversational-language-understanding/quickstart?pivots=rest-api&azure-portal=true#create-a-clu-project',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/language-service/?azure-portal=true'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/build-language-understanding-model/media/language-studio-conversational-small.png',\n",
       "    'image_alt': 'Screenshot of the Language Studio home page.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Build a conversational language understanding model',\n",
       "  'topic': 'Define intents, utterances, and entities',\n",
       "  'text': 'Utterances are the phrases that a user might enter when interacting with an application that uses your language model. An intent represents a task or action the user wants to perform, or more simply the meaning of an utterance. You create a model by defining intents and associating them with one or more utterances. For example, consider the following list of intents and associated utterances: GetTime : \"What time is it?\" \"What is the time?\" \"Tell me the time\" GetWeather : \"What is the weather forecast?\" \"Do I need an umbrella?\" \"Will it snow?\" TurnOnDevice \"Turn the light on.\" \"Switch on the light.\" \"Turn on the fan\" None : \"Hello\" \"Goodbye\" In your model, you must define the intents that you want your model to understand, so spend some time considering the domain your model must support and the kinds of actions or information that users might request. In addition to the intents that you define, every model includes a None intent that you should use to explicitly identify utterances that a user might submit, but for which there is no specific action required (for example, conversational greetings like \"hello\") or that fall outside of the scope of the domain for this model. After you\\'ve identified the intents your model must support, it\\'s important to capture various different example utterances for each intent.  Collect utterances that you think users will enter; including utterances meaning the same thing but that are constructed in different ways. Keep these guidelines in mind: Capture multiple different examples, or alternative ways of saying the same thing Vary the length of the utterances from short, to medium, to long Vary the location of the noun or subject of the utterance.  Place it at the beginning, the end, or somewhere in between Use correct grammar and incorrect grammar in different utterances to offer good training data examples The precision, consistency and completeness of your labeled data are key factors to determining model performance. Label precisely : Label each entity to its right type always. Only include what you want extracted, avoid unnecessary data in your labels. Label consistently : The same entity should have the same label across all the utterances. Label completely : Label all the instances of the entity in all your utterances. Entities are used to add specific context to intents. For example, you might define a TurnOnDevice intent that can be applied to multiple devices, and use entities to define the different devices. Consider the following utterances, intents, and entities: Utterance Intent Entities What is the time? GetTime What time is it in London ? GetTime Location (London) What\\'s the weather forecast for Paris ? GetWeather Location (Paris) Will I need an umbrella tonight ? GetWeather Time (tonight) What\\'s the forecast for Seattle tomorrow ? GetWeather Location (Seattle), Time (tomorrow) Turn the light on. TurnOnDevice Device (light) Switch on the fan . TurnOnDevice Device (fan) You can split entities into a few different component types: Learned entities are the most flexible kind of entity, and should be used in most cases. You define a learned component with a suitable name, and then associate words or phrases with it in training utterances. When you train your model, it learns to match the appropriate elements in the utterances with the entity. List entities are useful when you need an entity with a specific set of possible values - for example, days of the week. You can include synonyms in a list entity definition, so you could define a DayOfWeek entity that includes the values \"Sunday\", \"Monday\", \"Tuesday\", and so on; each with synonyms like \"Sun\", \"Mon\", \"Tue\", and so on. Prebuilt entities are useful for common types such as numbers, datetimes, and names. For example, when prebuilt components are added, you will automatically detect values such as \"6\" or organizations such as \"Microsoft\". You can see this article for a list of supported prebuilt entities .',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/language-service/conversational-language-understanding/prebuilt-component-reference?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Build a conversational language understanding model',\n",
       "  'topic': 'Use patterns to differentiate similar utterances',\n",
       "  'text': 'In some cases, a model might contain multiple intents for which utterances are likely to be similar. You can use the pattern of utterances to disambiguate the intents while minimizing the number of sample utterances. For example, consider the following utterances: \"Turn on the kitchen light\" \"Is the kitchen light on?\" \"Turn off the kitchen light\" These utterances are syntactically similar, with only a few differences in words or punctuation. However, they represent three different intents (which could be named TurnOnDevice , GetDeviceStatus , and TurnOffDevice ). Additionally, the intents could apply to a wide range of entity values. In addition to \"kitchen light\", the intent could apply to \"living room light\", television\", or any other device that the model might need to support. To correctly train your model, provide a handful of examples of each intent that specify the different formats of utterances. TurnOnDevice : \"Turn on the {DeviceName}\" \"Switch on the {DeviceName}\" \"Turn the {DeviceName} on\" GetDeviceStatus : \"Is the {DeviceName} on[?]\" TurnOffDevice : \"Turn the {DeviceName} off\" \"Switch off the {DeviceName}\" \"Turn off the {DeviceName}\" When you teach your model with each different type of utterance, the Azure AI Language service can learn how to categorize intents correctly based off format and punctuation.',\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Build a conversational language understanding model',\n",
       "  'topic': 'Use pre-built entity components',\n",
       "  'text': 'You can create your own language models by defining all the intents and utterances it requires, but often you can use prebuilt components to detect common entities such as numbers, emails, URLs, or choices. For a full list of prebuilt entities the Azure AI Language service can detect, see the list of supported prebuilt entity components. Using prebuilt components allows you to let the Azure AI Language service automatically detect the specified type of entity, and not have to train your model with examples of that entity. To add a prebuilt component, you can create an entity in your project, then select Add new prebuilt to that entity to detect certain entities. You can have up to five prebuilt components per entity. Using prebuilt model elements can significantly reduce the time it takes to develop a conversational language understanding solution.',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/language-service/conversational-language-understanding/prebuilt-component-reference?azure-portal=true',\n",
       "   'https://learn.microsoft.com/wwl-data-ai/build-language-understanding-model/media/add-prebuilt-entity.png#lightbox'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/build-language-understanding-model/media/add-prebuilt-entity-small.png',\n",
       "    'image_alt': 'Screenshot of adding a prebuilt entity component.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Build a conversational language understanding model',\n",
       "  'topic': 'Train, test, publish, and review a conversational language understanding model',\n",
       "  'text': 'Creating a model is an iterative process with the following activities: Train a model to learn intents and entities from sample utterances. Test the model interactively or using a testing dataset with known labels Deploy a trained model to a public endpoint so client apps can use it Review predictions and iterate on utterances to train your model By following this iterative approach, you can improve the language model over time based on user input, helping you develop solutions that reflect the way users indicate their intents using natural language.',\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/build-language-understanding-model/media/train-test-publish-review.png',\n",
       "    'image_alt': 'Diagram that shows the train, test, publish, review cycle.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Build a conversational language understanding model',\n",
       "  'topic': 'Exercise - Build an Azure AI services conversational language understanding model',\n",
       "  'text': \"In this exercise, you use Azure AI Language to build a conversational language understanding model. Note To complete this lab, you need an Azure subscription . Launch the exercise and follow the instructions. Tip After completing the exercise, if you've finished exploring Azure AI services, delete the Azure resources that you created during the exercise.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2322136&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Build a conversational language understanding model',\n",
       "  'topic': 'Summary',\n",
       "  'text': \"In this module, you learned how to create a conversational language understanding model. Now that you've completed this module, you can: Provision an Azure AI Language resource Define intents, entities, and utterances Use patterns to differentiate similar utterances Use pre-built entity components Train, test, publish, and review a model To learn more about language understanding, see the Azure AI Language documentation .\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/language-service/'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create custom text classification solutions',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"Natural language processing (NLP) is one of the most common AI problems, where software must interpret text or speech in the natural form that humans use. Part of NLP is the ability to classify text, and Azure provides ways to classify text including sentiment, language, and custom categories defined by the user. In this module, you'll learn how to use the Azure AI Language service to classify text into custom groups. After completing this module, you'll be able to: Understand types of classification projects. Build a custom text classification project. Tag data, train, and deploy a model. Submit classification tasks from your own app.\",\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create custom text classification solutions',\n",
       "  'topic': 'Understand types of classification projects',\n",
       "  'text': 'Custom text classification assigns labels, which in the Azure AI Language service is a class that the developer defines, to text files. For example, a video game summary might be classified as \"Adventure\", \"Strategy\", \"Action\" or \"Sports\". Custom text classification falls into two types of projects: Single label classification - you can assign only one class to each file. Following the above example, a video game summary could only be classified as \"Adventure\" or \"Strategy\". Multiple label classification - you can assign multiple classes to each file. This type of project would allow you to classify a video game summary as \"Adventure\" or \"Adventure and Strategy\". When creating your custom text classification project, you can specify which project you want to build. Single vs. multiple label projects Beyond the ability to put files into multiple classifications, the key differences with multiple label classification projects are labeling, considerations for improving your model, and the API payload for classification tasks. Labeling data In single label projects, each file is assigned one class during the labeling process; class assignment in Azure AI Language only allows you to select one class. When labeling multiple label projects, you can assign as many classes that you want per file. The impact of the added complexity means your data has to remain clear and provide a good distribution of possible inputs for your model to learn from. Labeling data correctly, especially for multiple label projects, is directly correlated with how well your model performs. The higher the quality, clarity, and variation of your data set is, the more accurate your model will be. Evaluating and improving your model Measuring predictive performance of your model goes beyond how many predictions were correct. Correct classifications are when the actual label is x and the model predicts a label x . In the real world, documents result in different kinds of errors when a classification isn\\'t correct: False positive - model predicts x , but the file isn\\'t labeled x . False negative - model doesn\\'t predict label x , but the file in fact is labeled x . These metrics are translated into three measures provided by Azure AI Language: Recall - Of all the actual labels, how many were identified; the ratio of true positives to all that was labeled. Precision - How many of the predicted labels are correct; the ratio of true positives to all identified positives. F1 Score - A function of recall and precision , intended to provide a single score to maximize for a balance of each component Tip Learn more about the Azure AI Language evaluation metrics , including exactly how these metrics are calculated With a single label project, you can identify which classes aren\\'t classified as well as others and find more quality data to use in training your model. For multiple label projects, figuring out quality data becomes more complex due to the matrix of possible permutations of combined labels. For example, let\\'s your model is correctly classifying \"Action\" games and some \"Action and Strategy\" games, but failing at \"Strategy\" games. To improve your model, you\\'ll want to find more high quality and varied summaries for both \"Action and Strategy\" games, as well as \"Strategy\" games to teach your model how to differentiate the two. This challenge increases exponentially with more possible classes your model is classifying into. API payload Azure AI Language provides a REST API to build and interact with your model, using a JSON body to specify the request. This API is abstracted into multiple language-specific SDKs, however for this module we\\'ll focus our examples on the base REST API. To submit a classification task, the API requires the JSON body to specify which task to execute. You\\'ll learn more about the REST API in the next unit, but worth familiarizing yourself with parts of the required body. Single label classification models specify a project type of customSingleLabelClassification : {\\n  \"projectFileVersion\": \"<API-VERSION>\",\\n  \"stringIndexType\": \"Utf16CodeUnit\",\\n  \"metadata\": {\\n    \"projectName\": \"<PROJECT-NAME>\",\\n    \"storageInputContainerName\": \"<CONTAINER-NAME>\",\\n    \"projectKind\": \"customSingleLabelClassification\",\\n    \"description\": \"Trying out custom single label text classification\",\\n    \"language\": \"<LANGUAGE-CODE>\",\\n    \"multilingual\": true,\\n    \"settings\": {}\\n  },\\n  \"assets\": {\\n    \"projectKind\": \"customSingleLabelClassification\",\\n        \"classes\": [\\n            {\\n                \"category\": \"Class1\"\\n            },\\n            {\\n                \"category\": \"Class2\"\\n            }\\n        ],\\n        \"documents\": [\\n            {\\n                \"location\": \"<DOCUMENT-NAME>\",\\n                \"language\": \"<LANGUAGE-CODE>\",\\n                \"dataset\": \"<DATASET>\",\\n                \"class\": {\\n                    \"category\": \"Class2\"\\n                }\\n            },\\n            {\\n                \"location\": \"<DOCUMENT-NAME>\",\\n                \"language\": \"<LANGUAGE-CODE>\",\\n                \"dataset\": \"<DATASET>\",\\n                \"class\": {\\n                    \"category\": \"Class1\"\\n                }\\n            }\\n        ]\\n    }\\n} Multiple label classification models specify a project type of customMultiLabelClassification {\\n  \"projectFileVersion\": \"<API-VERSION>\",\\n  \"stringIndexType\": \"Utf16CodeUnit\",\\n  \"metadata\": {\\n    \"projectName\": \"<PROJECT-NAME>\",\\n    \"storageInputContainerName\": \"<CONTAINER-NAME>\",\\n    \"projectKind\": \"customMultiLabelClassification\",\\n    \"description\": \"Trying out custom multi label text classification\",\\n    \"language\": \"<LANGUAGE-CODE>\",\\n    \"multilingual\": true,\\n    \"settings\": {}\\n  },\\n  \"assets\": {\\n    \"projectKind\": \"customMultiLabelClassification\",\\n    \"classes\": [\\n      {\\n        \"category\": \"Class1\"\\n      },\\n      {\\n        \"category\": \"Class2\"\\n      }\\n    ],\\n    \"documents\": [\\n      {\\n        \"location\": \"<DOCUMENT-NAME>\",\\n        \"language\": \"<LANGUAGE-CODE>\",\\n        \"dataset\": \"<DATASET>\",\\n        \"classes\": [\\n          {\\n            \"category\": \"Class1\"\\n          },\\n          {\\n            \"category\": \"Class2\"\\n          }\\n        ]\\n      },\\n      {\\n        \"location\": \"<DOCUMENT-NAME>\",\\n        \"language\": \"<LANGUAGE-CODE>\",\\n        \"dataset\": \"<DATASET>\",\\n        \"classes\": [\\n          {\\n            \"category\": \"Class2\"\\n          }\\n        ]\\n      }\\n    ]\\n  }\\n}',\n",
       "  'links': ['https://learn.microsoft.com/wwl-data-ai/custom-text-classification/media/single-multiple-graphic.png#lightbox',\n",
       "   'https://aka.ms/language-evaluation-metrics'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/custom-text-classification/media/single-multiple-graphic-small.png',\n",
       "    'image_alt': 'Conceptual diagram that shows mapping of documents to labels for single label and multiple label classifications.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create custom text classification solutions',\n",
       "  'topic': 'Understand how to build text classification projects',\n",
       "  'text': 'Custom text classification projects are your workspace to build, train, improve, and deploy your classification model. You can work with your project in two ways: through Language Studio and via the REST API. Language Studio is the GUI that will be used in the lab, but the REST API has the same functionality. Regardless of which method you prefer, the steps for developing your model are the same. Azure AI Language project life cycle Define labels : Understanding the data you want to classify, identify the possible labels you want to categorize into. In our video game example, our labels would be \"Action\", \"Adventure\", \"Strategy\", and so on. Tag data : Tag, or label, your existing data, specifying the label or labels each file falls under. Labeling data is important since it\\'s how your model will learn how to classify future files. Best practice is to have clear differences between labels to avoid ambiguity, and provide good examples of each label for the model to learn from. For example, we\\'d label the game \"Quest for the Mine Brush\" as \"Adventure\", and \"Flight Trainer\" as \"Action\". Train model : Train your model with the labeled data. Training will teach our model what types of video game summaries should be labeled which genre. View model : After your model is trained, view the results of the model. Your model is scored between 0 and 1, based on the precision and recall of the data tested. Take note of which genre didn\\'t perform well. Improve model : Improve your model by seeing which classifications failed to evaluate to the right label, see your label distribution, and find out what data to add to improve performance. For example, you might find your model mixes up \"Adventure\" and \"Strategy\" games. Try to find more examples of each label to add to your dataset for retraining your model. Deploy model : Once your model performs as desired, deploy your model to make it available via the API. Your model might be named \"GameGenres\", and once deployed can be used to classify game summaries. Classify text : Use your model for classifying text. The lab covers how to use the API, and you can view the API reference How to split datasets for training When labeling your data, you can specify which dataset you want each file to be: Training - The training dataset is used to actually train the model; the data and labels provided are fed into the machine learning algorithm to teach your model what data should be classified to which label. The training dataset will be the larger of the two datasets, recommended to be about 80% of your labeled data. Testing - The testing dataset is labeled data used to verify you model after it\\'s trained. Azure will take the data in the testing dataset, submit it to the model, and compare the output to how you labeled your data to determine how well the model performed. The result of that comparison is how your model gets scored and helps you know how to improve your predictive performance. During the Train model step, there are two options for how to train your model. Automatic split - Azure takes all of your data, splits it into the specified percentages randomly, and applies them in training the model. This option is best when you have a larger dataset, data is naturally more consistent, or the distribution of your data extensively covers your classes. Manual split - Manually specify which files should be in each dataset. When you submit the training job, the Azure AI Language service will tell you the split of the dataset and the distribution. This split is best used with smaller datasets to ensure the correct distribution of classes and variation in data are present to correctly train your model. To use the automatic split, put all files into the training dataset when labeling your data (this option is the default). To use the manual split, specify which files should be in testing versus training during the labeling of your data. Deployment options Azure AI Language allows each project to create both multiple models and multiple deployments, each with their own unique name. Benefits include ability to: Test two models side by side Compare how the split of datasets impact performance Deploy multiple versions of your model Note Each project has a limit of ten deployment names During deployment you can choose the name for the deployed model, which can then be selected when submitting a classification task: <...>\\n  \"tasks\": [\\n    {\\n      \"kind\": \"CustomSingleLabelClassification\",\\n      \"taskName\": \"MyTaskName\",\\n      \"parameters\": {\\n        \"projectName\": \"MyProject\",\\n        \"deploymentName\": \"MyDeployment\"\\n      }\\n    }\\n  ]\\n<...> Using the REST API The REST API available for the Azure AI Language service allows for CLI development of Azure AI Language projects in the same way that Language Studio provides a user interface for building projects. Language Studio is explored further in this module\\'s lab. Pattern of using the API The API for the Azure AI Language service operates asynchronously for most calls. In each step we submit a request to the service first, then check back with the service via a subsequent call to get the status or result. With each request, a header is required to authenticate your request: Key Value Ocp-Apim-Subscription-Key The key to your Azure AI Language resource Submit initial request The URL to submit the request to varies on which step you are on, but all are prefixed with the endpoint provided by your Azure AI Language resource. For example, to train a model, you would create a POST to the URL that would look something like the following: <YOUR-ENDPOINT>/language/analyze-text/projects/<PROJECT-NAME>/:train?api-version=<API-VERSION> Placeholder Value Example <YOUR-ENDPOINT> The endpoint for your API request https://<your-custom-resource>.cognitiveservices.azure.com <PROJECT-NAME> The name for your project (value is case-sensitive) myProject The following body would be attached to the request: {\\n        \"modelLabel\": \"<MODEL-NAME>\",\\n        \"trainingConfigVersion\": \"<CONFIG-VERSION>\",\\n        \"evaluationOptions\": {\\n            \"kind\": \"percentage\",\\n            \"trainingSplitPercentage\": 80,\\n            \"testingSplitPercentage\": 20\\n        }\\n    } Key Value <YOUR-MODEL> Your model name. trainingConfigVersion The model version to use to train your model. runValidation Boolean value to run validation on the test set. evaluationOptions Specifies evaluation options. kind Specifies data split type. Can be percentage if you\\'re using an automatic split, or set if you manually split your dataset testingSplitPercentage Required integer field only if type is percentage . Specifies testing split. trainingSplitPercentage Required integer field only if type is percentage . Specifies training split. The response to the above request will be a 202 , meaning the request was successful. Grab the location value from the response headers, which will look similar to the following URL: <ENDPOINT>/language/analyze-text/projects/<PROJECT-NAME>/train/jobs/<JOB-ID>?api-version=<API-VERSION> Key Value <JOB-ID> Identifier for your request This URL is used in the next step to get the training status. Get training status To get the training status, use the URL from the header of the request response to submit a GET request, with same header that provides our Azure AI Language service key for authentication. The response body will be similar to the following JSON: {\\n  \"result\": {\\n    \"modelLabel\": \"<MODEL-NAME>\",\\n    \"trainingConfigVersion\": \"<CONFIG-VERSION>\",\\n    \"estimatedEndDateTime\": \"2023-05-18T15:47:58.8190649Z\",\\n    \"trainingStatus\": {\\n      \"percentComplete\": 3,\\n      \"startDateTime\": \"2023-05-18T15:45:06.8190649Z\",\\n      \"status\": \"running\"\\n    },\\n    \"evaluationStatus\": {\\n      \"percentComplete\": 0,\\n      \"status\": \"notStarted\"\\n    }\\n  },\\n  \"jobId\": \"<JOB-ID>\",\\n  \"createdDateTime\": \"2023-05-18T15:44:44Z\",\\n  \"lastUpdatedDateTime\": \"2023-05-18T15:45:48Z\",\\n  \"expirationDateTime\": \"2023-05-25T15:44:44Z\",\\n  \"status\": \"running\"\\n} Training a model can take some time, so periodically check back at this status URL until the response status returns succeeded . Once the training has succeeded, you can view, verify, and deploy your model. Consuming a deployed model Using the model to classify text follows the same pattern as outlined above, with a POST request submitting the job and a GET request to retrieve the results. Submit text for classification To use your model, submit a POST to the analyze endpoint at the following URL: <ENDPOINT>/language/analyze-text/jobs?api-version=<API-VERSION> Placeholder Value Example <YOUR-ENDPOINT> The endpoint for your API request https://<your-custom-resource>.cognitiveservices.azure.com Important Remember to include your resource key in the header for Ocp-Apim-Subscription-Key The following JSON structure would be attached to the request: {\\n  \"displayName\": \"Classifying documents\",\\n  \"analysisInput\": {\\n    \"documents\": [\\n      {\\n        \"id\": \"1\",\\n        \"language\": \"<LANGUAGE-CODE>\",\\n        \"text\": \"Text1\"\\n      },\\n      {\\n        \"id\": \"2\",\\n        \"language\": \"<LANGUAGE-CODE>\",\\n        \"text\": \"Text2\"\\n      }\\n    ]\\n  },\\n  \"tasks\": [\\n     {\\n      \"kind\": \"<TASK-REQUIRED>\",\\n      \"taskName\": \"<TASK-NAME>\",\\n      \"parameters\": {\\n        \"projectName\": \"<PROJECT-NAME>\",\\n        \"deploymentName\": \"<DEPLOYMENT-NAME>\"\\n      }\\n    }\\n  ]\\n} Key Value <TASK-REQUIRED> Which task you\\'re requesting. The task is CustomMultiLabelClassification for multiple label projects, or CustomSingleLabelClassification for single label projects <LANGUAGE-CODE> The language code such as en-us . <TASK-NAME> Your task name. <PROJECT-NAME> Your project name. <DEPLOYMENT-NAME> Your deployment name. The response to the above request will be a 202 , meaning the request was successful. Look for the operation-location value in the response headers, which will look something like the following URL: <ENDPOINT>/language/analyze-text/jobs/<JOB-ID>?api-version=<API-VERSION> Key Value <YOUR-ENDPOINT> The endpoint for your API request <JOB-ID> Identifier for your request This URL is used to get your task results. Get classification results Submit a GET request to the endpoint from the previous request, with the same header for authentication. The response body will be similar to the following JSON: {\\n  \"createdDateTime\": \"2023-05-19T14:32:25.578Z\",\\n  \"displayName\": \"MyJobName\",\\n  \"expirationDateTime\": \"2023-05-19T14:32:25.578Z\",\\n  \"jobId\": \"xxxx-xxxxxx-xxxxx-xxxx\",\\n  \"lastUpdateDateTime\": \"2023-05-19T14:32:25.578Z\",\\n  \"status\": \"succeeded\",\\n  \"tasks\": {\\n    \"completed\": 1,\\n    \"failed\": 0,\\n    \"inProgress\": 0,\\n    \"total\": 1,\\n    \"items\": [\\n      {\\n        \"kind\": \"customSingleClassificationTasks\",\\n        \"taskName\": \"Classify documents\",\\n        \"lastUpdateDateTime\": \"2022-10-01T15:01:03Z\",\\n        \"status\": \"succeeded\",\\n        \"results\": {\\n          \"documents\": [\\n            {\\n              \"id\": \"<DOC-ID>\",\\n              \"class\": [\\n                  {\\n                      \"category\": \"Class_1\",\\n                      \"confidenceScore\": 0.0551877357\\n                  }\\n              ],\\n              \"warnings\": []\\n            }\\n          ],\\n          \"errors\": [],\\n          \"modelVersion\": \"2022-04-01\"\\n        }\\n      }\\n    ]\\n  }\\n} The classification result is within the items array\\'s results object, for each document submitted.',\n",
       "  'links': ['https://learn.microsoft.com/wwl-data-ai/custom-text-classification/media/classify-development-lifecycle.png#lightbox',\n",
       "   'https://aka.ms/ct-runtime-swagger'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/custom-text-classification/media/classify-development-lifecycle-small.png',\n",
       "    'image_alt': 'Diagram that shows a life cycle with steps to define labels, tag data, train model, view model, improve model, deploy model, and classify text.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create custom text classification solutions',\n",
       "  'topic': 'Exercise - Classify text',\n",
       "  'text': \"In this exercise, you use Azure AI Language to build a custom text classification model. Note To complete this lab, you need an Azure subscription . Launch the exercise and follow the instructions. Tip After completing the exercise, if you've finished exploring Azure AI services, delete the Azure resources that you created during the exercise.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2322137&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create custom text classification solutions',\n",
       "  'topic': 'Summary',\n",
       "  'text': \"In this module, you learned about custom text classification and how to build a text classification service. Now that you've completed this module, you can: Understand types of classification projects. Build a custom text classification project. Tag data, train, and deploy a model. Submit classification tasks from your own app. To learn more about the Azure AI Language service, see the Azure AI Language service documentation .\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/language-service/overview'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Custom named entity recognition',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"Custom named entity recognition (NER), otherwise known as custom entity extraction, is one of the many features for natural language processing (NLP) offered by Azure AI Language service. Custom NER enables developers to extract predefined entities from text documents, without those documents being in a known format - such as legal agreements or online ads. An entity is a person, place, thing, event, skill, or value. In this module, you'll learn how to use the Azure AI Language service to extract entities from unstructured documents. After completing this module, you'll be able to: Understand custom named entities and how they're labeled. Build a custom named entity extraction project. Label data, train, and deploy an entity extraction model. Submit extraction tasks from your own app.\",\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Custom named entity recognition',\n",
       "  'topic': 'Understand custom named entity recognition',\n",
       "  'text': 'Custom NER is an Azure API service that looks at documents, identifies, and extracts user defined entities. These entities could be anything from names and addresses from bank statements to knowledge mining to improve search results. Custom NER is part of Azure AI Language in Azure AI services. Custom vs built-in NER Azure AI Language provides certain built-in entity recognition, to recognize things such as a person, location, organization, or URL. Built-in NER allows you to set up the service with minimal configuration, and extract entities. To call a built-in NER, create your service and call the endpoint for that NER service like this: <YOUR-ENDPOINT>/language/analyze-text/jobs?api-version=<API-VERSION> Placeholder Value Example <YOUR-ENDPOINT> The endpoint for your API request https://<your-resource>.cognitiveservices.azure.com <API-VERSION> The version of the API you are calling 2023-05-01 The body of that call will contain the document(s) the entities are extracted from, and the headers contain your service key. The response from the call above contains an array of entities recognized, such as: <...>\\n\"entities\":[\\n    {\\n        \"text\":\"Seattle\",\\n        \"category\":\"Location\",\\n        \"subcategory\":\"GPE\",\\n        \"offset\":45,\\n        \"length\":7,\\n        \"confidenceScore\":0.99\\n    },\\n    {\\n        \"text\":\"next week\",\\n        \"category\":\"DateTime\",\\n        \"subcategory\":\"DateRange\",\\n        \"offset\":104,\\n        \"length\":9,\\n        \"confidenceScore\":0.8\\n    }\\n]\\n<...> Examples of when to use the built-in NER include finding locations, names, or URLs in long text documents. Tip A full list of recognized entity categories is available in the NER docs . Custom NER, which is the focus of the rest of this module, is available when the entities you want to extract aren\\'t part of the built-in service or you only want to extract specific entities. You can make your custom NER model as simple or complex as is required for your app. Examples of when you\\'d want custom NER include specific legal or bank data, knowledge mining to enhance catalog search, or looking for specific text for audit policies. Each one of these projects requires a specific set of entities and data it needs to extract. Azure AI Language project life cycle Creating an entity extraction model typically follows a similar path to most Azure AI Language service features: Define entities : Understanding the data and entities you want to identify, and try to make them as clear as possible. For example, defining exactly which parts of a bank statement you want to extract. Tag data : Label, or tag, your existing data, specifying what text in your dataset corresponds to which entity. This step is important to do accurately and completely, as any wrong or missed labels will reduce the effectiveness of the trained model. A good variation of possible input documents is useful. For example, label bank name, customer name, customer address, specific loan or account terms, loan or account amount, and account number. Train model : Train your model once your entities are labeled. Training teaches your model how to recognize the entities you label. View model : After your model is trained, view the results of the model. This page includes a score of 0 to 1 that is based on the precision and recall of the data tested. You can see which entities worked well (such as customer name) and which entities need improvement (such as account number). Improve model : Improve your model by seeing which entities failed to be identified, and which entities were incorrectly extracted. Find out what data needs to be added to your model\\'s training to improve performance. This page shows you how entities failed, and which entities (such as account number) need to be differentiated from other similar entities (such as loan amount). Deploy model : Once your model performs as desired, deploy your model to make it available via the API. In our example, you can send to requests to the model when it\\'s deployed to extract bank statement entities. Extract entities : Use your model for extracting entities. The lab covers how to use the API, and you can view the API reference for more details. Considerations for data selection and refining entities For the best performance, you\\'ll need to use both high quality data to train the model and clearly defined entity types. High quality data will let you spend less time refining and yield better results from your model. Diversity - use as diverse of a dataset as possible without losing the real-life distribution expected in the real data. You\\'ll want to use sample data from as many sources as possible, each with their own formats and number of entities. It\\'s best to have your dataset represent as many different sources as possible. Distribution - use the appropriate distribution of document types. A more diverse dataset to train your model will help your model avoid learning incorrect relationships in the data. Accuracy - use data that is as close to real world data as possible. Fake data works to start the training process, but it likely will differ from real data in ways that can cause your model to not extract correctly. Entities need to also be carefully considered, and defined as distinctly as possible. Avoid ambiguous entities (such as two names next to each other on a bank statement), as it will make the model struggle to differentiate. If having some ambiguous entities is required, make sure to have more examples for your model to learn from so it can understand the difference. Keeping your entities distinct will also go a long way in helping your model\\'s performance. For example, trying to extract something like \"Contact info\" that could be a phone number, social media handle, or email address would require several examples to correctly teach your model. Instead, try to break them down into more specific entities such as \"Phone\", \"Email\", and \"Social media\" and let the model label whichever type of contact information it finds. How to extract entities To submit an extraction task, the API requires the JSON body to specify which task to execute. For custom NER, the task for the JSON payload is CustomEntityRecognition . Your payload will look similar to the following JSON: {\\n    \"displayName\": \"string\",\\n    \"analysisInput\": {\\n        \"documents\": [\\n            {\\n                \"id\": \"doc1\", \\n                \"text\": \"string\"\\n            },\\n            {\\n                \"id\": \"doc2\",\\n                \"text\": \"string\"\\n            }\\n        ]\\n    },\\n    \"tasks\": [\\n        {\\n            \"kind\": \"CustomEntityRecognition\",\\n            \"taskName\": \"MyRecognitionTaskName\",\\n            \"parameters\": {\\n            \"projectName\": \"MyProject\",\\n            \"deploymentName\": \"MyDeployment\"\\n            }\\n        }\\n    ]\\n} Project limits The Azure AI Language service enforces the following restrictions: Training - at least 10 files, and not more than 100,000 Deployments - 10 deployment names per project APIs Authoring - this API creates a project, trains, and deploys your model. Limited to 10 POST and 100 GET per minute Analyze - this API does the work of actually extracting the entities; it requests a task and retrieves the results. Limited to 20 GET or POST Projects - only 1 storage account per project, 500 projects per resource, and 50 trained models per project Entities - each entity can be up to 500 characters. You can have up to 200 entity types. See the Service limits for Azure AI Language page for detailed information.',\n",
       "  'links': ['https://aka.ms/ner-categories',\n",
       "   'https://learn.microsoft.com/wwl-data-ai/custom-name-entity-recognition/media/extraction-development-lifecycle.png#lightbox',\n",
       "   'https://aka.ms/ct-runtime-swagger',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/language-service/concepts/data-limits'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/custom-name-entity-recognition/media/extraction-development-lifecycle.png',\n",
       "    'image_alt': 'Conceptual diagram showing a project steps to define entities, tag data, train model, view model, improve model, deploy model, and extract entities.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Custom named entity recognition',\n",
       "  'topic': 'Label your data',\n",
       "  'text': 'Labeling, or tagging, your data correctly is an important part of the process to create a custom entity extraction model. Labels identify examples of specific entities in text used to train the model. Three things to focus on are: Consistency - Label your data the same way across all files for training. Consistency allows your model to learn without any conflicting inputs. Precision - Label your entities consistently, without unnecessary extra words. Precision ensures only the correct data is included in your extracted entity. Completeness - Label your data completely, and don\\'t miss any entities. Completeness helps your model always recognize the entities present. How to label your data Language Studio is the most straight forward method for labeling your data. Language Studio allows you to see the file, select the beginning and end of your entity, and specify which entity it is. Each label that you identify gets saved into a file that lives in your storage account with your dataset, in an auto-generated JSON file. This file then gets used by the model to learn how to extract custom entities. It\\'s possible to provide this file when creating your project (if you\\'re importing the same labels from a different project, for example) however it must be in the Accepted custom NER data formats .\\nFor example: {\\n  \"projectFileVersion\": \"{DATE}\",\\n  \"stringIndexType\": \"Utf16CodeUnit\",\\n  \"metadata\": {\\n    \"projectKind\": \"CustomEntityRecognition\",\\n    \"storageInputContainerName\": \"{CONTAINER-NAME}\",\\n    \"projectName\": \"{PROJECT-NAME}\",\\n    \"multilingual\": false,\\n    \"description\": \"Project-description\",\\n    \"language\": \"en-us\",\\n    \"settings\": {}\\n  },\\n  \"assets\": {\\n    \"projectKind\": \"CustomEntityRecognition\",\\n    \"entities\": [\\n      {\\n        \"category\": \"Entity1\"\\n      },\\n      {\\n        \"category\": \"Entity2\"\\n      }\\n    ],\\n    \"documents\": [\\n      {\\n        \"location\": \"{DOCUMENT-NAME}\",\\n        \"language\": \"{LANGUAGE-CODE}\",\\n        \"dataset\": \"{DATASET}\",\\n        \"entities\": [\\n          {\\n            \"regionOffset\": 0,\\n            \"regionLength\": 500,\\n            \"labels\": [\\n              {\\n                \"category\": \"Entity1\",\\n                \"offset\": 25,\\n                \"length\": 10\\n              },\\n              {\\n                \"category\": \"Entity2\",\\n                \"offset\": 120,\\n                \"length\": 8\\n              }\\n            ]\\n          }\\n        ]\\n      },\\n      {\\n        \"location\": \"{DOCUMENT-NAME}\",\\n        \"language\": \"{LANGUAGE-CODE}\",\\n        \"dataset\": \"{DATASET}\",\\n        \"entities\": [\\n          {\\n            \"regionOffset\": 0,\\n            \"regionLength\": 100,\\n            \"labels\": [\\n              {\\n                \"category\": \"Entity2\",\\n                \"offset\": 20,\\n                \"length\": 5\\n              }\\n            ]\\n          }\\n        ]\\n      }\\n    ]\\n  }\\n} Field Description documents Array of labeled documents location Path to file within container connected to the project language Language of the file entities Array of present entities in the current document regionOffset Inclusive character position for start of text regionLength Length in characters of the data used in training category Name of entity to extract labels Array of labeled entities in the files offset Inclusive character position for start of entity length Length in characters of the entity dataset Which dataset the file is assigned to',\n",
       "  'links': ['https://learn.microsoft.com/wwl-data-ai/custom-name-entity-recognition/media/tag-entity-screenshot.png#lightbox',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/language-service/custom-named-entity-recognition/concepts/data-formats'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/custom-name-entity-recognition/media/tag-entity-screenshot.png',\n",
       "    'image_alt': 'Screenshot of labeling an entity in Language Studio.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Custom named entity recognition',\n",
       "  'topic': 'Train and evaluate your model',\n",
       "  'text': \"Training and evaluating your model is an iterative process of adding data and labels to your training dataset to teach the model more accurately. To know what types of data and labels need to be improved, Language Studio provides scoring in the View model details page on the left hand pane. Individual entities and your overall model score are broken down into three metrics to explain how they're performing and where they need to improve. Metric Description Precision The ratio of successful entity recognitions to all attempted recognitions. A high score means that as long as the entity is recognized, it's labeled correctly. Recall The ratio of successful entity recognitions to the actual number of entities in the document. A high score means it finds the entity or entities well, regardless of if it assigns them the right label F1 score Combination of precision and recall providing a single scoring metric Scores are available both per entity and for the model as a whole. You may find an entity scores well, but the whole model doesn't. How to interpret metrics Ideally we want our model to score well in both precision and recall, which means the entity recognition works well. If both metrics have a low score, it means the model is both struggling to recognize entities in the document, and when it does extract that entity, it doesn't assign it the correct label with high confidence. If precision is low but recall is high, it means that the model recognizes the entity well but doesn't label it as the correct entity type. If precision is high but recall is low, it means that the model doesn't always recognize the entity, but when the model extracts the entity, the correct label is applied. Confusion matrix On the same View model details page, there's another tab on the top for the Confusion matrix . This view provides a visual table of all the entities and how each performed, giving a complete view of the model and where it's falling short. The confusion matrix allows you to visually identify where to add data to improve your model's performance.\",\n",
       "  'links': ['https://learn.microsoft.com/wwl-data-ai/custom-name-entity-recognition/media/model-scoring-new.png#lightbox',\n",
       "   'https://learn.microsoft.com/wwl-data-ai/custom-name-entity-recognition/media/model-confusion-matrix-new.png#lightbox'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/custom-name-entity-recognition/media/model-scoring-new.png',\n",
       "    'image_alt': 'Screenshot of the View model scoring tab.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/custom-name-entity-recognition/media/model-confusion-matrix-new.png',\n",
       "    'image_alt': 'Screenshot of a sample confusion matrix.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Custom named entity recognition',\n",
       "  'topic': 'Exercise - Extract custom entities',\n",
       "  'text': \"In this exercise, you use Azure AI Language to build a custom named entity recognition model. Note To complete this lab, you need an Azure subscription . Launch the exercise and follow the instructions. Tip After completing the exercise, if you've finished exploring Azure AI services, delete the Azure resources that you created during the exercise.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://microsoftlearning.github.io/mslearn-ai-language/Instructions/Labs/05-extract-custom-entities.html?azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Custom named entity recognition',\n",
       "  'topic': 'Summary',\n",
       "  'text': \"In this module, you learned about custom named entity recognition and how to extract entities. In this module, you learned how to: Understand custom named entities and how they're labeled. Build a Language service project. Label data, train, and deploy an entity extraction model. Submit extraction tasks from your own app. To learn more about Azure AI Language, see the Azure AI Language documentation .\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/language-service/?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Translate text with Azure AI Translator service',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"There are many commonly used languages throughout the world, and the ability to exchange information between speakers of different languages is often a critical requirement for global solutions. The Azure AI Translator provides an API for translating text between 90 supported languages. After completing this module, you'll be able to: Provision an Azure AI Translator resource Understand language detection, translation, and transliteration Specify translation options Define and run custom translations\",\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Translate text with Azure AI Translator service',\n",
       "  'topic': 'Provision an Azure AI Translator resource',\n",
       "  'text': \"Azure AI Translator provides a multilingual text translation API that you can use for: Language detection. One-to-many translation. Script transliteration (converting text from its native script to an alternative script). Azure resource for Azure AI Translator To use the Azure AI Translator service, you must provision a resource for it in your Azure subscription. You can provision a single-service Azure AI Translator resource, or you can use the Text Translation API in a multi-service Azure AI Services resource. After you provision a suitable resource in your Azure subscription, you can use the location where you deployed the resource and one of its subscription keys to call the Azure AI Translator APIs from your code. You can call the APIs by submitting requests in JSON format to the REST interface, or by using any of the available programming language-specific SDKs. Note The code examples in the subsequent units in this module show the JSON requests and responses exchanged with the REST interface. When using an SDK, the JSON requests are abstracted by appropriate objects and methods that encapsulate the same data values. You'll get a chance to try the SDK for C# or Python for yourself in the exercise later in the module.\",\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/translate-text-with-translator-service/media/translator-resource.png',\n",
       "    'image_alt': 'Diagram showing an Azure AI Translator resource performing language detection, one-to-many translation, and transliteration.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Translate text with Azure AI Translator service',\n",
       "  'topic': 'Understand language detection, translation, and transliteration',\n",
       "  'text': 'Let\\'s explore the capabilities of Azure AI Translator . These capabilities include: Language detection You can use the Detect function of the REST API to detect the language in which text is written. For example, you could submit the following text to the https://api.cognitive.microsofttranslator.com/detect?api-version=3.0 endpoint using curl. Here\\'s the text we want to translate: { \\'Text\\' : \\'こんにちは\\' } Here\\'s a call using curl to the endpoint to detect the language of our text: curl -X POST \"https://api.cognitive.microsofttranslator.com/detect?api-version=3.0\" -H \"Ocp-Apim-Subscription-Region: <your-service-region>\" -H \"Ocp-Apim-Subscription-Key: <your-key>\" -H \"Content-Type: application/json\" -d \"[{ \\'Text\\' : \\'こんにちは\\' }] The response to this request looks as follows, indicating that the text is written in Japanese: [\\n  {\\n    \"language\": \"ja\",\\n    \"score\": 1.0,\\n    \"isTranslationSupported\": true,\\n    \"isTransliterationSupported\": true\\n    \\n    \\n   }\\n] Translation To translate text from one language to another, use the Translate function; specifying a single from parameter to indicate the source language, and one or more to parameters to specify the languages into which you want the text translated. For example, you could submit the same JSON we previously used to detect the language, specifying a from parameter of ja (Japanese) and two to parameters with the values en (English) and fr (French). To do this, you\\'d call: curl -X POST \"https://api.cognitive.microsofttranslator.com/translate?api-version=3.0&from=ja&to=fr&to=en\" -H \"Ocp-Apim-Subscription-Key: <your-key>\" -H \"Ocp-Apim-Subscription-Region: <your-service-region>\" -H \"Content-Type: application/json; charset=UTF-8\" -d \"[{ \\'Text\\' : \\'こんにちは\\' }]\" This would produce the following result: [\\n  {\"translations\": \\n    [\\n      {\"text\": \"Hello\", \"to\": \"en\"},   \\n      {\"text\": \"Bonjour\", \"to\": \"fr\"}\\n    ]\\n  }\\n] Transliteration Our Japanese text is written using Hiragana script, so rather than translate it to a different language, you may want to transliterate it to a different script - for example to render the text in Latin script (as used by English language text). To accomplish this, we can submit the Japanese text to the Transliterate function with a fromScript parameter of Jpan and a toScript parameter of Latn : curl -X POST \"https://api.cognitive.microsofttranslator.com/transliterate?api-version=3.0&fromScript=Jpan&toScript=Latn\" -H \"Ocp-Apim-Subscription-Key: <your-key>\" -H \"Ocp-Apim-Subscription-Region: <your-service-region>\" -H \"Content-Type: application/json\" -d \"[{ \\'Text\\' : \\'こんにちは\\' }]\" The response would give you the following result: [\\n    {\\n        \"script\": \"Latn\",\\n        \"text\": \"Kon\\'nichiwa\"\\n    }\\n]',\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Translate text with Azure AI Translator service',\n",
       "  'topic': 'Specify translation options',\n",
       "  'text': 'The Translate function of the API supports numerous parameters that affect the output. Word alignment In written English (using Latin script), spaces are used to separate words. However, in some other languages (and more specifically, scripts) this is not always the case. For example, translating \"Smart Services\" from en (English) to zh (Simplified Chinese) produces the result \"智能服务\", and it\\'s difficult to understand the relationship between the characters in the source text and the corresponding characters in the translation. To resolve this problem, you can specify the includeAlignment parameter with a value of true in your call to produce the following result: [\\n   {\\n      \"translations\":[\\n         {\\n            \"text\":\"智能服务\",\\n            \"to\":\"zh-Hans\",\\n            \"alignment\":{\\n               \"proj\":\"0:4-0:1 6:13-2:3\"\\n            }\\n         }\\n      ]\\n   }\\n] These results tell us that characters 0 to 4 in the source correspond to characters 0 to 1 in the translation, while characters 6 to 13 in the source correspond to characters 2 to 3 in the translation. Sentence length Sometimes it might be useful to know the length of a translation, for example to determine how best to display it in a user interface. You can get this information by setting the includeSentenceLength parameter to true . For example, specifying this parameter when translating the English ( en ) text \"Hello world\" to French ( fr ) produces the following results: [\\n   {\\n      \"translations\":[\\n         {\\n            \"text\":\"Salut tout le monde\",\\n            \"to\":\"fr\",\\n            \"sentLen\":{\"srcSentLen\":[12],\"transSentLen\":[20]}\\n         }\\n      ]\\n   }\\n] Profanity filtering Sometimes text contains profanities, which you might want to obscure or omit altogether in a translation. You can handle profanities by specifying the profanityAction parameter, which can have one of the following values: NoAction : Profanities are translated along with the rest of the text. Deleted : Profanities are omitted in the translation. Marked : Profanities are indicated using the technique indicated in the profanityMarker parameter (if supplied). The default value for this parameter is Asterisk , which replaces characters in profanities with \"*\". As an alternative, you can specify a profanityMarker value of Tag , which causes profanities to be enclosed in XML tags. For example, translating the English ( en ) text \"JSON is ▇▇▇▇ great!\" (where the blocked out word is a profanity) to German ( de ) with a profanityAction of Marked and a profanityMarker of Asterisk produces the following result: [\\n   {\\n      \"translations\":[\\n         {\\n            \"text\":\"JSON ist *** erstaunlich.\",\\n            \"to\":\"de\"\\n         }\\n      ]\\n   }\\n] Note To learn more about the translation options, including some not described here, see the Azure AI Translator API documentation .',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/translator/reference/v3-0-translate'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Translate text with Azure AI Translator service',\n",
       "  'topic': 'Define custom translations',\n",
       "  'text': 'While the default translation model used by Azure AI Translator is effective for general translation, you may need to develop a translation solution for businesses or industries in that have specific vocabularies of terms that require custom translation. To solve this problem, you can create a custom model that maps your own sets of source and target terms for translation. To create a custom model, use the Custom Translator portal to: Create a workspace linked to your Azure AI Translator resource. Create a project . Upload training data files and train a model . Test your model and publish your model . Make translation calls to the API. Your custom model is assigned a unique category Id (highlighted in the screenshot), which you can specify in translate calls to your Azure AI Translator resource by using the category parameter, causing translation to be performed by your custom model instead of the default model. How to call the API To initiate a translation, you send a POST request to the following request URL: https://api.cognitive.microsofttranslator.com/translate?api-version=3.0 Your request needs to include a couple of parameters: api-version : The required version of the API. to : The target language to translate to. For example: to=fr for French. category : Your category Id . Your request must also include a number of required headers: Ocp-Apim-Subscription-Key . Header for your client key.  For example: Ocp-Apim-Subscription-Key=<your-client-key> . Content-Type . The content type of the payload. The required format is: Content-Type: application/json; charset=UTF-8 . The request body should contain an array that includes a JSON object with a Text property that specifies the text that you want to translate: [\\n    {\"Text\":\"Where can I find my employee details?\"}\\n] There are different ways you can send your request to the API, including using the C#, Python, and curl. For instance, to make a quick call, you can send a POST request using curl: curl -X POST \"https://api.cognitive.microsofttranslator.com/translate?api-version=3.0&from=en&to=nl&category=<category-id>\" -H \"Ocp-Apim-Subscription-Key: <your-key\" -H \"Content-Type: application/json; charset=UTF-8\" -d \"[{\\'Text\\':\\'Where can I find my employee details?\\'}]\" The request above makes a call to translate a sentence from English to Dutch. Response returned The response returns a response code of 200 if the request was successful. It also returns a response body that contains the translated text, like this: [\\n    {\\n        \"translations\":[\\n            {\"text\":\"Waar vind ik mijn personeelsgegevens?\",\"to\":\"nl\"}\\n        ]\\n    }\\n] If the request wasn\\'t successful, then a number of different status codes may be returned depending on the error type, such as 400 (missing or invalid query parameters). See Response status codes for a full list of codes and their explanation. Note For more information about custom translation, see Quickstart: Build, publish, and translate with custom models .',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/translator/custom-translator/quickstart',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/translator/custom-translator/quickstart',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/translator/custom-translator/quickstart',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/translator/custom-translator/quickstart',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/translator/custom-translator/quickstart',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/translator/custom-translator/quickstart',\n",
       "   'https://learn.microsoft.com/wwl-data-ai/translate-text-with-translator-service/media/custom-translator-new.png#lightbox',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/translator/reference/v3-0-translate?tabs=curl',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/translator/custom-translator/quickstart'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/translate-text-with-translator-service/media/custom-translator-new-small.png',\n",
       "    'image_alt': 'Screenshot showing the Custom Translator portal.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Translate text with Azure AI Translator service',\n",
       "  'topic': 'Exercise - Translate text with the Azure AI Translator service',\n",
       "  'text': \"In this exercise, you build an app that translates text between languages. Note To complete this lab, you need an Azure subscription . Launch the exercise and follow the instructions. Tip After completing the exercise, if you've finished exploring Azure AI services, delete the Azure resources that you created during the exercise.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2322068&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Translate text with Azure AI Translator service',\n",
       "  'topic': 'Summary',\n",
       "  'text': 'In this module, you learned how to: Provision an Azure AI Translator resource Understand language detection, translation, and transliteration Specify translation options Define custom translations To learn more about Azure AI Translator, see the Azure AI Translator documentation .',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/translator/translator-overview'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create speech-enabled apps with Azure AI services',\n",
       "  'topic': 'Introduction',\n",
       "  'text': 'Azure AI Speech provides APIs that you can use to build speech-enabled applications. This includes: Speech to text : An API that enables speech recognition in which your application can accept spoken input. Text to speech : An API that enables speech synthesis in which your application can provide spoken output. Speech Translation : An API that you can use to translate spoken input into multiple languages. Keyword Recognition : An API that enables your application to recognize keywords or short phrases. Intent Recognition : An API that uses conversational language understanding to determine the semantic meaning of spoken input. This module focuses on speech recognition and speech synthesis, which are core capabilities of any speech-enabled application. Note The code examples in this module are provided in Python, but you can use any of the available Azure AI Speech SDK packages to develop speech-enabled applications in your preferred language. Available SDK packages include: azure-cognitiveservices-speech for Python Microsoft.CognitiveServices.Speech for Microsoft .NET microsoft-cognitiveservices-speech-sdk for JavaScript Microsoft Cognitive Services Speech SDK For Java',\n",
       "  'links': ['https://pypi.org/project/azure-cognitiveservices-speech?azure-portal=true',\n",
       "   'https://www.nuget.org/packages/Microsoft.CognitiveServices.Speech?azure-portal=true',\n",
       "   'https://www.npmjs.com/package/microsoft-cognitiveservices-speech-sdk?azure-portal=true',\n",
       "   'https://mvnrepository.com/artifact/com.microsoft.cognitiveservices.speech/client-sdk?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create speech-enabled apps with Azure AI services',\n",
       "  'topic': 'Provision an Azure resource for speech',\n",
       "  'text': \"Before you can use Azure AI Speech, you need to create an Azure AI Speech resource in your Azure subscription. You can use either a dedicated Azure AI Speech resource or a multi-service Azure AI Services or Azure AI Foundry resource. After you create your resource, you'll need the following information to use it from a client application through one of the supported SDKs: The location in which the resource is deployed (for example, eastus ) One of the keys assigned to your resource. You can view of these values on the Keys and Endpoint page for your resource in the Azure portal. While the specific syntax and parameters can vary between language-specific SDKs, most interactions with the Azure AI Speech service start with the creation of a SpeechConfig object that encapsulates the connection to your Azure AI Speech resource. For example, the following Python code instantiates a SpeechConfig object based on an Azure AI Speech resource in the East US region: import azure.cognitiveservices.speech as speech_sdk\\n\\nspeech_config = speech_sdk.SpeechConfig(your_project_key, 'eastus') Note This example assumes that the Speech SDK package for python has been installed, like this: pip install azure-cognitiveservices-speech\",\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create speech-enabled apps with Azure AI services',\n",
       "  'topic': 'Use the Azure AI Speech to Text API',\n",
       "  'text': \"The Azure AI Speech service supports speech recognition through the following features: Real-time transcription : Instant transcription with intermediate results for live audio inputs. Fast transcription : Fastest synchronous output for situations with predictable latency. Batch transcription : Efficient processing for large volumes of prerecorded audio. Custom speech : Models with enhanced accuracy for specific domains and conditions. Using the Azure AI Speech SDK While the specific details vary, depending on the SDK being used (Python, C#, and so on); there's a consistent pattern for using the Speech to text API: Use a SpeechConfig object to encapsulate the information required to connect to your Azure AI Speech resource. Specifically, its location and key . Optionally, use an AudioConfig to define the input source for the audio to be transcribed. By default, this is the default system microphone, but you can also specify an audio file. Use the SpeechConfig and AudioConfig to create a SpeechRecognizer object. This object is a proxy client for the Speech to text API. Use the methods of the SpeechRecognizer object to call the underlying API functions. For example, the RecognizeOnceAsync() method uses the Azure AI Speech service to asynchronously transcribe a single spoken utterance. Process the response from the Azure AI Speech service. In the case of the RecognizeOnceAsync() method, the result is a SpeechRecognitionResult object that includes the following properties: Duration OffsetInTicks Properties Reason ResultId Text If the operation was successful, the Reason property has the enumerated value RecognizedSpeech , and the Text property contains the transcription. Other possible values for Result include NoMatch (indicating that the audio was successfully parsed but no speech was recognized) or Canceled , indicating that an error occurred (in which case, you can check the Properties collection for the CancellationReason property to determine what went wrong).\",\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/create-speech-enabled-apps/media/speech-to-text.png',\n",
       "    'image_alt': 'A diagram showing how a SpeechRecognizer object is created from a SpeechConfig and AudioConfig, and its RecognizeOnceAsync method is used to call the Speech API.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create speech-enabled apps with Azure AI services',\n",
       "  'topic': 'Use the text to speech API',\n",
       "  'text': 'Similarly to its Speech to text APIs, the Azure AI Speech service offers other REST APIs for speech synthesis: The Text to speech API, which is the primary way to perform speech synthesis. The Batch synthesis API, which is designed to support batch operations that convert large volumes of text to audio - for example to generate an audio-book from the source text. You can learn more about the REST APIs in the Text to speech REST API documentation . In practice, most interactive speech-enabled applications use the Azure AI Speech service through a (programming) language-specific SDK. Using the Azure AI Speech SDK As with speech recognition, in practice most interactive speech-enabled applications are built using the Azure AI Speech SDK. The pattern for implementing speech synthesis is similar to that of speech recognition: Use a SpeechConfig object to encapsulate the information required to connect to your Azure AI Speech resource. Specifically, its location and key . Optionally, use an AudioConfig to define the output device for the speech to be synthesized. By default, this is the default system speaker, but you can also specify an audio file, or by explicitly setting this value to a null value, you can process the audio stream object that is returned directly. Use the SpeechConfig and AudioConfig to create a SpeechSynthesizer object. This object is a proxy client for the Text to speech API. Use the methods of the SpeechSynthesizer object to call the underlying API functions. For example, the SpeakTextAsync() method uses the Azure AI Speech service to convert text to spoken audio. Process the response from the Azure AI Speech service. In the case of the SpeakTextAsync method, the result is a SpeechSynthesisResult object that contains the following properties: AudioData Properties Reason ResultId When speech has been successfully synthesized, the Reason property is set to the SynthesizingAudioCompleted enumeration and the AudioData property contains the audio stream (which, depending on the AudioConfig may have been automatically sent to a speaker or file).',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-synthesis'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/create-speech-enabled-apps/media/text-to-speech.png',\n",
       "    'image_alt': 'A diagram showing how a SpeechSynthesizer object is created from a SpeechConfig and AudioConfig, and its SpeakTextAsync method is used to call the Speech API.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create speech-enabled apps with Azure AI services',\n",
       "  'topic': 'Configure audio format and voices',\n",
       "  'text': 'When synthesizing speech, you can use a SpeechConfig object to customize the audio that is returned by the Azure AI Speech service. Audio format The Azure AI Speech service supports multiple output formats for the audio stream that is generated by speech synthesis. Depending on your specific needs, you can choose a format based on the required: Audio file type Sample-rate Bit-depth For example, the following Python code sets the speech output format for a previously defined SpeechConfig object named speech_config : speech_config.set_speech_synthesis_output_format(SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm) For a full list of supported formats and their enumeration values, see the Azure AI Speech SDK documentation . Voices The Azure AI Speech service provides multiple voices that you can use to personalize your speech-enabled applications. Voices are identified by names that indicate a locale and a person\\'s name - for example en-GB-George . The following Python example code sets the voice to be used speech_config.speech_synthesis_voice_name = \"en-GB-George\" For information about voices, see the Azure AI Speech SDK documentation .',\n",
       "  'links': ['https://learn.microsoft.com/en-us/python/api/azure-cognitiveservices-speech/azure.cognitiveservices.speech.speechsynthesisoutputformat',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=tts'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create speech-enabled apps with Azure AI services',\n",
       "  'topic': 'Use Speech Synthesis Markup Language',\n",
       "  'text': 'While the Azure AI Speech SDK enables you to submit plain text to be synthesized into speech, the service also supports an XML-based syntax for describing characteristics of the speech you want to generate. This Speech Synthesis Markup Language (SSML) syntax offers greater control over how the spoken output sounds, enabling you to: Specify a speaking style, such as \"excited\" or \"cheerful\" when using a neural voice. Insert pauses or silence. Specify phonemes (phonetic pronunciations), for example to pronounce the text \"SQL\" as \"sequel\". Adjust the prosody of the voice (affecting the pitch, timbre, and speaking rate). Use common \"say-as\" rules, for example to specify that a given string should be expressed as a date, time, telephone number, or other form. Insert recorded speech or audio, for example to include a standard recorded message or simulate background noise. For example, consider the following SSML: <speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" \\n                     xmlns:mstts=\"https://www.w3.org/2001/mstts\" xml:lang=\"en-US\"> \\n    <voice name=\"en-US-AriaNeural\"> \\n        <mstts:express-as style=\"cheerful\"> \\n          I say tomato \\n        </mstts:express-as> \\n    </voice> \\n    <voice name=\"en-US-GuyNeural\"> \\n        I say <phoneme alphabet=\"sapi\" ph=\"t ao m ae t ow\"> tomato </phoneme>. \\n        <break strength=\"weak\"/>Lets call the whole thing off! \\n    </voice> \\n</speak> This SSML specifies a spoken dialog between two different neural voices, like this: Ariana ( cheerfully ): \"I say tomato: Guy : \"I say tomato (pronounced tom-ah-toe ) ... Let\\'s call the whole thing off!\" To submit an SSML description to the Speech service, you can use an appropriate method of a SpeechSynthesizer object, like this: speech_synthesizer.speak_ssml(\\'<speak>...\\'); For more information about SSML, see the Azure AI Speech SDK documentation .',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/speech-service/speech-synthesis-markup'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create speech-enabled apps with Azure AI services',\n",
       "  'topic': 'Exercise - Create a speech-enabled app',\n",
       "  'text': \"In this exercise, build a speech enabled app for both speech recognition and synthesis. Note To complete this lab, you need an Azure subscription . Launch the exercise and follow the instructions. Tip After completing the exercise, if you've finished exploring Azure AI services, delete the Azure resources that you created during the exercise.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2322214&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Create speech-enabled apps with Azure AI services',\n",
       "  'topic': 'Summary',\n",
       "  'text': 'In this module, you learned how to: Provision an Azure resource for the Azure AI Speech service Use the Speech to text API to implement speech recognition Use the Text to speech API to implement speech synthesis Configure audio format and voices Use Speech Synthesis Markup Language (SSML) To learn more about the Azure AI Speech, refer to the Azure AI Speech service documentation .',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/speech-service/'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Translate speech with the Azure AI Speech service',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"Translation of speech builds on speech recognition by recognizing and transcribing spoken input in a specified language, and returning translations of the transcription in one or more other languages. In this module, you'll learn how to: Provision Azure resources for speech translation. Generate text translation from speech. Synthesize spoken translations. The units in the module include important conceptual information about Azure AI Speech  and how to use its API through one of the supported software development kits (SDKs), after which you're able to try Azure AI Speech for yourself in a hands-on exercise. To complete the hands-on exercise, you'll need a Microsoft Azure subscription. If you don't already have one, you can sign up for a free trial at https://azure.com/free\",\n",
       "  'links': ['https://azure.com/free'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Translate speech with the Azure AI Speech service',\n",
       "  'topic': 'Provision an Azure resource for speech translation',\n",
       "  'text': \"The Azure AI Speech service provides robust, machine learning and artificial intelligence-based speech translation services, enabling developers to add end-to-end, real-time, speech translations to their applications or services. You can use either a dedicated Azure AI Speech resource or a multi-service Azure AI Services resource. Before you can use the service, you need to create an Azure AI Speech resource in your Azure subscription. After creating your Azure resource, you'll need the following information to use it from a client application through one of the supported SDKs: The location in which the resource is deployed (for example, eastus) One of the keys assigned to your resource. You can view of these values on the Keys and Endpoint page for your resource in the Azure portal.\",\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Translate speech with the Azure AI Speech service',\n",
       "  'topic': 'Translate speech to text',\n",
       "  'text': 'The pattern for speech translation using the Azure AI Speech SDK is similar to speech recognition, with the addition of information about the source and target languages for translation: Use a SpeechTranslationConfig object to encapsulate the information required to connect to your Azure AI Speech resource. Specifically, its location and key. The SpeechTranslationConfig object is also used to specify the speech recognition language (the language in which the input speech is spoken) and the target languages into which it should be translated. Optionally, use an AudioConfig to define the input source for the audio to be transcribed. By default, this is the default system microphone, but you can also specify an audio file. Use the SpeechTranslationConfig , and AudioConfig to create a TranslationRecognizer object. This object is a proxy client for the Azure AI Speech translation API. Use the methods of the TranslationRecognizer object to call the underlying API functions. For example, the RecognizeOnceAsync () method uses the Azure AI Speech service to asynchronously translate a single spoken utterance. Process the response from Azure AI Speech. In the case of the RecognizeOnceAsync () method, the result is a SpeechRecognitionResult object that includes the following properties: Duration OffsetInTicks Properties Reason ResultId Text Translations If the operation was successful, the Reason property has the enumerated value RecognizedSpeech , the Text property contains the transcription in the original language. You can also access a Translations property which contains a dictionary of the translations (using the two-character ISO language code, such as \"en\" for English, as a key).',\n",
       "  'links': ['https://learn.microsoft.com/wwl-data-ai/translate-speech-speech-service/media/translate-speech.png#lightbox'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/translate-speech-speech-service/media/translate-speech-small.png',\n",
       "    'image_alt': 'A TranslationRecognizer object is created from a SpeechConfig, TranslationConfig, and AudioConfig; and its RecognizeOnceAsync method is used to call the Speech API.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Translate speech with the Azure AI Speech service',\n",
       "  'topic': 'Synthesize translations',\n",
       "  'text': \"The TranslationRecognizer returns translated transcriptions of spoken input - essentially translating audible speech to text. You can also synthesize the translation as speech to create speech-to-speech translation solutions. There are two ways you can accomplish this. Event-based synthesis When you want to perform 1:1 translation (translating from one source language into a single target language), you can use event-based synthesis to capture the translation as an audio stream. To do this, you need to: Specify the desired voice for the translated speech in the TranslationConfig .\\nCreate an event handler for the TranslationRecognizer object's Synthesizing event.\\nIn the event handler, use the GetAudio () method of the Result parameter to retrieve the byte stream of translated audio.\\nThe specific code used to implement an event handler varies depending on the programming language you're using. See the C# and Python examples in the Speech SDK documentation. Manual synthesis Manual synthesis is an alternative approach to event-based synthesis that doesn't require you to implement an event handler. You can use manual synthesis to generate audio translations for one or more target languages. Manual synthesis of translations is essentially just the combination of two separate operations in which you: Use a TranslationRecognizer to translate spoken input into text transcriptions in one or more target languages. Iterate through the Translations dictionary in the result of the translation operation, using a SpeechSynthesizer to synthesize an audio stream for each language.\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-speech-translation?pivots=programming-language-csharp',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-speech-translation?pivots=programming-language-python'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Translate speech with the Azure AI Speech service',\n",
       "  'topic': 'Exercise - Translate speech',\n",
       "  'text': \"In this exercise, build an app that recognizes and translates speech into a specific language. Note To complete this lab, you need an Azure subscription . Launch the exercise and follow the instructions. Tip After completing the exercise, if you've finished exploring Azure AI services, delete the Azure resources that you created during the exercise.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2322069&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Translate speech with the Azure AI Speech service',\n",
       "  'topic': 'Summary',\n",
       "  'text': \"Now that you've completed this module, you learned how to: Provision Azure resources for speech translation. Generate text translation from speech. Synthesize spoken translations. For more information about speech translation, refer to the Azure AI Speech documentation .\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/speech-service/speech-translation'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Develop an audio-enabled generative AI application',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"Generative AI models make it possible to build intelligent chat-based applications that can understand and reason over input. Traditionally, text input is the primary mode of interaction with AI models, but multimodal models are increasingly becoming available. These models make it possible for chat applications to respond to audio input as well as text. In this module, we'll discuss audio-enabled generative AI and explore how you can use Azure AI Foundry to create generative AI solutions that respond to prompts that include a mix of text and audio data.\",\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Develop an audio-enabled generative AI application',\n",
       "  'topic': 'Deploy a multimodal model',\n",
       "  'text': 'To handle prompts that include audio, you need to deploy a multimodal generative AI model - in other words, a model that supports not only text-based input, but audio-based input as well. Multimodal models available in Azure AI Foundry include (among others): Microsoft Phi-4-multimodal-instruct OpenAI gpt-4o OpenAI gpt-4o-mini Tip To learn more about available models in Azure AI Foundry, see the Model catalog and collections in Azure AI Foundry portal article in the Azure AI Foundry documentation. Testing multimodal models with audio-based prompts After deploying a multimodal model, you can test it in the chat playground in Azure AI Foundry portal. Some models allow you to include audio attachments in the playground, either by uploading a file or recording a message. In the chat playground, you can upload a local audio file and add text to the message to elicit a response from a multimodal model.',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/model-catalog-overview'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/develop-generative-ai-audio-apps/media/audio-prompt.png',\n",
       "    'image_alt': 'Screenshot of the chat playground with an audio-based prompt.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Develop an audio-enabled generative AI application',\n",
       "  'topic': 'Develop an audio-based chat app',\n",
       "  'text': 'To develop a client app that engages in audio-based chats with a multimodal model, you can use the same basic techniques used for text-based chats. You require a connection to the endpoint where the model is deployed, and you use that endpoint to submit prompts that consists of messages to the model and process the responses. The key difference is that prompts for an audio-based chat include multi-part user messages that contain both a text content item and an audio content item. The JSON representation of a prompt that includes a multi-part user message looks something like this: { \\n    \"messages\": [ \\n        { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, \\n        { \"role\": \"user\", \"content\": [  \\n            { \\n                \"type\": \"text\", \\n                \"text\": \"Transcribe this audio:\" \\n            },\\n            { \\n                \"type\": \"audio_url\",\\n                \"audio_url\": {\\n                    \"url\": \"https://.....\"\\n                }\\n            }\\n        ] } \\n    ]\\n} The audio content item can be: A URL to an audio file in a web site. Binary audio data When using binary data to submit a local audio file, the audio_url content takes the form of a base64 encoded value in a data URL format: {\\n    \"type\": \"audio_url\",\\n    \"audio_url\": {\\n       \"url\": \"data:audio/mp3;base64,<binary_audio_data>\"\\n    }\\n} Depending on the model type, and where you deployed it, you can use Microsoft Azure AI Model Inference or OpenAI APIs to submit audio-based prompts. These libraries also provide language-specific SDKs that abstract the underlying REST APIs. In the exercise that follows in this module, you can use the Python or .NET SDK for the Azure AI Model Inference API and the OpenAI API to develop an audio-enabled chat application.',\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/develop-generative-ai-audio-apps/media/multi-part-prompt.png',\n",
       "    'image_alt': 'Diagram of a multi-part prompt being submitted to a model.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Develop an audio-enabled generative AI application',\n",
       "  'topic': 'Exercise - Develop an audio-enabled chat app',\n",
       "  'text': \"If you have an Azure subscription, you can complete this exercise to develop an audio-enabled chat app. Note If you don't have an Azure subscription, you can sign up for an account , which includes credits for the first 30 days. Launch the exercise and follow the instructions.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2320123&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Develop an audio-enabled generative AI application',\n",
       "  'topic': 'Summary',\n",
       "  'text': \"In this module, you learned about audio-enabled generative AI models and how to implement chat solutions that include audio-based input. Audio-enabled models let you create AI solutions that can understand audio and respond to related questions or instructions. Beyond just identifying spoken words, some models can also use reasoning based on what they hear. For instance, they can summarize a message or assess the speaker's sentiment. Tip For more information about working with multimodal models in Azure AI Foundry, see How to use image and audio in chat completions with Azure AI model inference and Quickstart: Use speech and audio in your AI chats .\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-foundry/model-inference/how-to/use-chat-multi-modal',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/openai/realtime-audio-quickstart'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Develop an Azure AI Voice Live agent',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"Voice-enabled applications are transforming how we interact with technology, and this module guides you through building a real-time, interactive voice solutions using advanced APIs and tools. The Azure AI Voice live API is a solution enabling low-latency, high-quality speech to speech interactions for voice agents. The API is designed for developers seeking scalable and efficient voice-driven experiences as it eliminates the need to manually orchestrate multiple components. After completing this module, you'll be able to: Implement the Azure AI Voice Live API to enable real-time, bidirectional communication. Set up and configure the agent session. Develop and manage event handlers to create dynamic and interactive user experiences. Build and deploy a Python-based web app with real-time voice interaction capabilities to Azure.\",\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Develop an Azure AI Voice Live agent',\n",
       "  'topic': 'Explore the Azure Voice Live API',\n",
       "  'text': 'The Voice live API enables developers to create voice-enabled applications with real-time, bidirectional communication. This unit explores its architecture, configuration, and implementation. Key features of the Voice Live API The Voice live API provides real-time communication using WebSocket connections. It supports advanced features such as speech recognition, text-to-speech synthesis, avatar streaming, and audio processing. JSON-formatted events manage conversations, audio streams, and responses. Events are categorized into client events (sent from client to server) and server events (sent from server to client). Key features include: Real-time audio processing with support for multiple formats like PCM16 and G.711. Advanced voice options, including OpenAI voices and Azure custom voices. Avatar integration using WebRTC for video and animation. Built-in noise reduction and echo cancellation. Note Voice Live API is optimized for Azure AI Foundry resources. We recommend using Azure AI Foundry resources for full feature availability and best Azure AI Foundry integration experience. For a table of supported models and regions, visit the Voice Live API overview . Connect to the Voice Live API The Voice live API supports two authentication methods: Microsoft Entra (keyless) and API key. Microsoft Entra uses token-based authentication for an Azure AI Foundry resource. You apply a retrieved authentication token using a Bearer token with the Authorization header. For the recommended keyless authentication with Microsoft Entra ID, you need to assign the Cognitive Services User role to your user account or a managed identity. You generate a token using the Azure CLI or Azure SDKs. The token must be generated with the https://ai.azure.com/.default scope, or the legacy https://cognitiveservices.azure.com/.default scope. Use the token in the Authorization header of the WebSocket connection request, with the format Bearer <token> . For key access, an API key can be provided in one of two ways. You can use an api-key connection header on the prehandshake connection. This option isn\\'t available in a browser environment. Or, you can use an api-key query string parameter on the request URI. Query string parameters are encrypted when using https/wss. Note The api-key connection header on the prehandshake connection isn\\'t available in a browser environment. WebSocket endpoint The endpoint to use varies depending on how you want to access your resources. You can access resources through a connection to the AI Foundry project (Agent), or through a connection to the model. Project connection: The endpoint is wss://<your-ai-foundry-resource-name>.services.ai.azure.com/voice-live/realtime?api-version=2025-10-01 Model connection: The endpoint is wss://<your-ai-foundry-resource-name>.cognitiveservices.azure.com/voice-live/realtime?api-version=2025-10-01 . The endpoint is the same for all models. The only difference is the required model query parameter, or, when using the Agent service, the agent_id and project_id parameters. Voice Live API events Client and server events facilitate communication and control within the Voice live API. Key client events include: session.update : Modify session configurations. input_audio_buffer.append : Add audio data to the buffer. response.create : Generate responses via model inference. Server events provide feedback and status updates: session.updated : Confirm session configuration changes. response.done : Indicate response generation completion. conversation.item.created : Notify when a new conversation item is added. For a full list of client/server events, visit Voice live API Reference . Note Proper handling of events ensures seamless interaction between client and server. Configure session settings for the Voice live API Often, the first event sent by the caller on a newly established Voice live API session is the session.update event. This event controls a wide set of input and output behavior. Session settings can be updated dynamically using the session.update event. Developers can configure voice types, modalities, turn detection, and audio formats. Example configuration: {\\n  \"type\": \"session.update\",\\n  \"session\": {\\n    \"modalities\": [\"text\", \"audio\"],\\n    \"voice\": {\\n      \"type\": \"openai\",\\n      \"name\": \"alloy\"\\n    },\\n    \"instructions\": \"You are a helpful assistant. Be concise and friendly.\",\\n    \"input_audio_format\": \"pcm16\",\\n    \"output_audio_format\": \"pcm16\",\\n    \"input_audio_sampling_rate\": 24000,\\n    \"turn_detection\": {\\n      \"type\": \"azure_semantic_vad\",\\n      \"threshold\": 0.5,\\n      \"prefix_padding_ms\": 300,\\n      \"silence_duration_ms\": 500\\n    },\\n    \"temperature\": 0.8,\\n    \"max_response_output_tokens\": \"inf\"\\n  }\\n} Tip Use Azure semantic VAD for intelligent turn detection and improved conversational flow. Implement real-time audio processing with the Voice live API Real-time audio processing is a core feature of the Voice live API. Developers can append, commit, and clear audio buffers using specific client events. Append audio: Add audio bytes to the input buffer. Commit audio: Process the audio buffer for transcription or response generation. Clear audio: Remove audio data from the buffer. Noise reduction and echo cancellation can be configured to enhance audio quality. For example: {\\n  \"type\": \"session.update\",\\n  \"session\": {\\n    \"input_audio_noise_reduction\": {\\n      \"type\": \"azure_deep_noise_suppression\"\\n    },\\n    \"input_audio_echo_cancellation\": {\\n      \"type\": \"server_echo_cancellation\"\\n    }\\n  }\\n} Note Noise reduction improves VAD accuracy and model performance by filtering input audio. Integrate avatar streaming using the Voice live API The Voice live API supports WebRTC-based avatar streaming for interactive applications. Developers can configure video, animation, and blendshape settings. Use the session.avatar.connect event to provide the client\\'s SDP offer. Configure video resolution, bitrate, and codec settings. Define animation outputs such as blendshapes and visemes. Example configuration: {\\n  \"type\": \"session.avatar.connect\",\\n  \"client_sdp\": \"<client_sdp>\"\\n} Tip Use high-resolution video settings for enhanced visual quality in avatar interactions.',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/speech-service/voice-live#supported-models-and-regions',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/speech-service/voice-live-api-reference'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Develop an Azure AI Voice Live agent',\n",
       "  'topic': 'Explore the AI Voice Live client library for Python',\n",
       "  'text': 'The Azure AI Voice Live client library for Python provides a real-time, speech-to-speech client for Azure AI Voice Live API. It opens a WebSocket session to stream microphone audio to the service and receives server events for responsive conversations. Important As of version 1.0.0, this SDK is async-only. The synchronous API is deprecated to focus exclusively on async patterns. All examples and samples use async/await syntax. In this unit, you learn how to use the SDK to implement authentication and handle events. You also see a minimal example of creating a session. For a full reference to the Voice Live package, visit the voice live Package reference . Implement authentication You can implement authentication with an API key or a Microsoft Entra ID token. The following code sample shows an API key implementation. It assumes environment variables are set in a .env file, or directly in your environment. import asyncio\\nfrom azure.core.credentials import AzureKeyCredential\\nfrom azure.ai.voicelive import connect\\n\\nasync def main():\\n    async with connect(\\n        endpoint=\"your-endpoint\",\\n        credential=AzureKeyCredential(\"your-api-key\"),\\n        model=\"gpt-4o\"\\n    ) as connection:\\n        # Your async code here\\n        pass\\n\\nasyncio.run(main()) For production applications, Microsoft Entra authentication is recommended. The following code sample shows implementing the DefaultAzureCredential for authentication: import asyncio\\nfrom azure.identity.aio import DefaultAzureCredential\\nfrom azure.ai.voicelive import connect\\n\\nasync def main():\\n    credential = DefaultAzureCredential()\\n    \\n    async with connect(\\n        endpoint=\"your-endpoint\",\\n        credential=credential,\\n        model=\"gpt-4o\"\\n    ) as connection:\\n        # Your async code here\\n        pass\\n\\nasyncio.run(main()) Handling events Proper handling of events ensures a more seamless interaction between the client and agent. For example, when handling a user interrupting the voice agent you need to cancel agent audio playback immediately in the client. If you don\\'t, the client continues to play the last agent response until the interrupt is processed in the API - resulting in the agent \"talking over\" the user. The following code sample shows some basic event handling: async for event in connection:\\n    if event.type == ServerEventType.SESSION_UPDATED:\\n        print(f\"Session ready: {event.session.id}\")\\n        # Start audio capture\\n        \\n    elif event.type == ServerEventType.INPUT_AUDIO_BUFFER_SPEECH_STARTED:\\n        print(\"User started speaking\")\\n        # Stop playback and cancel any current response\\n        \\n    elif event.type == ServerEventType.RESPONSE_AUDIO_DELTA:\\n        # Play the audio chunk\\n        audio_bytes = event.delta\\n        \\n    elif event.type == ServerEventType.ERROR:\\n        print(f\"Error: {event.error.message}\") Minimal example The following code sample shows authenticating to the API and configuring the session. import asyncio\\nfrom azure.core.credentials import AzureKeyCredential\\nfrom azure.ai.voicelive.aio import connect\\nfrom azure.ai.voicelive.models import (\\n    RequestSession, Modality, InputAudioFormat, OutputAudioFormat, ServerVad, ServerEventType\\n)\\n\\nAPI_KEY = \"your-api-key\"\\nENDPOINT = \"your-endpoint\"\\nMODEL = \"gpt-4o\"\\n\\nasync def main():\\n    async with connect(\\n        endpoint=ENDPOINT,\\n        credential=AzureKeyCredential(API_KEY),\\n        model=MODEL,\\n    ) as conn:\\n        session = RequestSession(\\n            modalities=[Modality.TEXT, Modality.AUDIO],\\n            instructions=\"You are a helpful assistant.\",\\n            input_audio_format=InputAudioFormat.PCM16,\\n            output_audio_format=OutputAudioFormat.PCM16,\\n            turn_detection=ServerVad(\\n                threshold=0.5, \\n                prefix_padding_ms=300, \\n                silence_duration_ms=500\\n            ),\\n        )\\n        await conn.session.update(session=session)\\n\\n        # Process events\\n        async for evt in conn:\\n            print(f\"Event: {evt.type}\")\\n            if evt.type == ServerEventType.RESPONSE_DONE:\\n                break\\n\\nasyncio.run(main())',\n",
       "  'links': ['https://learn.microsoft.com/en-us/python/api/azure-ai-voicelive/azure.ai.voicelive?view=azure-python'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Develop an Azure AI Voice Live agent',\n",
       "  'topic': 'Exercise - Develop an Azure AI Voice Live agent',\n",
       "  'text': \"In this exercise, you complete a Flask-based Python web app based that enables real-time voice interactions with an agent. You add the code to initialize the session, and handle session events. You use a deployment script that: deploys the AI model; creates an image of the app in Azure Container Registry (ACR) using ACR tasks; and then creates an Azure App Service instance that pulls the image. To test the app, you need an audio device with microphone and speaker capabilities. While this exercise is based on Python, you can develop similar applications other language-specific SDKs; including: Azure VoiceLive client library for .NET Tasks performed in this exercise: Download the base files for the app Add code to complete the web app Review the overall code base Update and run the deployment script View and test the application This exercise takes approximately 30 minutes to complete. Before you start To complete the exercise, you need: An Azure subscription. If you don't already have one, you can sign up for one https://azure.microsoft.com/ . An audio device with microphone and speaker capabilities. Get started Select the Launch Exercise button to open the exercise instructions in a new browser window. When you're finished with the exercise, return here to: Complete the module Earn a badge for completing this module\",\n",
       "  'links': ['https://www.nuget.org/packages/Azure.AI.VoiceLive/',\n",
       "   'https://azure.microsoft.com/'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2338402'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop natural language solutions in Azure - Training',\n",
       "  'unit': 'Develop an Azure AI Voice Live agent',\n",
       "  'topic': 'Summary',\n",
       "  'text': \"In this module, you learned about the Voice live API's features, including WebSocket connections, speech recognition, text-to-speech synthesis, and avatar streaming. You also explored Azure AI Voice Live for creating real-time speech-to-speech applications using Python, including setting up the client library and managing sessions. Additionally, you learned how to implement event handlers in Python for dynamic responses and real-time audio processing. Finally, you developed a Python-based web application using Flask, integrated it with Azure resources, and tested the application. Additional reading What is the Speech service? How to customize voice live input and output\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/speech-service/voice-live-how-to-customize'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Analyze images',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"Computer Vision is a branch of artificial intelligence (AI) in which software interprets visual input, often from images or video feeds. In Microsoft Azure, you can use the Azure AI Vision service to implement multiple computer vision scenarios, including: Image analysis Optical character recognition (OCR) Face detection and analysis Video analysis In this module, we'll focus on image analysis , and explore how to build applications that use the Azure AI Vision service to analyze and extract and infer insights from images. As shown in this conceptual diagram, the Azure AI Vision service provides services that you can use to analyze images and: Generate a caption for an image based on its contents. Suggest appropriate tags to associate with an image. Detect and locate common objects in an image. Detect and locate people in an image.\",\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-images/media/computer-vision.png',\n",
       "    'image_alt': 'Diagram of the Azure AI Vision service performing image analysis tasks.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Analyze images',\n",
       "  'topic': 'Provision an Azure AI Vision resource',\n",
       "  'text': \"To use Azure AI Vision image analysis services, you need to provision an Azure AI Vision resource in your Azure subscription. You can choose from multiple provisioning options: Create an Azure AI Foundry project and an associated hub . By default, an Azure AI Foundry hub includes an Azure AI services multi-service resource, which includes Azure AI Vision. Azure AI Foundry projects are recommended for development of AI solutions on Azure that combine generative AI, agents, and pre-built Azure AI services, or which involve collaborative development by a team of software engineers and service operators. If you don't need all of the functionality in an Azure AI Foundry hub, you can create an Azure AI services multi-service resource in your Azure subscription. You can then use this resource to access Azure AI Vision services and other AI services through a single endpoint and key. If you only need to use Azure AI Vision functionality, or you're just experimenting with the service, you can create a standalone Computer Vision resource in your Azure subscription. One benefit of this approach is that the standalone service provides a free tier that you can use to explore the service at no cost. Tip If you're unfamiliar with Azure AI Foundry and Azure AI services, consider completing the Plan and prepare to develop AI solutions on Azure module. Connecting to your resource After you've deployed your resource, you can use the Azure AI Vision REST API or a language-specific SDK (such as the Python SDK or Microsoft .NET SDK ) to connect to it from a client application. Every Azure AI Vision resource provides an endpoint to which client applications must connect. You can find the endpoint for your resource in the Azure portal, or if you're working in an Azure AI Foundry project, in the Azure AI Foundry portal. The endpoint is in the form of a URL, and typically looks something like this: https://<resource_name>.cognitiveservices.azure.com/ To connect to the endpoint, client applications must be authenticated. Options for authentication include: Key-based authentication : Client applications are authenticated by passing an authorization key (which you can find and manage in the portal). Microsoft Entra ID authentication : Client applications are authenticated by using a Microsoft Entra ID token for credentials that have permission to access the Azure AI Vision resource in Azure. When developing and testing an application, it's common to use key-based authentication or Microsoft Entra ID authentication based on your own Azure credentials. In production, consider using Microsoft Entra ID authentication based on a managed identity for your Azure application or use Azure Key Vault to store authorization keys securely. Note When using an Azure AI services resource in an Azure AI Foundry project, you can use the Azure AI Foundry SDK to connect to the project using Microsoft Entra ID authentication, and then retrieve the connection information for your Azure AI services resource, including the authorization key, from the project.\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/training/modules/prepare-azure-ai-development/',\n",
       "   'https://learn.microsoft.com/en-us/rest/api/computervision/image-analysis?azure-portal=true',\n",
       "   'https://learn.microsoft.com/en-us/python/api/overview/azure/ai-vision-imageanalysis-readme?azure-portal=true',\n",
       "   'https://learn.microsoft.com/en-us/dotnet/api/overview/azure/ai.vision.imageanalysis-readme?azure-portal=true'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-images/media/ai-vision-resources.png',\n",
       "    'image_alt': 'Diagram of AI Foundry containing AI services, which in turn contain AI Vision.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Analyze images',\n",
       "  'topic': 'Analyze an image',\n",
       "  'text': 'After connecting to your Azure AI Vision resource endpoint, your client application can use the service to perform image analysis tasks. Note the following requirements for image analysis: The image must be presented in JPEG, PNG, GIF, or BMP format. The file size of the image must be less than 4 megabytes (MB). The dimensions of the image must be greater than 50 x 50 pixels. Submitting an image for analysis To analyze an image, you can use the Analyze Image REST method or the equivalent method in the SDK for your preferred programming language, specifying the visual features you want to include in the analysis. from azure.ai.vision.imageanalysis import ImageAnalysisClient\\nfrom azure.ai.vision.imageanalysis.models import VisualFeatures\\nfrom azure.core.credentials import AzureKeyCredential\\n\\nclient = ImageAnalysisClient(\\n    endpoint=\"<YOUR_RESOURCE_ENDPOINT>\",\\n    credential=AzureKeyCredential(\"<YOUR_AUTHORIZATION_KEY>\")\\n)\\n\\nresult = client.analyze(\\n    image_data=<IMAGE_DATA_BYTES>, # Binary data from your image file\\n    visual_features=[VisualFeatures.CAPTION, VisualFeatures.TAGS],\\n    gender_neutral_caption=True,\\n) Note In this code example, the client app uses key-based authentication. To use Microsoft Entra ID authentication, you can use a TokenCredential instead of an AzureKeyCredential . The code example submits the image data as a binary object (which would typically be read from an image file). You can also analyze an image based on a URL by using the analyze_from_url method. Available visual features are contained in the VisualFeatures enumeration: VisualFeatures.TAGS: Identifies tags about the image, including objects, scenery, setting, and actions VisualFeatures.OBJECTS: Returns the bounding box for each detected object VisualFeatures.CAPTION: Generates a caption of the image in natural language VisualFeatures.DENSE_CAPTIONS: Generates more detailed captions for the objects detected VisualFeatures.PEOPLE: Returns the bounding box for detected people VisualFeatures.SMART_CROPS: Returns the bounding box of the specified aspect ratio for the area of interest VisualFeatures.READ: Extracts readable text using Azure.AI.Vision.ImageAnalysis;\\n\\nImageAnalysisClient client = new ImageAnalysisClient(\\n    \"<YOUR_RESOURCE_ENDPOINT>\",\\n    new AzureKeyCredential(\"<YOUR_AUTHORIZATION_KEY>\"));\\n\\nImageAnalysisResult result = client.Analyze(\\n    <IMAGE_DATA_BYTES>, // Binary data from your image file\\n    VisualFeatures.Caption | VisualFeatures.Tags,\\n    new ImageAnalysisOptions { GenderNeutralCaption = true }); Note In this code example, the client app uses key-based authentication. To use Microsoft Entra ID authentication, you can use a TokenCredential instead of an AzureKeyCredential . The code example submits the image data as a binary object (which would typically be read from an image file). You can also analyze an image based on a URL. Available visual features are contained in the VisualFeatures enumeration: VisualFeatures.Tags: Identifies tags about the image, including objects, scenery, setting, and actions VisualFeatures.Objects: Returns the bounding box for each detected object VisualFeatures.Caption: Generates a caption of the image in natural language VisualFeatures.DenseCaptions: Generates more detailed captions for the objects detected VisualFeatures.People: Returns the bounding box for detected people VisualFeatures.SmartCrops: Returns the bounding box of the specified aspect ratio for the area of interest VisualFeatures.Read: Extracts readable text Specifying the visual features you want analyzed in the image determines what information the response will contain. Most responses will contain a bounding box (if a location in the image is reasonable) or a confidence score (for features such as tags or captions). Processing the response This method returns a JSON document containing the requested information. The JSON response for image analysis looks similar to this example, depending on your requested features: {\\n  \"apim-request-id\": \"abcde-1234-5678-9012-f1g2h3i4j5k6\",\\n  \"modelVersion\": \"<version>\",\\n  \"denseCaptionsResult\": {\\n    \"values\": [\\n      {\\n        \"text\": \"a house in the woods\",\\n        \"confidence\": 0.7055229544639587,\\n        \"boundingBox\": {\\n          \"x\": 0,\\n          \"y\": 0,\\n          \"w\": 640,\\n          \"h\": 640\\n        }\\n      },\\n      {\\n        \"text\": \"a trailer with a door and windows\",\\n        \"confidence\": 0.6675070524215698,\\n        \"boundingBox\": {\\n          \"x\": 214,\\n          \"y\": 434,\\n          \"w\": 154,\\n          \"h\": 108\\n        }\\n      }\\n    ]\\n  },\\n  \"metadata\": {\\n    \"width\": 640,\\n    \"height\": 640\\n  }\\n}',\n",
       "  'links': ['https://learn.microsoft.com/en-us/rest/api/computervision/image-analysis/analyze-image?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Analyze images',\n",
       "  'topic': 'Exercise - Analyze images',\n",
       "  'text': \"Now it's your turn to try using the Azure AI Vision service. In this exercise, you use the Azure AI Vision SDK to develop a client application that analyzes images. Note To complete this lab, you need an Azure subscription in which you have administrative access. Launch the exercise and follow the instructions. Tip After completing the exercise, if you've finished exploring Azure AI services, delete the Azure resources that you created during the exercise.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2319402&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Analyze images',\n",
       "  'topic': 'Summary',\n",
       "  'text': \"In this module, you learned how to provision an Azure AI Vision resource and use it from a client application to analyze images. You can use Azure AI Vision's image analysis capabilities in scenarios that require information extraction or inference from images. A common use case is digital asset management (DAM), in which you need to tag, catalog, and index image-based data. To learn more about image analysis with the Azure AI Vision service, see the Azure AI Vision documentation .\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/overview-image-analysis?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Read text in images',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"We live in a digital world, in which data is increasingly captured as images. Often, those images contain text, which you need to be able to extract from their pixelated format in the image for processing, indexing, and other tasks. Everyday examples include: Meeting a new business associate and taking a photograph of their business card to store their contact details digitally. Scanning a document or ID card to include in an application for a government or commercial service. Taking a photo of a menu or recipe to store it in a digital notebook. Photographing street signs or store fronts so you can submit the text they contain to a translation app. Digitizing handwritten notes using a cellphone camera. In this module, we'll explore the optical character recognition (OCR) capabilities of the Azure AI Vision Image Analysis API, which makes these scenarios, and more, possible.\",\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/read-text-images-documents-with-computer-vision-service/media/optical-character-recognition.png',\n",
       "    'image_alt': 'Diagram of an image containing text being read by the Azure AI Vision image analysis service.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Read text in images',\n",
       "  'topic': 'Explore Azure AI options for reading text',\n",
       "  'text': \"There are multiple Azure AI services that read text from documents and images, each optimized for results depending on the input and the specific requirements of your application. Azure AI Vision Azure AI Vision includes an image analysis capability that supports optical character recognition (OCR). Consider using Azure AI Vision in the following scenarios: Text location and extraction from scanned documents : Azure AI Vision is a great solution for general, unstructured documents that have been scanned as images. For example, reading text in labels, menus, or business cards. Finding and reading text in photographs : Examples include photo's that include street signs and store names. Digital asset management (DAM) : Azure AI Vision includes functionality for analyzing images beyond extracting text; including object detection, describing or categorizing an image, generating smart-cropped thumbnails and more. These capabilities make it a useful service when you need to catalog, index, or analyze large volumes of digital image-based content. Azure AI Document Intelligence Azure AI Document Intelligence is a service that you can use to extract information from complex digital documents. Azure AI Document Intelligence is designed for extracting text, key-value pairs, tables, and structures from documents automatically and accurately. Key considerations for choosing Azure AI Document Intelligence include: Form processing : Azure AI Document Intelligence is specifically designed to extract data from forms, invoices, receipts, and other structured documents. Prebuilt models : Azure AI Document Intelligence provides prebuilt models for common document types to reduce complexity and integrate into workflows or applications. Custom models : Creating custom models tailored to your specific documents, makes Azure AI Document Intelligence a flexible solution that can be used in many business scenarios. Azure AI Content Understanding Azure AI Content Understanding is a service that you can use to analyze and extract information from multiple kinds of content; including documents, images, audio streams, and video. It is suitable for: Multimodal content extraction : Extracting content and structured fields from documents, forms, audio, video, and images. Custom content analysis scenarios : Support for customizable analyzers enables you to extract specific content or fields tailored to business needs. Note In the rest of this module, we'll focus on the OCR image analysis feature in Azure AI Vision . To learn more about Azure AI Document Intelligence and Azure AI Content understanding, consider completing the following training modules: Plan an Azure AI Document Intelligence solution Analyze content with Azure AI Content Understanding\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/training/modules/plan-form-recognizer-solution/',\n",
       "   'https://learn.microsoft.com/en-us/training/modules/analyze-content-ai/'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/read-text-images-documents-with-computer-vision-service/media/ai-vision.png',\n",
       "    'image_alt': 'Azure AI Vision icon'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/read-text-images-documents-with-computer-vision-service/media/document-intelligence.png',\n",
       "    'image_alt': 'Azure AI Document Intelligence icon'},\n",
       "   {'image_name': 'image3',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/read-text-images-documents-with-computer-vision-service/media/content-understanding.png',\n",
       "    'image_alt': 'Azure AI Content Understanding icon'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Read text in images',\n",
       "  'topic': 'Read text with Azure AI Vision Image Analysis',\n",
       "  'text': 'To use Azure AI Vision for image analysis, including optical character recognition, you must provision an Azure AI Vision resource in an Azure subscription. The resource can be: An Azure AI Services multi-service resource (either deployed as part of an Azure AI Foundry hub and project, or as a standalone resource). A Computer Vision resource. To use your deployed resource in an application, you must connect to its endpoint using either key-based authentication or Microsoft Entra ID authentication. You can find the endpoint for your resource in the Azure portal, or if you\\'re working in an Azure AI Foundry project, in the Azure AI Foundry portal. The endpoint is in the form of a URL, and typically looks something like this: https://<resource_name>.cognitiveservices.azure.com/ After establishing a connection, you can use the OCR feature by calling the ImageAnalysis function (via the REST API or with an equivalent SDK method), passing the image URL or binary data, and optionally specifying the language the text is written in (with a default value of en for English). https://<endpoint>/computervision/imageanalysis:analyze?features=read&... To use the Azure AI Vision Python SDK to extract text from an image, install the azure-ai-vision-imageanalysis package. Then, in your code, use either key-based authentication or Microsoft Entra ID authentication to connect an ImageAnalysisClient object to an Azure AI Vision resource. To find and read text in an image, call the analyze (or analyze_from_url ) method, specifying the VisualFeatures.READ enumeration. from azure.ai.vision.imageanalysis import ImageAnalysisClient\\nfrom azure.ai.vision.imageanalysis.models import VisualFeatures\\nfrom azure.core.credentials import AzureKeyCredential\\n\\nclient = ImageAnalysisClient(\\n    endpoint=\"<YOUR_RESOURCE_ENDPOINT>\",\\n    credential=AzureKeyCredential(\"<YOUR_AUTHORIZATION_KEY>\")\\n)\\n\\nresult = client.analyze(\\n    image_data=<IMAGE_DATA_BYTES>, # Binary data from your image file\\n    visual_features=[VisualFeatures.READ],\\n    language=\"en\",\\n) To use the Azure AI Vision .NET SDK to extract text from an image, install the Azure.AI.Vision.ImageAnalysis package. Then, in your code, use either key-based authentication or Microsoft Entra ID authentication to connect an ImageAnalysisClient object to an Azure AI Vision resource. To find and read text in an image, call the Analyze method, specifying the VisualFeatures.Read enumeration. using Azure.AI.Vision.ImageAnalysis;\\n\\nImageAnalysisClient client = new ImageAnalysisClient(\\n    \"<YOUR_RESOURCE_ENDPOINT>\",\\n    new AzureKeyCredential(\"<YOUR_AUTHORIZATION_KEY>\"));\\n\\nImageAnalysisResult result = client.Analyze(\\n    <IMAGE_DATA_BYTES>, // Binary data from your image file\\n    VisualFeatures.Read,\\n    new ImageAnalysisOptions { Language = t\"en\" }); The results of the Read OCR function are returned synchronously, either as JSON or the language-specific object of a similar structure. These results are broken down in blocks (with the current service only using one block), then lines , and then words . Additionally, the text values are included at both the line and word levels, making it easier to read entire lines of text if you don\\'t need to extract text at the individual word level. {\\n    \"metadata\":\\n    {\\n        \"width\": 500,\\n        \"height\": 430\\n    },\\n    \"readResult\":\\n    {\\n        \"blocks\":\\n        [\\n            {\\n                \"lines\":\\n                [\\n                    {\\n                        \"text\": \"Hello World!\",\\n                        \"boundingPolygon\":\\n                        [\\n                            {\"x\":251,\"y\":265},\\n                            {\"x\":673,\"y\":260},\\n                            {\"x\":674,\"y\":308},\\n                            {\"x\":252,\"y\":318}\\n                        ],\\n                        \"words\":\\n                        [\\n                            {\\n                                \"text\":\"Hello\",\\n                                \"boundingPolygon\":\\n                                [\\n                                    {\"x\":252,\"y\":267},\\n                                    {\"x\":307,\"y\":265},\\n                                    {\"x\":307,\"y\":318},\\n                                    {\"x\":253,\"y\":318}\\n                                ],\\n                            \"confidence\":0.996\\n                            },\\n                            {\\n                                \"text\":\"World!\",\\n                                \"boundingPolygon\":\\n                                [\\n                                    {\"x\":318,\"y\":264},\\n                                    {\"x\":386,\"y\":263},\\n                                    {\"x\":387,\"y\":316},\\n                                    {\"x\":319,\"y\":318}\\n                                ],\\n                                \"confidence\":0.99\\n                            }\\n                        ]\\n                    },\\n                ]\\n            }\\n        ]\\n    }\\n}',\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Read text in images',\n",
       "  'topic': 'Exercise - Read text in images',\n",
       "  'text': \"Now it's your turn to try using the OCR capabilities of Azure AI Vision. In this exercise, you use the Azure AI Vision Image Analysis SDK to develop a client application that extracts text from images. Note To complete this lab, you need an Azure subscription in which you have administrative access. Launch the exercise and follow the instructions. Tip After completing the exercise, if you've finished exploring Azure AI services, delete the Azure resources that you created during the exercise.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2320100&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Read text in images',\n",
       "  'topic': 'Summary',\n",
       "  'text': 'In this module, you learned how to provision an Azure AI Vision resource and use it from a client application to extract text from images. To learn more about using Azure AI Vision for OCR, see the OCR - Optical Character Recognition in the Azure AI Vision documentation.',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/overview-ocr'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Detect, analyze, and recognize faces',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"Face detection, analysis, and recognition are all common computer vision challenges for AI systems. The ability to detect when a person is present, analyze a person's facial features, or recognize an individual based on their face is a key way in which AI systems can exhibit human-like behavior and build empathy with users. In this module, you'll explore how the Azure AI Vision Face API enables you to build solutions that analyze faces in images. Note Access to the full capabilities of the Face API is restricted in accordance with Microsoft's responsible AI policies. For details, see Limited Access to Face API . This module describes some capabilities that require explicit access. The practical exercise in the module is based on unrestricted features of the service.\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/legal/cognitive-services/computer-vision/limited-access-identity'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/detect-analyze-recognize-faces/media/face-api.png',\n",
       "    'image_alt': 'Diagram of faces being detected from an image by the Azure AI Vision Face API.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Detect, analyze, and recognize faces',\n",
       "  'topic': 'Plan a face detection, analysis, or recognition solution',\n",
       "  'text': 'The Face service provides comprehensive facial detection, analysis, and recognition capabilities. The Face service provides functionality that you can use for: Face detection - for each detected face, the results include an ID that identifies the face and the bounding box coordinates indicating its location in the image. Face attribute analysis - you can return a wide range of facial attributes, including: Head pose ( pitch , roll , and yaw orientation in 3D space) Glasses ( No glasses , Reading glasses , Sunglasses , or Swimming Goggles ) Mask (the presence of a face mask) Blur ( low , medium , or high ) Exposure ( under exposure , good exposure , or over exposure ) Noise (visual noise in the image) Occlusion (objects obscuring the face) Accessories (glasses, headwear, mask) QualityForRecognition ( low , medium , or high ) Facial landmark location - coordinates for key landmarks in relation to facial features (for example, eye corners, pupils, tip of nose, and so on) Face comparison - you can compare faces across multiple images for similarity (to find individuals with similar facial features) and verification (to determine that a face in one image is the same person as a face in another image) Facial recognition - you can train a model with a collection of faces belonging to specific individuals, and use the model to identify those people in new images. Facial liveness - liveness can be used to determine if the input video is a real stream or a fake to prevent bad-intentioned individuals from spoofing a facial recognition system. Face detection and recognition models The Azure AI Vision Face API is built on face detection and recognition models that have been pre-trained. Multiple versions of these models are available, each with specific strengths and capabilities. For example, newer models exhibit greater accuracy when working with small images; but may not provide the same breadth of facial analysis capabilities. When you use the service in an application, you must select the model you want to use based on your requirements. Tip For guidance about selecting a detection model, see Specify a face detection model . For guidance about how to select a recognition model, see Specify a face recognition model .',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/specify-detection-model',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/specify-recognition-model'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/detect-analyze-recognize-faces/media/face-service.png',\n",
       "    'image_alt': 'Diagram of Face service capabilities.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Detect, analyze, and recognize faces',\n",
       "  'topic': 'Detect and analyze faces',\n",
       "  'text': 'To use the Azure AI Vision Face API, you must provision a resource for the service in an Azure subscription. You can provision Face as a single-service resource, or you can use the Face API in a multi-service Azure AI Services resource; which can be provisioned as a standalone resource or as part of an Azure AI Foundry hub. To use your resource from a client application you must connect to its endpoint using either key-based authentication or Microsoft Entra AI authentication . When using the REST interface you can provide the authentication key or token in the request header. When using a language-specific SDK (for example, the Python azure-ai-vision-face package or the Microsoft .NET Azure.AI.Vision.Face package), you use a FaceClient object to connect to the service. from azure.ai.vision.face import FaceClient\\nfrom azure.ai.vision.face.models import *\\nfrom azure.core.credentials import AzureKeyCredential\\n\\nface_client = FaceClient(\\n    endpoint=\"<YOUR_RESOURCE_ENDPOINT>\",\\n    credential=AzureKeyCredential(\"<YOUR_RESOURCE_KEY>\")) using Azure;\\nusing Azure.AI.Vision.Face;\\n\\nFaceClient faceClient = new FaceClient(\\n    new Uri(\"<YOUR_RESOURCE_ENDPOINT>\"),\\n    new AzureKeyCredential(\"<YOUR_RESOURCE_KEY>\")); To detect and analyze faces in an image, you must specify the model-specific features you want the service to return, and then use the client to call the Detect method. # Specify facial features to be retrieved\\nfeatures = [FaceAttributeTypeDetection01.HEAD_POSE,\\n            FaceAttributeTypeDetection01.OCCLUSION,\\n            FaceAttributeTypeDetection01.ACCESSORIES]\\n\\n# Use client to detect faces in an image\\nwith open(\"<IMAGE_FILE_PATH>\", mode=\"rb\") as image_data:\\n    detected_faces = face_client.detect(\\n        image_content=image_data.read(),\\n        detection_model=FaceDetectionModel.DETECTION01,\\n        recognition_model=FaceRecognitionModel.RECOGNITION01,\\n        return_face_id=True,\\n        return_face_attributes=features,\\n    ) // Specify facial features to be retrieved\\nFaceAttributeType[] features = new FaceAttributeType[]\\n{\\n    FaceAttributeType.Detection01.HeadPose,\\n    FaceAttributeType.Detection01.Occlusion,\\n    FaceAttributeType.Detection01.Accessories\\n};\\n\\n// Use client to detect faces in an image\\nusing (var imageData = File.OpenRead(imageFile))\\n{    \\n    var response = await faceClient.DetectAsync(\\n        BinaryData.FromStream(imageData),\\n        FaceDetectionModel.Detection01,\\n        FaceRecognitionModel.Recognition01,\\n        returnFaceId: false,\\n        returnFaceAttributes: features);\\n    IReadOnlyList<FaceDetectionResult> detected_faces = response.Value;\\n} The response from the service depends on: The model-specific features requested. The number of faces detected in the image. A response for an image containing a single face might look similar to the following example: [\\n    {\\n        \\'faceRectangle\\': {\\'top\\': 174, \\'left\\': 247, \\'width\\': 246, \\'height\\': 246}\\n        \\'faceAttributes\\':\\n        {\\n            \\'headPose\\':{\\'pitch\\': 3.7, \\'roll\\': -7.7, \\'yaw\\': -20.9},\\n            \\'accessories\\':\\n                [\\n                    {\\'type\\': \\'glasses\\', \\'confidence\\': 1.0}\\n                ],\\n            \\'occlusion\\':{\\'foreheadOccluded\\': False, \\'eyeOccluded\\': False, \\'mouthOccluded\\': False}\\n        }\\n    }\\n]',\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Detect, analyze, and recognize faces',\n",
       "  'topic': 'Verify and identify faces',\n",
       "  'text': \"In addition to detecting and analyzing faces, you can use the Azure AI Vision Face service to compare and recognize faces. Important Usage of facial recognition, comparison, and verification requires approval through a Limited Access policy . Verifying faces When a face is detected by the Face service, a unique ID is assigned to it and retained in the service resource for 24 hours. The ID is a GUID, with no indication of the individual's identity other than their facial features. While the detected face ID is cached, subsequent images can be used to compare the new faces to the cached identity and determine if they're similar (in other words, they share similar facial features) or to verify that the same person appears in two images. This ability to compare faces anonymously can be useful in systems where it's important to confirm that the same person is present on two occasions, without the need to know the actual identity of the person. For example, by taking images of people as they enter and leave a secured space to verify that everyone who entered leaves. Identifying faces For scenarios where you need to positively identify individuals, you can train a facial recognition model using face images. To train a facial recognition model with the Face service: Create a Person Group that defines the set of individuals you want to identify (for example, employees ). Add a Person to the Person Group for each individual you want to identify. Add detected faces from multiple images to each person , preferably in various poses. The IDs of these faces will no longer expire after 24 hours (so they're now referred to as persisted faces). Train the model. The trained model is stored in your Face (or Azure AI Services) resource, and can be used by client applications to: Identify individuals in images. Verify the identity of a detected face. Analyze new images to find faces that are similar to a known, persisted face. Tip To learn more about using face verification and identification to implement a facial recognition solution, see Face recognition in the Azure AI Vision Face documentation.\",\n",
       "  'links': ['https://aka.ms/cog-services-limited-access',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/concept-face-recognition'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/detect-analyze-recognize-faces/media/face-matching.png',\n",
       "    'image_alt': 'Diagram of a detected face matched in two images.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/detect-analyze-recognize-faces/media/person-groups.png',\n",
       "    'image_alt': 'Diagram of person groups containing Person records with persisted faces.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Detect, analyze, and recognize faces',\n",
       "  'topic': 'Responsible AI considerations for face-based solutions',\n",
       "  'text': \"While all applications of artificial intelligence require considerations for responsible, system that rely on facial or other biometric data can be particularly problematic. When building a solution that uses facial data, considerations include (but aren't limited to): Data privacy and security . Facial data is personally identifiable, and should be considered sensitive and private. You should ensure that you have implemented adequate protection for facial data used for model training and inferencing. Transparency . Ensure that users are informed about how their facial data is used, and who will have access to it. Fairness and inclusiveness . Ensure that your face-based system can't be used in a manner that is prejudicial to individuals based on their appearance, or to unfairly target individuals. Note You can find details of the responsible AI measures taken in the implementation of the Face API in Data and privacy for Face .\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/legal/cognitive-services/face/data-privacy-security'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Detect, analyze, and recognize faces',\n",
       "  'topic': 'Exercise - Detect and analyze faces',\n",
       "  'text': \"Now it's your turn to try using the Azure AI Vision Face service. In this exercise, you use the Azure AI Vision Face API to develop a client application that detects and analyzes faces in an image. Note To complete this lab, you need an Azure subscription in which you have administrative access. Launch the exercise and follow the instructions. Tip After completing the exercise, if you've finished exploring Azure AI services, delete the Azure resources that you created during the exercise.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2320103&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Detect, analyze, and recognize faces',\n",
       "  'topic': 'Summary',\n",
       "  'text': 'In this module, you have learned how to detect, analyze, and recognize faces. Facial analysis and recognition solutions can help you address business requirements for security and authorization, digital asset management, and other scenarios. However, you should be mindful of the need to use biometric data such as facial imaging responsibly. Tip To find out more about the Azure AI Vision Face service, see the Face documentation .',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/overview-identity'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Classify images',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"Image classification is a common computer vision problem that requires software to analyze an image and categorize (or classify ) it. For example, an unattended checkout system in a grocery store might use a camera to scan each item a customer adds to their cart, and use image classification to identify the product in the image. In this module, you'll learn how the Azure AI Custom Vision service enables you to build your own computer vision models for image classification.\",\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/classify-images/media/image-classification.png',\n",
       "    'image_alt': 'Diagram of an image classification solution for groceries.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Classify images',\n",
       "  'topic': 'Azure AI Custom Vision',\n",
       "  'text': 'The Azure AI Custom Vision service enables you to build your own computer vision models for image classification or object detection . To use the Custom Vision service to create a solution, you need two Custom Vision resources in your Azure subscription: An Azure AI Custom Vision training resource - used to train a custom model based on your own training images. An Azure AI Custom Vision prediction resource - used to generate predictions from new images based on your trained model. When you provision the Azure AI Custom Vision service in an Azure subscription, you can choose to create one or both of these resources. This separation of training and prediction provides flexibility. For example, you can use a training resource in one region to train your model using your own image data; and then deploy one or more prediction resources in other regions to support computer vision applications that need to use your model. Each resource has its own unique endpoint and authentication keys ; which are used by client applications to connect and authenticate to the service. The Custom Vision portal Azure AI Custom Vision provides a web-based portal, in which you can train, publish, and test custom vision models. You can sign into the Custom Vision portal at https://www.customvision.ai/ using your Azure credentials and use it to create image classification or object detection projects that use Azure AI Custom Vision resources in your Azure subscription. Each project has a unique project ID ; which is used by client applications to perform training or prediction tasks using code. Custom Vision SDKs You can write code to train and consume custom models by using the Azure AI Custom Vision language-specific SDKs. For example, Microsoft C# developers can use the Microsoft.Azure.CognitiveServices.Vision.CustomVision.Training and Microsoft.Azure.CognitiveServices.Vision.CustomVision.Prediction Microsoft .NET packages for training and prediction respectively. Python developers can perform both training and prediction tasks by using the azure-cognitiveservices-vision-customvision package.',\n",
       "  'links': ['https://www.customvision.ai?azure-portal=true',\n",
       "   'https://www.nuget.org/packages/Microsoft.Azure.CognitiveServices.Vision.CustomVision.Training?azure-portal=true',\n",
       "   'https://www.nuget.org/packages/Microsoft.Azure.CognitiveServices.Vision.CustomVision.Prediction?azure-portal=true',\n",
       "   'https://pypi.org/project/azure-cognitiveservices-vision-customvision?azure-portal=true'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/classify-images/media/custom-vision-portal.png',\n",
       "    'image_alt': 'Screenshot of the Azure AI Custom Vision portal.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Classify images',\n",
       "  'topic': 'Train an image classification model',\n",
       "  'text': \"Image classification is a computer vision technique in which a model is trained to predict a class label for an image based on its contents. Usually, the class label relates to the main subject of the image. For example, the following images have been classified based on the type of fruit they contain. Models can be trained for multiclass classification (in other words, there are multiple classes, but each image can belong to only one class) or multilabel classification (in other words, an image might be associated with multiple labels). Training an image classification model To train an image classification model with the Azure AI Custom Vision service, you can use the Azure AI Custom Vision portal, the Azure AI Custom Vision REST API or SDK, or a combination of both approaches. In most cases, you'll typically use the Azure AI Custom Vision portal to train your model. The portal provides a graphical interface that you can use to: Create an image classification project for your model and associate it with a training resource. Upload images, assigning class label tags to them. Review and edit tagged images. Train and evaluate a classification model. Test a trained model. Publish a trained model to a prediction resource. The REST API and SDKs enable you to perform the same tasks by writing code, which is useful if you need to automate model training and publishing as part of a DevOps process.\",\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/classify-images/media/classified-fruit.png',\n",
       "    'image_alt': 'Photographs of fruit classified as Apple, Banana, and Orange.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/classify-images/media/train-classifier.png',\n",
       "    'image_alt': 'Screenshot of the Azure AI Custom Vision portal.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Classify images',\n",
       "  'topic': 'Create an image classification client application',\n",
       "  'text': 'After you\\'ve trained an image classification model, you can use the Azure AI Custom Vision SDK to develop a client application that submits new images to be classified. from msrest.authentication import ApiKeyCredentials\\nfrom azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\\n\\n\\n # Authenticate a client for the prediction API\\ncredentials = ApiKeyCredentials(in_headers={\"Prediction-key\": \"<YOUR_PREDICTION_RESOURCE_KEY>\"})\\nprediction_client = CustomVisionPredictionClient(endpoint=\"<YOUR_PREDICTION_RESOURCE_ENDPOINT>\",\\n                                                 credentials=credentials)\\n\\n# Get classification predictions for an image\\nimage_data = open(\"<PATH_TO_IMAGE_FILE>\"), \"rb\").read()\\nresults = prediction_client.classify_image(\"<YOUR_PROJECT_ID>\",\\n                                           \"<YOUR_PUBLISHED_MODEL_NAME>\",\\n                                           image_data)\\n\\n# Process predictions\\nfor prediction in results.predictions:\\n    if prediction.probability > 0.5:\\n        print(image, \\': {} ({:.0%})\\'.format(prediction.tag_name, prediction.probability)) using System;\\nusing System.IO;\\nusing Microsoft.Azure.CognitiveServices.Vision.CustomVision.Prediction;\\n\\n// Authenticate a client for the prediction API\\nCustomVisionPredictionClient prediction_client = new CustomVisionPredictionClient(new ApiKeyServiceClientCredentials(\"<YOUR_PREDICTION_RESOURCE_KEY>\"))\\n{\\n    Endpoint = \"<YOUR_PREDICTION_RESOURCE_ENDPOINT>\"\\n};\\n\\n// Get classification predictions for an image\\nMemoryStream image_data = new MemoryStream(File.ReadAllBytes(\"<PATH_TO_IMAGE_FILE>\"));\\nvar result = prediction_client.ClassifyImage(\"<YOUR_PROJECT_ID>\",\\n                                             \"<YOUR_PUBLISHED_MODEL_NAME>\",\\n                                             image_data);\\n\\n// Process predictions\\nforeach (var prediction in result.Predictions)\\n{\\n    if (prediction.Probability > 0.5)\\n    {\\n        Console.WriteLine($\"{prediction.TagName} ({prediction.Probability})\");\\n    }\\n}',\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Classify images',\n",
       "  'topic': 'Exercise - Classify images',\n",
       "  'text': \"Now it's your turn to try using the Azure AI Custom Vision service. In this exercise, you train and publish a custom image classification model, and use the Azure AI Custom Vision SDK to test it. Note To complete this lab, you need an Azure subscription in which you have administrative access. Launch the exercise and follow the instructions. When you finish the exercise, don't forget to come back and complete the knowledge check to earn points for completing this module! Tip After completing the exercise, if you've finished exploring Azure AI services, delete the Azure resources that you created during the exercise.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2320009&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Classify images',\n",
       "  'topic': 'Summary',\n",
       "  'text': 'In this module, you learned how to use the Azure AI Custom Vision service to build your own custom vision models for image classification. Tip To find out more about the Azure AI Custom Vision service, see the Azure AI Custom Vision documentation .',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/custom-vision-service/'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Detect objects in images',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"Object detection is a common computer vision problem that requires software to identify the location of specific classes of object in an image. For example, an automated checkout system in a grocery store might use a camera to monitor a checkout conveyer belt on which there might be multiple different items at any one time. The system could use object detection to identify which items are on the belt, and where in the image they appear. In this module, you'll learn how to use the Azure AI Custom Vision service to create object detection models.\",\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/detect-objects-images/media/object-detection.png',\n",
       "    'image_alt': 'Diagram of an object detection model identifying objects in an image.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Detect objects in images',\n",
       "  'topic': 'Use Azure AI Custom Vision for object detection',\n",
       "  'text': 'To use the Custom Vision service to create an object detection solution, you need two Custom Vision resources in your Azure subscription: An Azure AI Custom Vision training resource - used to train a custom model based on your own training images. An Azure AI Custom Vision prediction resource - used to generate predictions from new images based on your trained model. When you provision the Azure AI Custom Vision service in an Azure subscription, you can choose to create one or both of these resources. This separation of training and prediction provides flexibility. For example, you can use a training resource in one region to train your model using your own image data; and then deploy one or more prediction resources in other regions to support computer vision applications that need to use your model. Each resource has its own unique endpoint and authentication keys ; which are used by client applications to connect and authenticate to the service. The Custom Vision portal Azure AI Custom Vision provides a web-based portal, in which you can train, publish, and test custom vision models. You can sign into the Custom Vision portal at https://www.customvision.ai/ using your Azure credentials and use it to create image classification or object detection projects that use Azure AI Custom Vision resources in your Azure subscription. Each project has a unique project ID ; which is used by client applications to perform training or prediction tasks using code. Custom Vision SDKs You can write code to train and consume custom models by using the Azure AI Custom Vision language-specific SDKs. For example, Microsoft C# developers can use the Microsoft.Azure.CognitiveServices.Vision.CustomVision.Training and Microsoft.Azure.CognitiveServices.Vision.CustomVision.Prediction Microsoft .NET packages for training and prediction respectively. Python developers can perform both training and prediction tasks by using the azure-cognitiveservices-vision-customvision package.',\n",
       "  'links': ['https://www.customvision.ai?azure-portal=true',\n",
       "   'https://www.nuget.org/packages/Microsoft.Azure.CognitiveServices.Vision.CustomVision.Training?azure-portal=true',\n",
       "   'https://www.nuget.org/packages/Microsoft.Azure.CognitiveServices.Vision.CustomVision.Prediction?azure-portal=true',\n",
       "   'https://pypi.org/project/azure-cognitiveservices-vision-customvision?azure-portal=true'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/detect-objects-images/media/custom-vision-portal.png',\n",
       "    'image_alt': 'Screenshot of the Azure AI Custom Vision portal.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Detect objects in images',\n",
       "  'topic': 'Train an object detector',\n",
       "  'text': 'Object detection is a form of computer vision in which a model is trained to detect the presence and location of one or more classes of object in an image. There are two components to an object detection prediction: The class label of each object detected in the image. For example, you might ascertain that an image contains an apple, an orange, and a banana. The location of each object within the image, indicated as coordinates of a bounding box that encloses the object. To train an object detection model, you can use the Azure AI Custom Vision portal to upload and label images before training, evaluating, testing, and publishing the model; or you can use the REST API or a language-specific SDK to write code that performs the training tasks. Image labeling You can use Azure AI Custom Vision to create projects for image classification or object detection . The most significant difference between training an image classification model and training an object detection model is the labeling of the images with tags. While image classification requires one or more tags that apply to the whole image, object detection requires that each label consists of a tag and a region that defines the bounding box for each object in an image. Labeling images in the Azure AI Custom Vision portal The Azure AI Custom Vision portal provides a graphical interface that you can use to label your training images. The easiest option for labeling images for object detection is to use the interactive interface in the Azure AI Custom Vision portal. This interface automatically suggests regions that contain objects, to which you can assign tags or adjust by dragging the bounding box to enclose the object you want to label. Additionally, after tagging an initial batch of images, you can train the model. Subsequent labeling of new images can benefit from the smart labeler tool in the portal, which can suggest not only the regions, but the classes of object they contain. Alternative labeling approaches Alternatively, you can use a custom or third-party labeling tool, or choose to label images manually, to take advantage of other features, such as assigning image labeling tasks to multiple team members. If you choose to use a labeling tool other than the Azure AI Custom Vision portal, you may need to adjust the output to match the measurement units expected by the Azure AI Custom Vision API. Bounding boxes are defined by four values that represent the left (X) and top (Y) coordinates of the top-left corner of the bounding box, and the width and height of the bounding box. These values are expressed as proportional values relative to the source image size. For example, consider this bounding box: Left: 0.1 Top: 0.5 Width: 0.5 Height: 0.25 This defines a box in which the left is located 0.1 (one tenth) from the left edge of the image, and the top is 0.5 (half the image height) from the top. The box is half the width and a quarter of the height of the overall image. The following image shows labeling information in JSON format for objects in an image.',\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/detect-objects-images/media/detected-objects.png',\n",
       "    'image_alt': 'Photograph with the location and type of fruits detected.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/detect-objects-images/media/tagged-images.png',\n",
       "    'image_alt': 'Screenshot of tagged images in the Azure AI Custom Vision portal.'},\n",
       "   {'image_name': 'image3',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/detect-objects-images/media/object-labels.png',\n",
       "    'image_alt': 'Photograph with JSON labels for the objects it contains.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Detect objects in images',\n",
       "  'topic': 'Develop an object detection client application',\n",
       "  'text': 'After you\\'ve trained an object detection model, you can use the Azure AI Custom Vision SDK to develop a client application that submits new images to be analyzed. from msrest.authentication import ApiKeyCredentials\\nfrom azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\\n\\n\\n # Authenticate a client for the prediction API\\ncredentials = ApiKeyCredentials(in_headers={\"Prediction-key\": \"<YOUR_PREDICTION_RESOURCE_KEY>\"})\\nprediction_client = CustomVisionPredictionClient(endpoint=\"<YOUR_PREDICTION_RESOURCE_ENDPOINT>\",\\n                                                 credentials=credentials)\\n\\n# Get classification predictions for an image\\nimage_data = open(\"<PATH_TO_IMAGE_FILE>\", \"rb\").read()\\nresults = prediction_client.detect_image(\"<YOUR_PROJECT_ID>\",\\n                                           \"<YOUR_PUBLISHED_MODEL_NAME>\",\\n                                           image_data)\\n\\n# Process predictions\\nfor prediction in results.predictions:\\n    if prediction.probability > 0.5:\\n        left = prediction.bounding_box.left\\n        top = prediction.bounding_box.top \\n        height = prediction.bounding_box.height\\n        width =  prediction.bounding_box.width\\n        print(f\"{prediction.tag_name} ({prediction.probability})\")\\n        print(f\"  Left:{left}, Top:{top}, Height:{height}, Width:{width}\") using System;\\nusing System.IO;\\nusing Microsoft.Azure.CognitiveServices.Vision.CustomVision.Prediction;\\n\\n// Authenticate a client for the prediction API\\nCustomVisionPredictionClient prediction_client = new CustomVisionPredictionClient(new ApiKeyServiceClientCredentials(\"<YOUR_PREDICTION_RESOURCE_KEY>\"))\\n{\\n    Endpoint = \"<YOUR_PREDICTION_RESOURCE_ENDPOINT>\"\\n};\\n\\n// Get classification predictions for an image\\nMemoryStream image_data = new MemoryStream(File.ReadAllBytes(\"<PATH_TO_IMAGE_FILE>\"));\\nvar result = prediction_client.DetectImage(\"<YOUR_PROJECT_ID>\",\\n                                             \"<YOUR_PUBLISHED_MODEL_NAME>\",\\n                                             image_data);\\n\\n// Process predictions\\nforeach (var prediction in result.Predictions)\\n{\\n    if (prediction.Probability > 0.5)\\n    {\\n        var left = prediction.BoundingBox.Left;\\n        var top = prediction.BoundingBox.Top;\\n        var height = prediction.BoundingBox.Height;\\n        var width =  prediction.BoundingBox.Width;\\n        Console.WriteLine($\"{prediction.TagName} ({prediction.Probability})\");\\n        Console.WriteLine($\"  Left:{left}, Top:{top}, Height:{height}, Width:{width}\");\\n    }\\n}',\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Detect objects in images',\n",
       "  'topic': 'Exercise - Detect objects in images',\n",
       "  'text': \"If you have an Azure subscription, you can use Azure AI Custom Vision to create a custom object detection model for yourself. In this exercise, you'll train an object detection model and test it from a client application. Note If you don't have an Azure subscription, and you want to explore Azure AI Studio, you can sign up for an account , which includes credits for the first 30 days. Launch the exercise and follow the instructions. Tip After completing the exercise, if you've finished exploring Azure AI services, delete the Azure resources that you created during the exercise.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2320011&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Detect objects in images',\n",
       "  'topic': 'Summary',\n",
       "  'text': 'In this module, you have learned how to use the Azure AI Custom Vision service to create object detection models. Tip To find out more about the Azure AI Custom Vision service, see the Azure AI Custom Vision documentation .',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Analyze video',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"It's increasingly common for organizations and individuals to generate content in video format. For example, you might use a cellphone to capture a live event, or you might record a teleconference that combines webcam footage and presentation of slides or documents. As a result, a great deal of information is encapsulated in video files, and you may need to extract this information for analysis or to support indexing for searchability. In this module, you will learn how to use the Azure Video Indexer service to analyze videos. After completing this module, you’ll be able to: Describe Azure Video Indexer capabilities. Extract custom insights. Use Azure Video Indexer widgets and APIs.\",\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Analyze video',\n",
       "  'topic': 'Understand Azure Video Indexer capabilities',\n",
       "  'text': 'The Azure Video Indexer service is designed to help you extract information from videos. It provides functionality that you can use for: Facial recognition - detecting the presence of individual people in the image. This requires Limited Access approval. Optical character recognition - reading text in the video. Speech transcription - creating a text transcript of spoken dialog in the video. Topics - identification of key topics discussed in the video. Sentiment - analysis of how positive or negative segments within the video are. Labels - label tags that identify key objects or themes throughout the video. Content moderation - detection of adult or violent themes in the video. Scene segmentation - a breakdown of the video into its constituent scenes. The Video Analyzer service provides a portal website that you can use to upload, view, and analyze videos interactively.',\n",
       "  'links': ['https://aka.ms/cog-services-limited-access'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-video/media/video-indexer-portal.png',\n",
       "    'image_alt': 'The Video Analyzer portal'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Analyze video',\n",
       "  'topic': 'Extract custom insights',\n",
       "  'text': 'Azure Video Indexer includes predefined models that can recognize well-known celebrities, do OCR, and transcribe spoken phrases into text. You can extend the recognition capabilities of Video Analyzer by creating custom models for: People . Add images of the faces of people you want to recognize in videos, and train a model. Video Indexer will then recognize these people in all of your videos. Note This only works after Limited Access approval, adhering to our Responsible AI standard. Language . If your organization uses specific terminology that may not be in common usage, you can train a custom model to detect and transcribe it. Brands . You can train a model to recognize specific names as brands, for example to identify products, projects, or companies that are relevant to your business.',\n",
       "  'links': ['https://aka.ms/cog-services-limited-access'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Analyze video',\n",
       "  'topic': 'Use Video Analyzer widgets and APIs',\n",
       "  'text': 'While you can perform all video analysis tasks in the Azure Video Indexer portal, you may want to incorporate the service into custom applications. There are two ways you can accomplish this. Azure Video Indexer widgets The widgets used in the Azure Video Indexer portal to play, analyze, and edit videos can be embedded in your own custom HTML interfaces. You can use this technique to share insights from specific videos with others without giving them full access to your account in the Azure Video Indexer portal. Azure Video Indexer API Azure Video Indexer provides a REST API that you can use to obtain information about your account, including an access token. https://api.videoindexer.ai/Auth/<location>/Accounts/<accountId>/AccessToken You can then use your token to consume the REST API and automate video indexing tasks, creating projects, retrieving insights, and creating or deleting custom models. For example, a GET call to https://api.videoindexer.ai/<location>/Accounts/<accountId>/Customization/CustomLogos/Logos/<logoId>?<accessToken> REST endpoint returns the specified logo. In another example, you can send a GET request to https://api.videoindexer.ai/<location>/Accounts/<accountId>/Videos?<accessToken> , which returns details of videos in your account, similar to the following JSON example: {\\n    \"accountId\": \"SampleAccountId\",\\n    \"id\": \"30e66ec1b1\",\\n    \"partition\": null,\\n    \"externalId\": null,\\n    \"metadata\": null,\\n    \"name\": \"test3\",\\n    \"description\": null,\\n    \"created\": \"2018-04-25T16=50=00.967+00=00\",\\n    \"lastModified\": \"2018-04-25T16=58=13.409+00=00\",\\n    \"lastIndexed\": \"2018-04-25T16=50=12.991+00=00\",\\n    \"privacyMode\": \"Private\",\\n    \"userName\": \"SampleUserName\",\\n    \"isOwned\": true,\\n    \"isBase\": true,\\n    \"state\": \"Processing\",\\n    \"processingProgress\": \"\",\\n    \"durationInSeconds\": 13,\\n    \"thumbnailVideoId\": \"30e66ec1b1\",\\n    \"thumbnailId\": \"55848b7b-8be7-4285-893e-cdc366e09133\",\\n    \"social\": {\\n        \"likedByUser\": false,\\n        \"likes\": 0,\\n        \"views\": 0\\n    },\\n    \"searchMatches\": [],\\n    \"indexingPreset\": \"Default\",\\n    \"streamingPreset\": \"Default\",\\n    \"sourceLanguage\": \"en-US\"\\n} Deploy with ARM template Azure Resource Manager (ARM) templates are available to create the Azure AI Video Indexer resource in your subscription, based on the parameters specified in the template file. For a full list of available APIs, see the Video Indexer Developer Portal .',\n",
       "  'links': ['https://api-portal.videoindexer.ai/'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-video/media/widgets.png',\n",
       "    'image_alt': 'Video Analyzer widgets in a custom web page'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Analyze video',\n",
       "  'topic': 'Exercise - Analyze video',\n",
       "  'text': \"Now it's your turn to try using the Azure AI Video Indexer service. In this exercise, you analyze a video using the Azure AI Video Indexer portal and its API. Note To complete this lab, you need an Azure subscription in which you have administrative access. Launch the exercise and follow the instructions. Tip After completing the exercise, if you've finished exploring Azure AI services, delete the Azure resources that you created during the exercise.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2320415&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Analyze video',\n",
       "  'topic': 'Summary',\n",
       "  'text': \"In this module, you learned how to use the Azure Video Indexer service to analyze videos. Now that you've completed this module, you can: Describe Azure Video Indexer capabilities. Extract custom insights. Use Azure Video Indexer widgets and APIs. To find out more about the Azure Video Indexer service, see the Azure Video Indexer documentation .\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/azure-video-indexer/'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Develop a vision-enabled generative AI application',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"Generative AI models enable you to develop chat-based applications that reason over and respond to input. Often this input takes the form of a text-based prompt, but increasingly multimodal models that can respond to visual input are becoming available. In this module, we'll discuss vision-enabled generative AI and explore how you can use Azure AI Foundry to create generative AI solutions that respond to prompts that include a mix of text and image data.\",\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Develop a vision-enabled generative AI application',\n",
       "  'topic': 'Deploy a multimodal model',\n",
       "  'text': 'To handle prompts that include images, you need to deploy a multimodal generative AI model - in other words, a model that supports not only text-based input, but image-based (and in some cases, audio-based) input as well. Multimodal models available in Azure AI Foundry include (among others): Microsoft Phi-4-multimodal-instruct OpenAI gpt-4o OpenAI gpt-4o-mini Tip To learn more about available models in Azure AI Foundry, see the Model catalog and collections in Azure AI Foundry portal article in the Azure AI Foundry documentation. Testing multimodal models with image-based prompts After deploying a multimodal model, you can test it in the chat playground in Azure AI Foundry portal. In the chat playground, you can upload an image from a local file and add text to the message to elicit a response from a multimodal model.',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/model-catalog-overview'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/develop-generative-ai-vision-apps/media/image-prompt.png',\n",
       "    'image_alt': 'Screenshot of the chat playground with an image-based prompt.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Develop a vision-enabled generative AI application',\n",
       "  'topic': 'Develop a vision-based chat app',\n",
       "  'text': 'To develop a client app that engages in vision-based chats with a multimodal model, you can use the same basic techniques used for text-based chats. You require a connection to the endpoint where the model is deployed, and you use that endpoint to submit prompts that consists of messages to the model and process the responses. The key difference is that prompts for a vision-based chat include multi-part user messages that contain both a text (or audio where supported) content item and an image content item. The JSON representation of a prompt that includes a multi-part user message looks something like this: { \\n    \"messages\": [ \\n        { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, \\n        { \"role\": \"user\", \"content\": [  \\n            { \\n                \"type\": \"text\", \\n                \"text\": \"Describe this picture:\" \\n            },\\n            { \\n                \"type\": \"image_url\",\\n                \"image_url\": {\\n                    \"url\": \"https://.....\"\\n                }\\n            }\\n        ] } \\n    ]\\n} The image content item can be: A URL to an image file in a web site. Binary image data When using binary data to submit a local image file, the image_url content takes the form of a base64 encoded value in a data URL format: {\\n    \"type\": \"image_url\",\\n    \"image_url\": {\\n       \"url\": \"data:image/jpeg;base64,<binary_image_data>\"\\n    }\\n} Depending on the model type, and where you deployed it, you can use Microsoft Azure AI Model Inference or OpenAI APIs to submit vision-based prompts. These libraries also provide language-specific SDKs that abstract the underlying REST APIs. In the exercise that follows in this module, you can use the Python or .NET SDK for the Azure AI Model Inference API and the OpenAI API to develop a vision-enabled chat application.',\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/develop-generative-ai-vision-apps/media/multi-part-prompt.png',\n",
       "    'image_alt': 'Diagram of a multi-part prompt being submitted to a model.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Develop a vision-enabled generative AI application',\n",
       "  'topic': 'Exercise - Develop a vision-enabled chat app',\n",
       "  'text': \"If you have an Azure subscription, you can complete this exercise to develop a vision-enabled chat app. Note If you don't have an Azure subscription, you can sign up for an account , which includes credits for the first 30 days. Launch the exercise and follow the instructions.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2318613&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Develop a vision-enabled generative AI application',\n",
       "  'topic': 'Summary',\n",
       "  'text': 'In this module, you learned about vision-enabled generative AI models and how to implement chat solutions that include image-based input. Vision-enabled models let you create AI solutions that can understand images and respond to related questions or instructions. Beyond just identifying objects in pictures, some models can also use reasoning based on what they see. For instance, they can interpret a chart or assess if an object is damaged. Tip For more information about working with multimodal models in Azure AI Foundry, see How to use image and audio in chat completions with Azure AI model inference and Quickstart: Use images in your AI chats .',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-foundry/model-inference/how-to/use-chat-multi-modal',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/openai/gpt-v-quickstart'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Generate images with AI',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"With Azure AI Foundry, you can use language models to generate content based on natural language prompts. Often the generated content is in the form of natural language text, but increasingly, models can generate other kinds of content. For example, the OpenAI DALL-E image generation model can create original graphical content based on a description of a desired image. The ability to use AI to generate graphics has many applications; including the creation of illustrations or photorealistic images for articles or marketing collateral, generation of unique product or company logos, or any scenario where a desired image can be described. In this module, you'll learn how to develop an application that uses generative AI to generate original images.\",\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/generate-images-azure-openai/media/image-generation.png',\n",
       "    'image_alt': 'Diagram of a prompt requesting a model to create an image.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Generate images with AI',\n",
       "  'topic': 'What are image-generation models?',\n",
       "  'text': \"Azure AI Foundry supports multiple models that are capable of generating images, including (but not limited to): DALL-E 3 GPT-Image 1 Tip For the latest information about model availability in Azure AI Foundry, view the model catalog. See Model catalog and collections in Azure AI Foundry portal for details. Image generation models are generative AI model that can create graphical data from natural language input. Put more simply, you can provide the model with a description and it can generate an appropriate image. For example, you might submit the following natural language prompt to an image generation model: A squirrel on a motorcycle This prompt could result in the generation of graphical output such as the following image: The images generated are original; they aren't retrieved from a curated image catalog. In other words, the model isn't a search system for finding appropriate images - it is an artificial intelligence (AI) model that generates new images based on the data on which it was trained.\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/model-catalog-overview?azure-portal=true'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/generate-images-azure-openai/media/squirrel-on-motorcycle.png',\n",
       "    'image_alt': 'A picture of a squirrel on a motorcycle.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Generate images with AI',\n",
       "  'topic': 'Explore image-generation models in Azure AI Foundry portal',\n",
       "  'text': 'To experiment with image generation models, you can create an Azure AI Foundry project and use the Images playground in Azure AI Foundry portal to submit prompts and view the resulting generated images. When using the playground, you can adjust the settings to control the output. For example, when using a DALL-E model you can specify: The resolution (size) of the generated images. Available sizes are 1024x1024 (which is the default value), 1792x1024 , or 1024x1792 . The image style to be generated (such as vivid or natural ). The image quality (choose from standard or hd ).',\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/generate-images-azure-openai/media/images-playground.png',\n",
       "    'image_alt': 'A screenshot of the Images playground in Azure AI Studio.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Generate images with AI',\n",
       "  'topic': 'Create a client application that uses an image generation model',\n",
       "  'text': 'You can use a REST API to consume DALL-E models from applications. Alternatively, you can use a language-specific SDK (for example, the OpenAI Python SDK or the Azure OpenAI .NET SDK) to abstract the REST methods. You initiate the image generation process by submitting a request to the service endpoint with the authorization key in the header. The request contains parameters describing the image-generation requirements. For example, parameters for a DALL-E model include: prompt : The description of the image to be generated. n : The number of images to be generated. DALL-E 3 only supports n=1. size : The resolution of the image(s) to be generated ( 1024x1024 , 1792x1024 , or 1024x1792 for DALL-E 3) quality Optional : The quality of the image ( standard or hd ). Defaults to standard . style Optional : The visual style of the image ( natural or vivid ). Defaults to vivid . For example, the following JSON could be submitted via the REST API to a DALL-E model, prompting it to generate an 1024 x 1024 image of a badger wearing a tuxedo: {\\n    \"prompt\": \"A badger wearing a tuxedo\",\\n    \"n\": 1,\\n    \"size\": \"1024x1024\",\\n    \"quality\": \"hd\", \\n    \"style\": \"vivid\"\\n} With DALL-E 3, the result from the request is processed synchronously with the response containing the URL for the generated image. The response is similar to the following JSON: {\\n    \"created\": 1686780744,\\n    \"data\": [\\n        {\\n            \"url\": \"<URL of generated image>\",\\n            \"revised_prompt\": \"<prompt that was used>\"\\n        }\\n    ]\\n} The data element includes the url value, which references a PNG image file generated from the prompt that you can then view or download. The response also contains a revised prompt that was used to generate the image, which was updated by the system to achieve the most desirable results. In this example, the image might look similar to the following image:',\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/generate-images-azure-openai/media/badger-wearing-tuxedo.png',\n",
       "    'image_alt': 'A picture of a badger wearing a tuxedo.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Generate images with AI',\n",
       "  'topic': 'Exercise - Generate images with AI',\n",
       "  'text': \"Now it's your chance to use generative AI to create images. In this exercise, you'll provision an Azure AI Foundry project and deploy a DALL-E model. Then, you'll explore image generation in the Azure AI Foundry portal. Finally, you'll use the Python or .NET SDK to consume the DALL-E model from a custom application. Launch the exercise and follow the instructions. Tip After completing the exercise, if you've finished exploring Azure AI Services, delete the Azure resources that you created during the exercise.\",\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2310727&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop computer vision solutions in Azure - Training',\n",
       "  'unit': 'Generate images with AI',\n",
       "  'topic': 'Summary',\n",
       "  'text': 'This module described image generation models, and how you can use them in Azure AI Foundry to generate images based on natural language prompts. You can explore image generation models using the Images playground in Azure AI Foundry portal, and you can use REST APIs or SDKs to build applications that generate new images. Tip To learn more about using DALL-E in the Azure OpenAI service, see Quickstart: Generate images with Azure OpenAI Service in the Azure OpenAI service documentation.',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/openai/dall-e-quickstart'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Create a multimodal analysis solution with Azure AI Content Understanding',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"Organizations today rely on information that is often locked up in content assets such as documents, images, videos, and audio recordings. Extracting information from this content can be challenging, laborious, and time-consuming, and organizations often need to build solutions based on multiple technologies for content analysis depending on the formats being used. Azure AI Content Understanding is a multimodal service that simplifies the creation of AI-powered analyzers that can extract information from content in practically any format. In this module, you'll explore the capabilities of Azure AI Content Understanding, and learn how to use it to build custom analyzers. Note Azure AI Content Understanding is currently in public preview. Details described in this module are subject to change.\",\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-content-ai/media/content-understanding.png',\n",
       "    'image_alt': 'Diagram of Azure AI Content Understanding extracting information from documents, audio files, videos, and images.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Create a multimodal analysis solution with Azure AI Content Understanding',\n",
       "  'topic': 'What is Azure AI Content Understanding?',\n",
       "  'text': 'Azure AI Content Understanding is a generative AI service that you can use to extract insights and data from multiple kinds of content. With Content Understanding, you can quickly build applications that analyze complex data and generate outputs that can be used to automate and optimize processes. Content Understanding is a component of Azure AI services. To use it, you need to provision an Azure AI services resource in your Azure subscription. You can develop and manage a Content Understanding solution: In the Azure AI Foundry portal By using the Content Understanding REST API Multimodal content analysis Content Understanding can extract information from common kinds of content, enabling you to use a single service with a straightforward and consistent development process to build multimodal content analysis solutions. Documents and forms You can use Content Understanding to analyze documents and forms and retrieve specific field values. For example, you could extract key data values from an invoice to automate payment processing. Images You can analyze images to infer information from visuals such as charts, identify physical defects in products or other items, detect the presence of specific objects or people, or determine other information visually. Audio Analysis of audio enables you to automate tasks like summarizing conference calls, determining sentiment of recorded customer conversations, or extracting key data from telephone messages. Video Video accounts for a large volume of the data captured today, and you can use Content Understanding to analyze and extract insights from video to support many scenarios. For example, to extract key points from video conference recordings, to summarize presentations, or to detect the presence of specific activity in security footage.',\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-content-ai/media/document-analysis.png',\n",
       "    'image_alt': 'Screenshot of an invoice being analyzed by Content Understanding.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-content-ai/media/image-analysis.png',\n",
       "    'image_alt': 'Screenshot of an image being analyzed by Content Understanding.'},\n",
       "   {'image_name': 'image3',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-content-ai/media/audio-analysis.png',\n",
       "    'image_alt': 'Screenshot of an audio file being analyzed by Content Understanding.'},\n",
       "   {'image_name': 'image4',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-content-ai/media/video-analysis.png',\n",
       "    'image_alt': 'Screenshot of an video file being analyzed by Content Understanding.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Create a multimodal analysis solution with Azure AI Content Understanding',\n",
       "  'topic': 'Create a Content Understanding analyzer',\n",
       "  'text': \"Content Understanding solutions are based on the creation of an analyzer ; which is trained to extract specific information from a particular type of content based on a schema that you define. The high-level process for creating a Content Understanding solution includes the following steps: Create an Azure AI services resource. Define a Content Understanding schema for the information to be extracted. This can be based on a content sample and an analyzer template. Build an analyzer based on the completed schema. Use the analyzer to extract or generate fields from new content. Numerous analyzer templates are provided to help you develop an appropriate analyzer for your needs quickly. Additionally, because of the generative AI capabilities of Content Understanding, you can use minimal training data to define a schema by example. In many cases, the service accurately identifies the data values in the sample content that map to the schema elements automatically, though you can also explicitly label fields in content such as documents to improve the performance of your analyzer. Creating an analyzer with Azure AI Foundry While you can provision an Azure AI services resource and develop a complete Content Understanding solution through the REST API, the preferred approach for AI development projects is to use Azure AI Foundry. Specifically, you can use the Azure AI Foundry portal to create a project, define a Content Understanding schema, and build and test an analyzer. Creating a Content Understanding project In Azure AI Foundry, you can create a project in an existing AI hub, or you can create a new hub as you create the project. In addition to the AI hub itself, creating a hub provisions the Azure resources needed to support one or more projects; including an Azure AI services resource, storage, and a key vault resource to store sensitive details like credentials and keys. Note Content Understanding schemas can only be created in Azure locations where the service is supported. For more information, see Content Understanding region and language support . Defining a schema After creating a project, the first step in building an analyzer is to define a schema for the content the analyzer will process, and the information it will extract. Azure AI Foundry provides a schema editor interface in which you can upload a file (document, image, audio, or video) on which the schema should be based. You can then apply an appropriate schema template and define the specific fields you want the analyzer to identify. Note The templates and field types available in a schema depend on the content type of the file on which the schema is based. Some content types support additional optional functionality, such as extracting barcodes and formulae from text in documents. For more information about using Content Understanding with different content types, see the following articles in the product documentation: Content Understanding document solutions Content Understanding image solutions Content Understanding audio solutions Content Understanding video solutions Testing You can test the analyzer schema at any time during the development process by running analysis on the sample file used to define the schema or other uploaded files. The test results include the extracted field values and the JSON format output returned by the analyzer to client applications. Building an analyzer When you're satisfied with the performance of your schema, you can build your analyzer. Building an analyzer makes it accessible to client applications through Content Understanding endpoint for the Azure AI services resource associated with your project. After building your analyzer, you can continue to test it in the Azure AI Foundry portal, and refine the schema to create new named versions with different capabilities.\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/language-region-support',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/document/overview',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/image/overview',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/audio/overview',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/video/overview'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-content-ai/media/analyzer.png',\n",
       "    'image_alt': 'Diagram of a sample document being used to create a schema for an analyzer that is consumed by a client application.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-content-ai/media/new-project.png',\n",
       "    'image_alt': 'Screenshot of the Create new project user interface in Azure AI Foundry.'},\n",
       "   {'image_name': 'image3',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-content-ai/media/define-schema.png',\n",
       "    'image_alt': 'Screenshot of the Define schema user interface in Azure AI Foundry.'},\n",
       "   {'image_name': 'image4',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-content-ai/media/document-analysis.png',\n",
       "    'image_alt': 'Screenshot of the Test analyzer user interface in Azure AI Foundry.'},\n",
       "   {'image_name': 'image5',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-content-ai/media/build-analyzer.png',\n",
       "    'image_alt': 'Screenshot of the Build analyzer user interface in Azure AI Foundry.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Create a multimodal analysis solution with Azure AI Content Understanding',\n",
       "  'topic': 'Use the Content Understanding REST API',\n",
       "  'text': 'The Content Understanding REST API provides a programmatic interface that you can use to create, manage, and consume analyzers. To use the REST API, your client application submits HTTP calls to the Content Understanding endpoint for your Azure AI services resource, passing one of the authorization keys in the header. You can obtain the endpoint and keys in the Azure portal or in the Azure AI Foundry portal. You can also use the Azure AI Foundry API to connect to the project and retrieve the endpoint and key for your Azure AI Services resource programmatically. Using the REST API to analyze content One of the most common uses of the REST API is to submit content to an existing analyzer that you have previously built, and retrieve the results of analysis. The analysis request returns an operation ID value that represents an asynchronous task. Your client application must then use another request to pass the operation ID back to the endpoint and retrieve the operation status - potentially polling multiple times until the operation is complete and the results are returned in JSON format. For example, to analyze a document, a client application might submit a POST request to the analyze function containing the following JSON body: POST {endpoint}/contentunderstanding/analyzers/{analyzer}:analyze?api-version={api version}\\n{\\n  \"url\": \"https://host.com/doc.pdf\"\\n} Note You can specify a URL for the content file location, or you can include the binary contents of the file. Assuming the request is authenticated and initiated successfully, the response will be similar to this example: Operation-Id: 1234abcd-1234-abcd-1234-abcd1234abcd\\nOperation-Location: {endpoint}/contentunderstanding/analyzers/{analyzer}/results/1234abcd-1234-abcd-1234-abcd1234abcd?api-version={api version}\\n{\\n  \"id\": \"1234abcd-1234-abcd-1234-abcd1234abcd\",\\n  \"status\": \"NotStarted\"\\n} Your client application must then use the operation ID that has been returned to check the status of the operation until it has succeeded (or failed) by submitting a GET request to the results function. GET {endpoint}/contentunderstanding/analyzers/{analyzer}/results/1234abcd-1234-abcd-1234-abcd1234abcd?api-version={api version} When the operation has completed successfully, the response contains a JSON payload representing the results of the analysis. The specific results depend on the content and schema. Note For more information about the Content Understanding REST API, see the reference documentation .',\n",
       "  'links': ['https://learn.microsoft.com/en-us/rest/api/contentunderstanding/operation-groups'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-content-ai/media/azure-ai-services.png',\n",
       "    'image_alt': 'Screenshot of the Test analyzer user interface in Azure AI Foundry.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Create a multimodal analysis solution with Azure AI Content Understanding',\n",
       "  'topic': 'Exercise - Extract information from multimodal content',\n",
       "  'text': \"Now it's time to put what you've learned about Content Understanding into practice! In this exercise, you use Azure AI Content Understanding to extract information from documents, images, audio files, and videos. Note To complete this lab, you need an Azure subscription in which you have administrative access. Launch the exercise and follow the instructions.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2320174&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Create a multimodal analysis solution with Azure AI Content Understanding',\n",
       "  'topic': 'Summary',\n",
       "  'text': 'Azure AI Content Understanding is a multimodal AI service that enables you to extract information from many different kinds of content. In this module, you learned how to use the Azure AI Foundry portal to create a Content Understanding project and build an analyzer. Note For more information about Azure AI Content Understanding, see Azure AI Content Understanding documentation .',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Create an Azure AI Content Understanding client application',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"Azure AI Content Understanding is a multimodal service that simplifies the creation of AI-powered analyzers that can extract information from multiple content formats, including documents, images, audio files, and videos. Tip To learn how to build Azure AI Content Understanding analyzers, complete the Create a multimodal analysis solution with Azure AI Content Understanding module. You can develop client applications that use Azure AI Content Understanding analyzers by using the Azure AI Content Understanding REST API; which is the focus of this module. In this module, you'll learn how to write code that uses the REST API to submit a content file to an analyzer and process the results. Note Azure AI Content Understanding is currently in public preview. Details described in this module are subject to change.\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/training/modules/analyze-content-ai/'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-content-ai-api/media/content-understanding.png',\n",
       "    'image_alt': 'Diagram of Azure AI Content Understanding and a client application.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Create an Azure AI Content Understanding client application',\n",
       "  'topic': 'Prepare to use the AI Content Understanding REST API',\n",
       "  'text': \"Before you can use the Azure AI Content Understanding REST API, you need an Azure AI services multi-services resource in your Azure subscription. You can provision this resource in the following ways: Create an Azure AI services resource in the Azure portal. Create an Azure AI Foundry hub, which includes an Azure AI services resource by default. Tip Creating an Azure AI Foundry hub enables you to work in an Azure AI Foundry project, in which you can use visual tools to create and manage Azure AI Content Understanding schemas and analyzers. After you've provisioned an Azure AI services resource, you need the following information to connect to the Azure AI Content Understanding REST API from a client application: The Azure AI services resource endpoint One of the API keys associated with the endpoint. You can obtain these values from the Azure portal, as shown in the following image: If you're working within an Azure AI Foundry project, you can find the endpoint and key for the associated Azure AI services resource in the Azure AI Foundry portal, as shown in the following image: When working in an Azure AI Foundry project, you can also write code that uses the Azure AI Foundry SDK to connect to the project using Microsoft Entra ID authentication, and retrieve the connection details for the Azure AI services resource; including the endpoint and key. Tip To learn more about programming with the Azure AI Foundry SDK, complete the Develop an AI app with the Azure AI Foundry SDK module.\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/training/modules/ai-foundry-sdk/'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-content-ai-api/media/azure-portal.png',\n",
       "    'image_alt': 'Screenshot of the Azure AI services settings in the Azure portal.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-content-ai-api/media/azure-ai-services.png',\n",
       "    'image_alt': 'Screenshot of the Azure AI services settings in the Azure AI Foundry portal.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Create an Azure AI Content Understanding client application',\n",
       "  'topic': 'Create a Content Understanding analyzer',\n",
       "  'text': 'In most scenarios, you should consider creating and testing analyzers using the visual interface in the Azure AI Foundry portal. However, in some cases you might want to create an analyzer by submitting a JSON definition of the schema for your desired content fields to the REST API. Defining a schema for an analyzer Analyzers are based on schemas that define the fields you want to extract or generate from a content file. At its simplest, a schema is a set of fields, which can be specified in a JSON document, as shown in this example of an analyzer definition: {\\n    \"description\": \"Simple business card\",\\n    \"baseAnalyzerId\": \"prebuilt-documentAnalyzer\",\\n    \"config\": {\\n        \"returnDetails\": true\\n    },\\n    \"fieldSchema\": {\\n        \"fields\": {\\n            \"ContactName\": {\\n                \"type\": \"string\",\\n                \"method\": \"extract\",\\n                \"description\": \"Name on business card\"\\n            },\\n            \"EmailAddress\": {\\n                \"type\": \"string\",\\n                \"method\": \"extract\",\\n                \"description\": \"Email address on business card\"\\n            }\\n        }\\n    }\\n} This example of a custom analyzer schema is based on the pre-built document analyzer, and describes two fields that you would expect to find on a business card: ContactName and EmailAddress . Both fields are defined as string data types, and are expected to be extracted from a document (in other words, the string values are expected to exist in the document so they can be \"read\"; rather than being fields that can be generated by inferring information about the document). Note This example is deliberately simple, with the minimal information needed to create a working analyzer. In reality, the schema would likely include more fields of different types, and the analyzer definition would include more configuration settings. The JSON might even include a sample document. See the Azure AI Content Understanding REST API documentation for more details. Using the REST API to create an analyzer With your analyzer definition in place, you can use the REST API to submit it to Azure AI Content Understanding to be created. The JSON data is submitted as a PUT request to the endpoint with the API key in the request header to start the analyzer creation operation. The response from the PUT request includes a Operation-Location in the header, which provides a callback URL that you can use to check on the status of the request by submitting a GET request. You can use any HTTP-capable client tool or language to submit the request. For example, the following Python code submits a request to create an analyzer based on the contents of a file named card.json (which is assumed to contain the JSON definition described previously) import json\\nimport requests\\n\\n# Get the buisness card schema\\nwith open(\"card.json\", \"r\") as file:\\n    schema_json = json.load(file)\\n\\n# Use a PUT request to submit the schema for a new analyzer\\nanalyzer_name = \"business_card_analyser\"\\n\\nheaders = {\\n    \"Ocp-Apim-Subscription-Key\": \"<YOUR_API_KEY>\",\\n    \"Content-Type\": \"application/json\"}\\n\\nurl = f\"{<YOUR_ENDPOINT>}/contentunderstanding/analyzers/{analyzer_name}?api-version=2025-05-01-preview\"\\n\\nresponse = requests.put(url, headers=headers, data=json.dumps(schema_json))\\n\\n# Get the response and extract the ID assigned to the operation\\ncallback_url = response.headers[\"Operation-Location\"]\\n\\n# Use a GET request to check the status of the operation\\nresult_response = requests.get(callback_url, headers=headers)\\n\\n# Keep polling until the operation is complete\\nstatus = result_response.json().get(\"status\")\\nwhile status == \"Running\":\\n    result_response = requests.get(callback_url, headers=headers)\\n    status = result_response.json().get(\"status\")\\n\\nprint(\"Done!\")',\n",
       "  'links': ['https://learn.microsoft.com/en-us/rest/api/contentunderstanding/content-analyzers/create-or-replace'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Create an Azure AI Content Understanding client application',\n",
       "  'topic': 'Analyze content',\n",
       "  'text': 'To analyze the contents of a file, you can use the Azure AI Content Understanding REST API to submit it to the endpoint using a POST request. You can specify the content as a URL (for a file hosted in an Internet-accessible location) or as the binary contents of the file (for example, a .pdf document, a .png image, an .mp3 audio file, or an .mp4 video file). The request header must include the API key, and the endpoint address for the analyze request includes the analyzer to be used. As with the request to create an analyzer, the analyze request starts an asynchronous operation. The POST request returns a unique operation ID, which you can then use in a GET request to check the status of the  analysis operation. For example, suppose you want to use the business card analyzer discussed previously to extract the name and email address from the following scanned business card image: The following Python code submits a request for analysis, and then polls the service until the operation is complete and the results are returned. import json\\nimport requests\\n\\n# Read the image data\\nwith open(\"business-card.png\", \"rb\") as file:\\n        image_data = file.read()\\n    \\n## Use a POST request to submit the image data to the analyzer\\nanalyzer_name = \"business_card_analyser\"\\n\\nheaders = {\\n        \"Ocp-Apim-Subscription-Key\": \"<YOUR_API_KEY>\",\\n        \"Content-Type\": \"application/octet-stream\"}\\n\\nurl = f\"{<YOUR_ENDPOINT>}/contentunderstanding/analyzers/{analyzer_name}:analyze?api-version=2025-05-01-preview\"\\n\\nresponse = requests.post(url, headers=headers, data=image_data)\\n\\n# Get the response and extract the ID assigned to the analysis operation\\nresponse_json = response.json()\\nid_value = response_json.get(\"id\")\\n\\n# Use a GET request to check the status of the analysis operation\\nresult_url = f\"{<YOUR_ENDPOINT>}/contentunderstanding/analyzerResults/{id_value}?api-version=2025-05-01-preview\"\\n\\nresult_response = requests.get(result_url, headers=headers)\\n\\n# Keep polling until the analysis is complete\\nstatus = result_response.json().get(\"status\")\\nwhile status == \"Running\":\\n        result_response = requests.get(result_url, headers=headers)\\n        status = result_response.json().get(\"status\")\\n\\n# Get the analysis results\\nif status == \"Succeeded\":\\n    result_json = result_response.json() Processing analysis results The results in the response JSON depend on: The kind of content the analyzer is designed to analyze (for example, document, video, image, or audio). The schema for the analyzer. The contents of the file that was analyzed. For example, the response from the document -based business card analyzer when analyzing the business card described previously contain: The extracted fields The optical character recognition (OCR) layout of the document, including locations of lines of text, individual words, and paragraphs on each page. Here\\'s the complete JSON response for the business card analysis: {\\n    \"id\": \"00000000-0000-0000-0000-a00000000000\",\\n    \"status\": \"Succeeded\",\\n    \"result\": {\\n        \"analyzerId\": \"biz_card_analyser_2\",\\n        \"apiVersion\": \"2025-05-01-preview\",\\n        \"createdAt\": \"2025-05-16T03:51:46Z\",\\n        \"warnings\": [],\\n        \"contents\": [\\n            {\\n                \"markdown\": \"John Smith\\\\nEmail: john@contoso.com\\\\n\",\\n                \"fields\": {\\n                    \"ContactName\": {\\n                        \"type\": \"string\",\\n                        \"valueString\": \"John Smith\",\\n                        \"spans\": [\\n                            {\\n                                \"offset\": 0,\\n                                \"length\": 10\\n                            }\\n                        ],\\n                        \"confidence\": 0.994,\\n                        \"source\": \"D(1,69,234,333,234,333,283,69,283)\"\\n                    },\\n                    \"EmailAddress\": {\\n                        \"type\": \"string\",\\n                        \"valueString\": \"john@contoso.com\",\\n                        \"spans\": [\\n                            {\\n                                \"offset\": 18,\\n                                \"length\": 16\\n                            }\\n                        ],\\n                        \"confidence\": 0.998,\\n                        \"source\": \"D(1,179,309,458,309,458,341,179,341)\"\\n                    }\\n                },\\n                \"kind\": \"document\",\\n                \"startPageNumber\": 1,\\n                \"endPageNumber\": 1,\\n                \"unit\": \"pixel\",\\n                \"pages\": [\\n                    {\\n                        \"pageNumber\": 1,\\n                        \"angle\": 0.03410444,\\n                        \"width\": 1000,\\n                        \"height\": 620,\\n                        \"spans\": [\\n                            {\\n                                \"offset\": 0,\\n                                \"length\": 35\\n                            }\\n                        ],\\n                        \"words\": [\\n                            {\\n                                \"content\": \"John\",\\n                                \"span\": {\\n                                    \"offset\": 0,\\n                                    \"length\": 4\\n                                },\\n                                \"confidence\": 0.992,\\n                                \"source\": \"D(1,69,234,181,234,180,283,69,283)\"\\n                            },\\n                            {\\n                                \"content\": \"Smith\",\\n                                \"span\": {\\n                                    \"offset\": 5,\\n                                    \"length\": 5\\n                                },\\n                                \"confidence\": 0.998,\\n                                \"source\": \"D(1,200,234,333,234,333,282,200,283)\"\\n                            },\\n                            {\\n                                \"content\": \"Email:\",\\n                                \"span\": {\\n                                    \"offset\": 11,\\n                                    \"length\": 6\\n                                },\\n                                \"confidence\": 0.995,\\n                                \"source\": \"D(1,75,310,165,309,165,340,75,340)\"\\n                            },\\n                            {\\n                                \"content\": \"john@contoso.com\",\\n                                \"span\": {\\n                                    \"offset\": 18,\\n                                    \"length\": 16\\n                                },\\n                                \"confidence\": 0.977,\\n                                \"source\": \"D(1,179,309,458,311,458,340,179,341)\"\\n                            }\\n                        ],\\n                        \"lines\": [\\n                            {\\n                                \"content\": \"John Smith\",\\n                                \"source\": \"D(1,69,234,333,233,333,282,69,282)\",\\n                                \"span\": {\\n                                    \"offset\": 0,\\n                                    \"length\": 10\\n                                }\\n                            },\\n                            {\\n                                \"content\": \"Email: john@contoso.com\",\\n                                \"source\": \"D(1,75,309,458,309,458,340,75,340)\",\\n                                \"span\": {\\n                                    \"offset\": 11,\\n                                    \"length\": 23\\n                                }\\n                            }\\n                        ]\\n                    }\\n                ],\\n                \"paragraphs\": [\\n                    {\\n                        \"content\": \"John Smith Email: john@contoso.com\",\\n                        \"source\": \"D(1,69,233,458,233,458,340,69,340)\",\\n                        \"span\": {\\n                            \"offset\": 0,\\n                            \"length\": 34\\n                        }\\n                    }\\n                ],\\n                \"sections\": [\\n                    {\\n                        \"span\": {\\n                            \"offset\": 0,\\n                            \"length\": 34\\n                        },\\n                        \"elements\": [\\n                            \"/paragraphs/0\"\\n                        ]\\n                    }\\n                ]\\n            }\\n        ]\\n    }\\n} Your application must typically parse the JSON to retrieve field values. For example, the following python code extracts all of the string values: # (continued from previous code example)\\n\\n# Iterate through the fields and extract the names and type-specific values\\ncontents = result_json[\"result\"][\"contents\"]\\nfor content in contents:\\n    if \"fields\" in content:\\n        fields = content[\"fields\"]\\n        for field_name, field_data in fields.items():\\n            if field_data[\\'type\\'] == \"string\":\\n                print(f\"{field_name}: {field_data[\\'valueString\\']}\") The output from this code is shown here: ContactName: John Smith\\nEmailAddress: john@contoso.com',\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-content-ai-api/media/business-card.png',\n",
       "    'image_alt': 'Photograph of a business card for John Smith.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Create an Azure AI Content Understanding client application',\n",
       "  'topic': 'Exercise - Develop a Content Understanding client application',\n",
       "  'text': \"Now it's your turn to build your own Content Understanding client application! In this exercise, you use the Azure AI Content Understanding REST API to extract information from content by submitting a file to an analyzer. Note To complete this lab, you need an Azure subscription in which you have administrative access. Launch the exercise and follow the instructions.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2320464&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Create an Azure AI Content Understanding client application',\n",
       "  'topic': 'Summary',\n",
       "  'text': 'Azure AI Content Understanding is a multimodal AI service that enables you to extract information from many different kinds of content. The REST API for the service enables you to create client applications that analyze content to extract and generate field values. Note For more information about Azure AI Content Understanding, see Azure AI Content Understanding documentation .',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Use prebuilt Document intelligence models',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"Many forms and documents that businesses handle are common across disparate companies in different sectors. For example, most companies use invoices and receipts. Microsoft Azure AI Document Intelligence includes prebuilt models so you can handle common document types easily. Imagine you conduct polls for private companies and political parties. Participants submit their responses as paper forms or as online PDFs. You've decided to deploy Azure AI Document Intelligence to streamline data entry. You want to know if you can use the prebuilt models to generate meaningful data from your forms. In this module, you'll learn about the capabilities of the prebuilt models in Azure AI Document Intelligence and how to use them. Learning objectives At the end of this module, you'll be able to: Identify business problems that you can solve by using prebuilt models in Azure AI Document Intelligence. Analyze forms by using the General Document, Read, and Layout models. Analyze forms by using financial, ID, and tax prebuilt models.\",\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Use prebuilt Document intelligence models',\n",
       "  'topic': 'Understand prebuilt models',\n",
       "  'text': 'Prebuilt models in Azure AI Document Intelligence enable you to extract data from common forms and documents without training your own models. In your polling company, polling forms are unique to each survey project, but you also use invoices and receipts to record financial transactions and you have many unstructured documents. You want to know how much work is required to extract names, addresses, amounts, and other information from these documents. Here, you learn how prebuilt models can help you analyze common document types. What are prebuilt models? The general approach used in AI solutions is to provide a large quantity of sample data and then train an optimized model by trying different data features, parameters, and statistical treatments. The combination that best predicts the values that interest you constitute the trained model, and you can use this model to predict values from new data. Many of the forms that businesses use from day to day are of a few common types. For example, most businesses issue or receive invoices and receipts. Any business that has employees in the United States must use the W-2 tax declaration form. Also you often have more general documents that you might want to extract data from. For these cases, Microsoft helps you by providing prebuilt models. Prebuilt models are already trained on large numbers of their target form type. If you want to use Document Intelligence to extract data from one of these common forms or documents, you can choose to use a prebuilt model and you don\\'t have to train your own. Because Microsoft trains these models on a large corpus of examples, you can expect them to provide accurate and reliable results when dealing with their intended forms. Several of the prebuilt models are trained on specific form types: Invoice model. Extracts common fields and their values from invoices. Receipt model. Extracts common fields and their values from receipts. US Tax model. Unified US tax model that can extract from forms such as W-2, 1098, 1099, and 1040. ID document model. Extracts common fields and their values from US drivers\\' licenses, European Union IDs and drivers license, and international passports. Business card model. Extracts common fields and their values from business cards. Health insurance card model. Extracts common fields and their values from health insurance cards. Marriage certificate. Extracts information from marriage certificates. Credit/Debit card model. Extracts common information from bank cards. Mortgage documents. Extracts information from mortgage closing disclosure, Uniform Residential Loan Application (Form 1003), Appraisal (Form 1004), Validation of Employment (Form 1005), and Uniform Underwriting and Transmittal Summary (Form 1008). Bank statement model. Extracts account information including beginning and ending balances, transaction details from bank statements. Pay Stub model. Extracts wages, hours, deductions, net pay, and other common pay stub fields. Check model. Extracts payee, amount, date, and other relevant information from checks. The other models are designed to extract values from documents with less specific structures: Read model. Extracts text and languages from documents. General document model. Extract text, keys, values, entities, and selection marks from documents. Layout model. Extracts text and structure information from documents. Features of prebuilt models The prebuilt models are designed to extract different types of data from the documents and forms users submit. To select the right model for your requirements, you must understand these features: Text extraction. All the prebuilt models extract lines of text and words from hand-written and printed text. Key-value pairs. Many models extract spans of text within a document that identify a label or key and its response or value as key-values pairs. For example, a typical key might be Weight and its value might be 31 kg . Entities. Text that includes common, more complex data structures can be extracted as entities. Entity types include people, locations, and dates. Selection marks. Some models extract spans of text that indicate a choice as selection marks. These marks include radio buttons and check boxes. Tables. Many models can extract tables in scanned forms included the data contained in cells, the numbers of columns and rows, and column and row headings. Tables with merged cells are supported. Fields. Models trained for a specific form type identify the values of a fixed set of fields. For example, the Invoice model includes CustomerName and InvoiceTotal fields. Also consider that prebuilt models are designed for and trained on generic document and form types. If you have an industry-specific or unique form type that you use often, you might be able to obtain more reliable and predictable results by using a custom model. However, custom models take time to develop because you must invest the time and resources to train them on example forms before you can use it. The larger the number of example forms you provide for training, the better the model is at predicting form content accurately. Input requirements The prebuilt models are flexible but you can help them to return accurate and helpful results by submitting one clear photo or high-quality scan for each document. You must also comply with these requirements when you submit a form for analysis: The file must be in JPEG, PNG, BMP, TIFF, or PDF format. Additionally, the Read model can accept Microsoft Office files. The file must be smaller than 500 MB for the standard tier, and 4 MB for the free tier. Images must have dimensions between 50 x 50 pixels and 10,000 x 10,000 pixels. PDF documents must have dimensions less than 17 x 17 inches or A3 paper size. PDF documents must not be protected with a password. Note If you can, submit text-embedded PDF files because they eliminate errors in character recognition. PDF and TIFF files can have any number of pages but, in the standard tier, only the first 2,000 pages are analyzed. In the free tier, only the first two pages are analyzed. Try out prebuilt models with Azure AI Document Intelligence Studio Azure AI Document Intelligence is designed as a web service you can call using code in your custom applications. However, it\\'s often helpful to explore the models and how they behave with your forms visually. You can perform such experiments by using Azure AI Document Intelligence Studio and use the experience to help design and write your code. You can choose any of the prebuilt models in Azure AI Document Intelligence Studio. Microsoft provides some sample documents for use with each model or you can add your own documents and analyze them. Calling prebuilt models by using APIs Because Azure AI Document Intelligence implements RESTful web services, you can use web service calls from any language that supports them. However, when you use Microsoft\\'s Azure AI Document Intelligence APIs, security and session management is simplified and you have to write less code. Azure AI Document Intelligence is available for: C# and other .NET languages. Java. Python. JavaScript. Whenever you want to call Azure AI Document Intelligence, you must start by connecting and authenticating with the service in your Azure subscription. To make that connection, you need: The service endpoint. This value is the URL where the service is published. The API key. This value is a unique key that grants access. You obtain both of these values from the Azure portal. Because the service can take a few seconds to respond, it\\'s best to use asynchronous calls to submit a form and then obtain results from the analysis: AnalyzeDocumentOperation operation = await client.AnalyzeDocumentFromUriAsync(WaitUntil.Completed, \"prebuilt-layout\", fileUri);\\n\\nAnalyzeResult result = operation.Value; poller = document_analysis_client.begin_analyze_document(\\n        \"prebuilt-layout\", AnalyzeDocumentRequest(url_source=docUrl\\n    ))\\nresult: AnalyzeResult = poller.result() The details you can extract from these results depend on the model you used. Learn more What is Azure AI Document Intelligence? Azure AI Document Intelligence models',\n",
       "  'links': ['https://formrecognizer.appliedai.azure.com/studio',\n",
       "   'https://learn.microsoft.com/wwl-data-ai/use-prebuilt-form-recognizer-models/media/2-studio-business-card-example.png#lightbox',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/overview',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept-model-overview'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/use-prebuilt-form-recognizer-models/media/2-studio-business-card-example.png',\n",
       "    'image_alt': 'Screenshot showing how to use Azure AI Document Intelligence Studio to explore the business card prebuilt model.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Use prebuilt Document intelligence models',\n",
       "  'topic': 'Use the General Document, Read, and Layout models',\n",
       "  'text': \"If you want to extract text, languages, and other information from documents with unpredictable structures, you can use the read, general document, or layout models. In your polling company, customers and partners often send specifications, tenders, statements of work, and other documents with unpredictable structures. You want to know if Azure AI Document Intelligence can analyze and extract values from these documents. Here, you'll learn about the prebuilt models that Microsoft provides for general documents. Using the read model The Azure AI Document Intelligence read model extracts printed and handwritten text from documents and images. It's used to provide text extraction in all the other prebuilt models. The read model can also detect the language that a line of text is written in and classify whether it's handwritten or printed text. Note The read model supports more languages for printed text than handwritten text. Check the documentation to see the current list of supported languages. For multi-page PDF or TIFF files, you can use the pages parameter in your request to fix a page range for the analysis. The read model is ideal if you want to extract words and lines from documents with no fixed or predictable structure. Using the general document model The general document model extends the functionality of the read model by adding the detection of key-value pairs, entities, selection marks, and tables. The model can extract these values from structured, semi-structured, and unstructured documents. The general document model is the only prebuilt model to support entity extraction. It can recognize entities such as people, organizations, and dates and it runs against the whole document, not just key-value pairs. This approach ensures that, when structural complexity has prevented the model extracting a key-value pair, an entity can be extracted instead. Remember, however, that sometimes a single piece of text might return both a key-value pair and an entity. The types of entities you can detect include: Person . The name of a person. PersonType . A job title or role. Location . Buildings, geographical features, geopolitical entities. Organization . Companies, government bodies, sports clubs, musical bands, and other groups. Event . Social gatherings, historical events, anniversaries. Product . Objects bought and sold. Skill . A capability belonging to a person. Address . Mailing address for a physical location. Phone number . Dialing codes and numbers for mobile phones and landlines. Email . Email addresses. URL . Webpage addresses. IP Address . Network addresses for computer hardware. DateTime . Calendar dates and times of day. Quantity . Numerical measurements with their units. Using the layout model As well as extracting text, the layout model returns selection marks and tables from the input image or PDF file. It's a good model to use when you need rich information about the structure of a document. When you digitize a document, it can be at an odd angle. Tables can have complicated structures with or without headers, cells that span columns or rows, and incomplete columns or rows. The layout model can handle all of these difficulties to extract the complete document structure. For example, each table cell is extracted with: Its content text. The size and position of its bounding box. If it's part of a header column. Indexes to indicate its row and column position in the table. Selection marks are extracted with their bounding box, a confidence indicator, and whether they're selected or not. Learn more Language support for Azure AI Document Intelligence Azure AI Document Intelligence read model Azure AI Document Intelligence general document model Azure AI Document Intelligence layout model\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/language-support-ocr#read-model',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/language-support',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept-read',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept-general-document',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept-layout'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Use prebuilt Document intelligence models',\n",
       "  'topic': 'Use financial, ID, and tax models',\n",
       "  'text': \"Azure AI Document Intelligence includes some prebuilt models that are trained on common form types. You can use these models to obtain the values of common fields from invoices, receipts, business cards, and more. In your polling company, invoices and receipts are often submitted as photos or scans of the paper documents. Sometimes the scan is poor and the paper is creased or damaged. You want to know if Azure AI Document Intelligence can get this information into your databases more efficiently than manual data entry. Here, you'll learn about the prebuilt models that handle financial, identity, and tax documents. Using the invoice model Your business both issues invoices and receives them from partner organization. There might be many different formats on paper or in digitized forms and some will have been scanned poorly at odd angles or from creased paper. The invoice model in Azure AI Document Intelligence can handle these challenges and uses the features of the read model to extract text from invoice scans. In addition, it extracts specific fields that are commonly used on invoices including: Customer name and reference ID Purchase order number Invoice and due dates Details about the vendor, such as name, tax ID, physical address. Similar details about the customer. Billing and shipping addresses. Amounts such as total tax, invoice total, and amount due. Invoices also feature lines, usually in a table, each of which is one purchased item. For each line, the invoice model identifies details including: The description and product code of the product or service invoiced. Amounts such as the unit price, the quantity of items, the tax incurred, and the line total. Using the receipt model Receipts have similar fields and structures to invoices, but they record amounts paid instead of amounts charged. Azure AI Document Intelligence faces the same challenges of poor scanning or digitization but can reliably identify fields including: Merchant details such a name, phone number, and address. Amounts such as receipt total, tax, and tip. The date and time of the transaction. As for invoices, receipts often include a table of items, each of which is a product or service purchased. For each of these lines, the model recognizes: The name of the item. The quantity of the item purchased. The unit price of the item. The total price for that quantity. Note In Azure AI Document Intelligence v3.0 and later, the receipt model supports single-page hotel receipt processing. If a receipt is classified as a hotel receipt, the model extracts extra relevant fields such as arrival and departure dates. Using the ID document model The ID document model is trained to analyze two types of identity document: United States drivers licenses. International passports. Note Only the biographical pages of passports can be analyzed. Visas and other travel documents are not supported. The ID document model can extract fields including: First and last names. Personal information such as sex, date of birth, and nationality. The country and region where the document was issued. Unique numbers such as the document number and machine readable zone. Endorsements, restrictions, and vehicle classifications. Important Since much of the data extracted by the ID document model is personal, it is of a sensitive nature and covered by data protection laws in most jurisdictions. Be sure that you have the permission of the individual to store their data and comply with all legal requirements in the way you handle this information. Using the business card model Business cards are a popular way to exchange contact information quickly and often include branding, unusual fonts, and graphic design elements. Fields that the business card model can extract include: First and last names. Postal addresses. Email and website addresses. Various telephone numbers. Using other prebuilt models Azure AI Document Intelligence offers several prebuilt models, with new models being released regularly. Before training a custom model, it's worth verifying if your use case can be analyzed accurately with one of these prebuilt models. Using a prebuilt model will benefit from rigorous testing, updated model versions, and reduced cost compared to a custom model. Learn more Azure AI Document Intelligence model overview\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept-model-overview'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Use prebuilt Document intelligence models',\n",
       "  'topic': 'Exercise - Analyze a document using Azure AI Document Intelligence',\n",
       "  'text': \"In this exercise, you use Azure AI Document Intelligence to analyze forms using a prebuilt model. Note To complete this lab, you need an Azure subscription in which you have administrative access. Launch the exercise and follow the instructions. Tip After completing the exercise, if you've finished exploring Azure AI services, delete the Azure resources that you created during the exercise.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2321470&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Use prebuilt Document intelligence models',\n",
       "  'topic': 'Summary',\n",
       "  'text': \"There are many document types that are common to most business and Azure AI Document Intelligence includes prebuilt models to handle them. If you have a collection of such forms that you want to analyze, you can extract data by using these prebuilt models and you don't have to train your own models. You can get up and running very quickly by submitting photos and scans to the most appropriate prebuilt model. Now that you've completed this module, you can: Identify business problems that you can solve by using prebuilt models in Azure AI Document Intelligence. Analyze forms by using the General Document, Read, and Layout models. Analyze forms by using financial, ID, and tax prebuilt models. Learn more What is Azure AI Document Intelligence? Azure AI Document Intelligence models Language support for Azure AI Document Intelligence Azure AI Document Intelligence read model Azure AI Document Intelligence general document model Azure AI Document Intelligence layout model Azure AI Document Intelligence invoice model Azure AI Document Intelligence receipt model Azure AI Document Intelligence ID document model Azure AI Document Intelligence business card model Azure AI Document Intelligence W-2 model\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/overview',\n",
       "   'https://learn.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/concept-model-overview',\n",
       "   'https://learn.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/language-support',\n",
       "   'https://learn.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/concept-read',\n",
       "   'https://learn.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/concept-general-document',\n",
       "   'https://learn.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/concept-layout',\n",
       "   'https://learn.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/concept-invoice',\n",
       "   'https://learn.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/concept-receipt',\n",
       "   'https://learn.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/concept-id-document',\n",
       "   'https://learn.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/concept-business-card',\n",
       "   'https://learn.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/concept-w2'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Extract data from forms with Azure Document intelligence',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"Forms are used to communicate information in every industry, every day. Many people still manually extract data from forms to exchange information. Consider some of the instances when a person needs to process form data: When filing claims When enrolling new patients in an online management system When entering data from receipts to an expense report When reviewing an operations report for anomalies When selecting data from a report to give to a stakeholder Without AI services, people need to manually sort through form documents to identify important information and then manually reenter data to record it. Some may also need to complete these tasks in real-time with a customer. Azure Document Intelligence services provide the building blocks for automation by using intelligent services to extract data at scale and with accuracy. Azure Document Intelligence is a Vision API that extracts key-value pairs and table data from form documents. Uses of the Azure Document Intelligence service include : Process automation Knowledge mining Industry-specific applications In this module, you'll learn how to : Identify how Azure Document Intelligence's document analysis, prebuilt, and custom models can automate processes Use Azure Document Intelligence's Optical Character Recognition (OCR) capabilities with SDKs and REST API Develop and test a custom Azure Document Intelligence model To complete this module, you'll need a Microsoft Azure subscription. If you don't already have one, you can sign up for a free trial at https://azure.microsoft.com .\",\n",
       "  'links': ['https://azure.microsoft.com?azure-portal=true'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/work-form-recognizer/media/introduction-image.png',\n",
       "    'image_alt': 'Screenshot of person handing a form to another person.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Extract data from forms with Azure Document intelligence',\n",
       "  'topic': 'What is Azure Document Intelligence?',\n",
       "  'text': \"Azure Document Intelligence is one of many Azure AI Services , cloud-based artificial intelligence (AI) services with REST APIs and client library SDKs that can be used to build intelligence into your applications. Azure Document Intelligence uses Optical Character Recognition (OCR) capabilities and deep learning models to extract text, key-value pairs, selection marks, and tables from documents. OCR captures document structure by creating bounding boxes around detected objects in an image. The locations of the bounding boxes are recorded as coordinates in relation to the rest of the page. Azure Document Intelligence services return bounding box data and other information in a structured form with the relationships from the original file. To build a high-accuracy model from scratch, people need to build deep learning models, use a large amount of compute resources, and face long model training times. These factors could make a project infeasible. Azure Document Intelligence provides underlying models that have been trained on thousands of form examples. The underlying models enable you to do high-accuracy data extraction from your forms with little to no model training. Azure Document Intelligence service components Azure Document Intelligence is composed of the following services: Document analysis models : which take an input of JPEG, PNG, PDF, and TIFF files and return a JSON file with the location of text in bounding boxes, text content, tables, selection marks (also known as checkboxes or radio buttons), and document structure. Prebuilt models : which detect and extract information from document images and return the extracted data in a structured JSON output. Azure Document Intelligence currently supports prebuilt models for several forms, including: W-2 forms Invoices Receipts ID documents Business cards Custom models : custom models extract data from forms specific to your business. Custom models can be trained through the Azure Document Intelligence Studio . Note Some Azure Document Intelligence features are in preview, as of the time this content was authored, and as a result, features and usage details might change. You should refer to the official page for up-to-date information. Access services You can access Azure Document Intelligence services in several ways. These options include using: A REST API Client library SDKs Azure Document Intelligence Studio Azure AI Foundry Tip This module's exercise focuses on the Python and .NET SDKs. The underlying REST services can be used by any language. Check out the documentation for quick start guides on all the available SDKs and the REST API.\",\n",
       "  'links': ['https://formrecognizer.appliedai.azure.com/studio',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/overview?azure-portal=true',\n",
       "   'https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/quickstarts/get-started-sdks-rest-api'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/work-form-recognizer/media/how-optical-character-recognition-works.png',\n",
       "    'image_alt': 'Screenshot of how OCR works.'},\n",
       "   {'image_name': 'image2',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/work-form-recognizer/media/json-output-sample.png',\n",
       "    'image_alt': 'Screenshot of JSON output sample.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Extract data from forms with Azure Document intelligence',\n",
       "  'topic': 'Get started with Azure Document Intelligence',\n",
       "  'text': \"To start a project with Azure Document Intelligence services, you need an Azure resource and selection of form files for data extraction. Subscribe to a resource You can access Azure Document Intelligence services via: An Azure AI Service resource : a multi-service subscription key (used across multiple Azure AI Services) OR An Azure Document Intelligence resource : a single-service subscription key (used only with a specific Azure AI Service) Note Create an Azure AI Services resource if you plan to access multiple Azure AI services under a single endpoint/key. For Azure Document Intelligence access only, create an Azure Document Intelligence resource. Note that you need a single-service resource if you intend to use Microsoft Entra authentication. You can subscribe to a service in the Azure portal or with the Azure Command Line Interface (CLI). You can learn more about the CLI commands here . Understand Azure Document Intelligence file input requirements Azure Document Intelligence works on input documents that meet these requirements: Format must be JPG, PNG, BMP, PDF (text or scanned), or TIFF. The file size must be less than 500 MB for paid (S0) tier and 4 MB for free (F0) tier. Image dimensions must be between 50 x 50 pixels and 10,000 x 10,000 pixels. The total size of the training data set must be 500 pages or less. More input requirements can be found in the documentation for specific models. Decide what component of Azure Document Intelligence to use After you collect your files, decide what you need to accomplish. Use case Recommended features to use Use OCR capabilities to capture document analysis Use the Layout model , Read model , or General Document model . Create an application that extracts data from W-2s, Invoices, Receipts, ID documents, Health insurance, vaccination, and business cards Use a prebuilt model . These models don't need to be trained. Azure Document Intelligence services analyze the documents and return a JSON output. Create an application to extract data from your industry-specific forms Create a custom model. This model needs to be trained on sample documents. After you train the custom model, it can analyze new documents and return a JSON output.\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/cli/azure/cognitiveservices/account#commands',\n",
       "   'https://learn.microsoft.com/en-us/azure/cognitive-services/form-recognizer/overview',\n",
       "   'https://learn.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/concept-model-overview#layout',\n",
       "   'https://learn.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/concept-model-overview#read-preview',\n",
       "   'https://learn.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/concept-model-overview#general-document-preview'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Extract data from forms with Azure Document intelligence',\n",
       "  'topic': 'Train custom models',\n",
       "  'text': \"Azure's Azure Document Intelligence service supports supervised machine learning. You can train custom models and create composite models with form documents and JSON documents that contain labeled fields. To train a custom model: Store sample forms in an Azure blob container, along with JSON files containing layout and label field information. You can generate an ocr.json file for each sample form using the Azure Document Intelligence's Analyze document function. Additionally, you need a single fields.json file describing the fields you want to extract, and a labels.json file for each sample form mapping the fields to their location in that form. Generate a shared access security (SAS) URL for the container. Use the Build model REST API function (or equivalent SDK method). Use the Get model REST API function (or equivalent SDK method) to get the trained model ID . OR Use the Azure Document Intelligence Studio to label and train. There are two types of underlying models for custom forms custom template models or custom neural models . Custom template models accurately extract labeled key-value pairs, selection marks, tables, regions, and signatures from documents. Training only takes a few minutes, and more than 100 languages are supported. Custom neural models are deep learned models that combine layout and language features to accurately extract labeled fields from documents.This model is best for semi-structured or unstructured documents.\",\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/work-form-recognizer/media/labeled-form-documents.png',\n",
       "    'image_alt': 'Screenshot of a sample form document needed for custom model training.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Extract data from forms with Azure Document intelligence',\n",
       "  'topic': 'Use Azure Document Intelligence models',\n",
       "  'text': 'Using the API To extract form data using a custom model, use the analyze document function of either a supported SDK, or the REST API, while supplying model ID (generated during model training). This function starts the form analysis. which you can then request the result to get the analysis. Example code to call your model: C# string endpoint = \"<endpoint>\";\\nstring apiKey = \"<apiKey>\";\\nAzureKeyCredential credential = new AzureKeyCredential(apiKey);\\nDocumentAnalysisClient client = new DocumentAnalysisClient(new Uri(endpoint), credential);\\n\\nstring modelId = \"<modelId>\";\\nUri fileUri = new Uri(\"<fileUri>\");\\n\\nAnalyzeDocumentOperation operation = await client.AnalyzeDocumentFromUriAsync(WaitUntil.Completed, modelId, fileUri);\\nAnalyzeResult result = operation.Value; Python endpoint = \"YOUR_DOC_INTELLIGENCE_ENDPOINT\"\\nkey = \"YOUR_DOC_INTELLIGENCE_KEY\"\\n\\nmodel_id = \"YOUR_CUSTOM_BUILT_MODEL_ID\"\\nformUrl = \"YOUR_DOCUMENT\"\\n\\ndocument_analysis_client = DocumentAnalysisClient(\\n    endpoint=endpoint, credential=AzureKeyCredential(key)\\n)\\n\\n# Make sure your document\\'s type is included in the list of document types the custom model can analyze\\ntask = document_analysis_client.begin_analyze_document_from_url(model_id, formUrl)\\nresult = task.result() A successful JSON response contains analyzeResult that contains the content extracted and an array of pages containing information about the document content. Example analyze document JSON response: {\\n\\t\"status\": \"succeeded\",\\n\\t\"createdDateTime\": \"2023-10-18T23:39:50Z\",\\n\\t\"lastUpdatedDateTime\": \"2023-10-18T23:39:54Z\",\\n\\t\"analyzeResult\": {\\n\\t\\t\"apiVersion\": \"2022-08-31\",\\n\\t\\t\"modelId\": \"DocIntelModel\",\\n\\t\\t\"stringIndexType\": \"utf16CodeUnit\",\\n\\t\\t\"content\": \"Purchase Order\\\\nHero Limited\\\\nCompany Phone: 555-348-6512 Website: www.herolimited.com Email: accounts@herolimited.com\\\\nPurchase Order\\\\nDated As: 12/20/2020 Purchase Order #: 948284\\\\nShipped To Vendor Name: Balozi Khamisi Company Name: Higgly Wiggly Books Address: 938 NE Burner Road Boulder City, CO 92848 Phone: 938-294-2949\\\\nShipped From Name: Kidane Tsehaye Company Name: Jupiter Book Supply Address: 383 N Kinnick Road Seattle, WA 38383\\\\nPhone: 932-299-0292\\\\nDetails\\\\nQuantity\\\\nUnit Price\\\\nTotal\\\\nBindings\\\\n20\\\\n1.00\\\\n20.00\\\\nCovers Small\\\\n20\\\\n1.00\\\\n20.00\\\\nFeather Bookmark\\\\n20\\\\n5.00\\\\n100.00\\\\nCopper Swirl Marker\\\\n20\\\\n5.00\\\\n100.00\\\\nSUBTOTAL\\\\n$140.00\\\\nTAX\\\\n$4.00\\\\nTOTAL\\\\n$144.00\\\\nKidane Tsehaye\\\\nManager\\\\nKidane Tsehaye\\\\nAdditional Notes: Do not Jostle Box. Unpack carefully. Enjoy. Jupiter Book Supply will refund you 50% per book if returned within 60 days of reading and offer you 25% off you next total purchase.\",\\n\\t\\t\"pages\": [\\n\\t\\t\\t{\\n\\t\\t\\t\\t\"pageNumber\": 1,\\n\\t\\t\\t\\t\"angle\": 0,\\n\\t\\t\\t\\t\"width\": 1159,\\n\\t\\t\\t\\t\"height\": 1486,\\n\\t\\t\\t\\t\"unit\": \"pixel\",\\n\\t\\t\\t\\t\"words\": [\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\t\"content\": \"Purchase\",\\n\\t\\t\\t\\t\\t\\t\"polygon\": [\\n\\t\\t\\t\\t\\t\\t\\t89,\\n\\t\\t\\t\\t\\t\\t\\t90,\\n\\t\\t\\t\\t\\t\\t\\t174,\\n\\t\\t\\t\\t\\t\\t\\t91,\\n\\t\\t\\t\\t\\t\\t\\t174,\\n\\t\\t\\t\\t\\t\\t\\t112,\\n\\t\\t\\t\\t\\t\\t\\t88,\\n\\t\\t\\t\\t\\t\\t\\t112\\n\\t\\t\\t\\t\\t\\t],\\n\\t\\t\\t\\t\\t\\t\"confidence\": 0.996,\\n\\t\\t\\t\\t\\t\\t\"span\": {\\n\\t\\t\\t\\t\\t\\t\\t\"offset\": 0,\\n\\t\\t\\t\\t\\t\\t\\t\"length\": 8\\n\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t},\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\t\"content\": \"Order\",\\n\\t\\t\\t\\t\\t\\t\"polygon\": [\\n\\t\\t\\t\\t\\t\\t\\t178,\\n\\t\\t\\t\\t\\t\\t\\t91,\\n\\t\\t\\t\\t\\t\\t\\t237,\\n\\t\\t\\t\\t\\t\\t\\t91,\\n\\t\\t\\t\\t\\t\\t\\t236,\\n\\t\\t\\t\\t\\t\\t\\t113,\\n\\t\\t\\t\\t\\t\\t\\t178,\\n\\t\\t\\t\\t\\t\\t\\t112\\n\\t\\t\\t\\t\\t\\t],\\n\\t\\t\\t\\t\\t\\t\"confidence\": 0.997,\\n\\t\\t\\t\\t\\t\\t\"span\": {\\n\\t\\t\\t\\t\\t\\t\\t\"offset\": 9,\\n\\t\\t\\t\\t\\t\\t\\t\"length\": 5\\n\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t},\\n                    ... Explore the documentation for supported language quickstarts . Understanding confidence scores If the confidence values of the analyzeResult are low, try to improve the quality of your input documents. You want to make sure that the form you\\'re analyzing has a similar appearance to forms in the training set if the confidence values are low. If the form appearance varies, consider training more than one model, with each model focused on one form format. Depending on the use case, you might find that a confidence score of 80% or higher is acceptable for a low-risk application. For more sensitive cases, like reading medical records or billing statements, a score of 100% is recommended.',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/quickstarts/get-started-sdks-rest-api?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Extract data from forms with Azure Document intelligence',\n",
       "  'topic': 'Use the Azure Document Intelligence Studio',\n",
       "  'text': 'In addition to SDKs and the REST API, Azure Document Intelligence services can be accessed through a user interface called the Azure Document Intelligence Studio, an online tool for visually exploring, understanding, and integrating features from the Azure Document Intelligence service. The Studio can be used to analyze form layouts, extract data from prebuilt models, and train custom models. The Azure Document Intelligence Studio currently supports the following projects: Document analysis models Read: Extract printed and handwritten text lines, words, locations, and detected languages from documents and images. Layout: Extract text, tables, selection marks, and structure information from documents (PDF and TIFF) and images (JPG, PNG, and BMP). General Documents: Extract key-value pairs, selection marks, and entities from documents. Prebuilt models Custom models Build Document analysis model projects To extract text, tables, structure, key-value pairs, and named entities with document analysis models: Create an Azure Document Intelligence or Azure AI Services resource Select either \"Read\", \"Layout\", or \"General Documents\" under the Document analysis models category Analyze your document. You\\'ll need your Azure Document Intelligence or Azure AI service endpoint and key. Build prebuilt model projects To extract data from common forms with prebuilt models: Create an Azure Document Intelligence or Azure AI Services resource Select one of the \"prebuilt models\" including W-2s, Invoices, Receipts, ID documents, Health insurance, vaccination, and business cards. Analyze your document. You\\'ll need your Azure Document Intelligence or Azure AI service endpoint and key. Build custom model projects You can use Azure Document Intelligence Studio\\'s custom service for the entire process of training and testing custom models. When you use Azure Document Intelligence Studio to build custom models, the ocr.json files, labels.json files, and fields.json file needed for training are automatically created and stored in your storage account. To train a custom model and use it to extract data with custom models: Create an Azure Document Intelligence or Azure AI Services resource Collect at least 5-6 sample forms for training and upload them to your storage account container. Configure cross-domain resource sharing (CORS). CORS enables Azure Document Intelligence Studio to store labeled files in your storage container. Create a custom model project in Azure Document Intelligence Studio. You\\'ll need to provide configurations linking your storage container and Azure Document Intelligence or Azure AI Service resource to the project. Use Azure Document Intelligence Studio to apply labels to text. Train your model. Once the model is trained, you\\'ll receive a Model ID and Average Accuracy for tags. Test your model by analyzing a new form that wasn\\'t used in training.',\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/work-form-recognizer/media/doc-intelligence-studio.png',\n",
       "    'image_alt': 'Gif of Azure Document Intelligence Studio capabilities.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Extract data from forms with Azure Document intelligence',\n",
       "  'topic': 'Exercise - Extract data from custom forms',\n",
       "  'text': \"In this exercise, you'll use the Azure Document Intelligence service to train and test a custom model. Note To complete this lab, you need an Azure subscription in which you have administrative access. Launch the exercise and follow the instructions. Tip After completing the exercise, if you've finished exploring Azure AI services, delete the Azure resources that you created during the exercise.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2322032&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Extract data from forms with Azure Document intelligence',\n",
       "  'topic': 'Summary',\n",
       "  'text': \"This module focused on Azure Document Intelligence's prebuilt service, custom service, and its client library SDKs and REST API. We also introduced the Azure Document Intelligence Studio to label and train your model. Azure Document Intelligence services can be integrated with other Azure AI Services. For example, you can try using this tutorial with Azure Document Intelligence and Cognitive Search. Document intelligence is just one part of the overall Vision API in Azure AI Services. Azure Document Intelligence services are ever evolving. You can read about the latest updates here .\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/search/cognitive-search-custom-skill-form',\n",
       "   'https://learn.microsoft.com/en-us/azure/cognitive-services/form-recognizer/whats-new'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Create a knowledge mining solution with Azure AI Search',\n",
       "  'topic': 'Introduction',\n",
       "  'text': \"Azure AI Search is a powerful cloud-based service that enables you to extract, enrich, and explore information from a wide variety of data sources. In this module, you'll learn how to build intelligent search and knowledge mining solutions using Azure AI Search. We'll start by introducing the core concepts of Azure AI Search, including how to connect to data sources and create indexes. You'll discover how the indexing process works, and how AI skills can be used to enrich your data with insights such as language detection, key phrase extraction, and image analysis. After learning how to implement an index, you'll explore how to query and filter results using full-text search. Finally, you'll see how to use the knowledge store to persist enriched data for further analysis and integration with other systems.\",\n",
       "  'links': None,\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Create a knowledge mining solution with Azure AI Search',\n",
       "  'topic': 'What is Azure AI Search?',\n",
       "  'text': \"Azure AI Search provides a cloud-based solution for indexing and querying a wide range of data sources, and creating comprehensive and high-scale search solutions. It provides the infrastructure and tools to create search solutions that extract data from structured, semi-structured, and non-structured documents and other data sources. With Azure AI Search, you can: Index documents and data from a range of sources. Use AI skills to enrich index data. Store extracted insights in a knowledge store for analysis and integration. Azure AI Search indexes contain insights extracted from your data; which can include text inferred or read using OCR from images, entities and key phrases detection through text analytics, and other derived information based on AI skills that are integrated into the indexing process. Azure AI search has many applications, including: Implementing an enterprise search solution to help employees or customers find information in websites or applications. Supporting retrieval augmented generation (RAG) in generative AI applications by using vector-based indexes for prompt grounding data. Creating knowledge mining solutions in which the indexing process is used to infer insights and extract granular data assets from documents to support data analytics. In this module, we'll focus on Azure AI Search in knowledge mining scenarios.\",\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/ai-knowldge-mining/media/ai-search.png',\n",
       "    'image_alt': 'Diagram of Azure AI Search supporting multiple applications.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Create a knowledge mining solution with Azure AI Search',\n",
       "  'topic': 'Extract data with an indexer',\n",
       "  'text': 'At the heart of Azure AI Search solutions is the creation of an index An index contains your searchable content and is created and updated, unsurprisingly, by an indexer . The indexing process starts with a data source: the storage location of your original data artifacts; for example, an Azure blob store container full of documents, a database, or some other store. The Indexer automates the extraction and indexing of data fields through an enrichment pipeline , in which it applies document cracking to extract the contents of the source documents and applies incremental steps to create a hierarchical (JSON-based) document with the required fields for the index definition. The result is a populated index, which can be queried to return specified fields from documents that match the query criteria. How documents are constructed during indexing The indexing process works by creating a document for each indexed entity. During indexing, an enrichment pipeline iteratively builds the documents that combine metadata from the data source with enriched fields extracted or generated by skills . You can think of each indexed document as a JSON structure, which initially consists of a document with the index fields you have mapped to fields extracted directly from the source data, like this: document metadata_storage_name metadata_author content When the documents in the data source contain images, you can configure the indexer to extract the image data and place each image in a normalized_images collection, like this: document metadata_storage_name metadata_author content normalized_images image0 image1 Normalizing the image data in this way enables you to use the collection of images as an input for skills that extract information from image data. Each skill adds fields to the document , so for example a skill that detects the language in which a document is written might store its output in a language field, like this: document metadata_storage_name metadata_author content normalized_images image0 image1 language The document is structured hierarchically, and the skills are applied to a specific context within the hierarchy, enabling you to run the skill for each item at a particular level of the document. For example, you could run an optical character recognition ( OCR ) skill for each image in the normalized images collection to extract any text they contain: document metadata_storage_name metadata_author content normalized_images image0 Text image1 Text language The output fields from each skill can be used as inputs for other skills later in the pipeline, which in turn store their outputs in the document structure. For example, we could use a merge skill to combine the original text content with the text extracted from each image to create a new merged_content field that contains all of the text in the document, including image text. document metadata_storage_name metadata_author content normalized_images image0 Text image1 Text language merged_content The fields in the final document structure at the end of the pipeline are mapped to index fields by the indexer in one of two ways: Fields extracted directly from the source data are all mapped to index fields. These mappings can be implicit (fields are automatically mapped to in fields with the same name in the index) or explicit (a mapping is defined to match a source field to an index field, often to rename the field to something more useful or to apply a function to the data value as it is mapped). Output fields from the skills in the skillset are explicitly mapped from their hierarchical location in the output to the target field in the index.',\n",
       "  'links': None,\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/ai-knowldge-mining/media/indexer.png',\n",
       "    'image_alt': 'Diagram of an indexer extracting documents from a source and creating an index.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Create a knowledge mining solution with Azure AI Search',\n",
       "  'topic': 'Enrich extracted data with AI skills',\n",
       "  'text': 'The enrichment pipeline that is orchestrated by an indexer uses a skillset of AI skills to create AI-enriched fields. The indexer applies each skill in order, refining the index document at each step. Built-in skills Azure AI Search provides a collection of built-in skills that you can include in a skillset for your indexer. Built-in skills include functionality from Azure AI services such as Azure AI Vision and Azure AI Language, enabling you to apply enrichments such as: Detecting the language that text is written in. Detecting and extracting places, locations, and other entities in the text. Determining and extracting key phrases within a body of text. Translating text. Identifying and extracting (or removing) personally identifiable information (PII) within the text. Extracting text from images. Generating captions and tags to describe images. To use the built-in skills, your indexer must have access to an Azure AI services resource. You can use a restricted Azure AI search resource that is included in Azure AI Search (and which is limited to indexing 20 or fewer documents) or you can attach an Azure AI services resource in your Azure subscription (which must be in the same region as your Azure AI Search resource). Custom skills You can further extend the enrichment capabilities of your index by creating custom skills. As the name suggests, custom skills perform custom logic on input data from your index document to return new field values that can be incorporated into the index. Often, custom skills are \"wrappers\" around services that are specifically designed to extract data from documents. For example, you could implement a custom skill as an Azure Function, and use it to pass data from your index document to an Azure AI Document Intelligence model, which can extract fields from a form. Tip To learn more about using custom skills with Azure AI Search, see Add a custom skill to an Azure AI Search enrichment pipeline in the Azure AI Search documentation.',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/search/cognitive-search-custom-skill-interface'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/ai-knowldge-mining/media/skillset.png',\n",
       "    'image_alt': 'Diagram of an indexer using a skillset to enrich data.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Create a knowledge mining solution with Azure AI Search',\n",
       "  'topic': 'Search an index',\n",
       "  'text': 'The index is the searchable result of the indexing process. It consists of a collection of JSON documents, with fields that contain the values extracted during indexing. Client applications can query the index to retrieve, filter, and sort information. Each index field can be configured with the following attributes: key : Fields that define a unique key for index records. searchable : Fields that can be queried using full-text search. filterable : Fields that can be included in filter expressions to return only documents that match specified constraints. sortable : Fields that can be used to order the results. facetable : Fields that can be used to determine values for facets (user interface elements used to filter the results based on a list of known field values). retrievable : Fields that can be included in search results ( by default, all fields are retrievable unless this attribute is explicitly removed ). Full-text search While you could retrieve index entries based on simple field value matching, most search solutions use full-text search semantics to query an index. Full-text search describes search solutions that parse text-based document contents to find query terms. Full-text search queries in Azure AI Search are based on the Lucene query syntax, which provides a rich set of query operations for searching, filtering, and sorting data in indexes. Azure AI Search supports two variants of the Lucene syntax: Simple - An intuitive syntax that makes it easy to perform basic searches that match literal query terms submitted by a user. Full - An extended syntax that supports complex filtering, regular expressions, and other more sophisticated queries. Client applications submit queries to Azure AI Search by specifying a search expression along with other parameters that determine how the expression is evaluated and the results returned. Some common parameters submitted with a query include: search - A search expression that includes the terms to be found. queryType - The Lucene syntax to be evaluated ( simple or full ). searchFields - The index fields to be searched. select - The fields to be included in the results. searchMode - Criteria for including results based on multiple search terms. For example, suppose you search an index of travel-related documents for comfortable hotel . A searchMode value of Any returns documents that contain \"comfortable\", \"hotel\", or both; while a searchMode value of All restricts results to documents that contain both \"comfortable\" and \"hotel\". Query processing consists of four stages: Query parsing . The search expression is evaluated and reconstructed as a tree of appropriate subqueries. Subqueries might include term queries (finding specific individual words in the search expression - for example hotel ), phrase queries (finding multi-term phrases specified in quotation marks in the search expression - for example, \"free parking\" ), and prefix queries (finding terms with a specified prefix - for example air* , which would match airway , air-conditioning , and airport ). Lexical analysis - The query terms are analyzed and refined based on linguistic rules. For example, text is converted to lower case and nonessential stopwords (such as \"the\", \"a\", \"is\", and so on) are removed. Then words are converted to their root form (for example, \"comfortable\" might be simplified to \"comfort\") and composite words are split into their constituent terms. Document retrieval - The query terms are matched against the indexed terms, and the set of matching documents is identified. Scoring - A relevance score is assigned to each result based on a term frequency/inverse document frequency (TF/IDF) calculation. Tip For more information about querying an index, and details about simple and full syntax, see Query types and composition in Azure AI Search in the Azure AI Search documentation. It\\'s common in a search solution for users to want to refine query results by filtering and sorting based on field values. Azure AI Search supports both of these capabilities through the search query API. Filtering results You can apply filters to queries in two ways: By including filter criteria in a simple search expression. By providing an OData filter expression as a $filter parameter with a full syntax search expression. You can apply a filter to any filterable field in the index. For example, suppose you want to find documents containing the text London that have an author field value of Reviewer . You can achieve this result by submitting the following simple search expression: search=London+author=\\'Reviewer\\'\\nqueryType=Simple Alternatively, you can use an OData filter in a $filter parameter with a full Lucene search expression like this: search=London\\n$filter=author eq \\'Reviewer\\'\\nqueryType=Full Note OData $filter expressions are case-sensitive! Filtering with facets Facets are a useful way to present users with filtering criteria based on field values in a result set. They work best when a field has a small number of discrete values that can be displayed as links or options in the user interface. To use facets, you must specify facetable fields for which you want to retrieve the possible values in an initial query. For example, you could use the following parameters to return all of the possible values for the author field: search=*\\nfacet=author The results from this query include a collection of discrete facet values that you can display in the user interface for the user to select. Then in a subsequent query, you can use the selected facet value to filter the results: search=*\\n$filter=author eq \\'selected-facet-value-here\\' Sorting results By default, results are sorted based on the relevancy score assigned by the query process, with the highest scoring matches listed first. However, you can override this sort order by including an OData orderby parameter that specifies one or more sortable fields and a sort order ( asc or desc ). For example, to sort the results so that the most recently modified documents are listed first, you could use the following parameter values: search=*\\n$orderby=last_modified desc Tip For more information about using filters, see Filters in Azure AI Search in the Azure AI Search documentation.',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/search/search-query-overview',\n",
       "   'https://learn.microsoft.com/en-us/azure/search/search-filters'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/ai-knowldge-mining/media/search-index.png',\n",
       "    'image_alt': 'Diagram of a client searching an Azure AI Search index.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Create a knowledge mining solution with Azure AI Search',\n",
       "  'topic': 'Persist extracted information in a knowledge store',\n",
       "  'text': 'While the index might be considered the primary output from an indexing process, the enriched data it contains might also be useful in other ways. For example: Since the index is essentially a collection of JSON objects, each representing an indexed record, it might be useful to export the objects as JSON files for integration into a data orchestration process for extract, transform, and load (ETL) operations. You may want to normalize the index records into a relational schema of tables for analysis and reporting. Having extracted embedded images from documents during the indexing process, you might want to save those images as files. Azure AI Search supports these scenarios by enabling you to define a knowledge store in the skillset that encapsulates your enrichment pipeline. The knowledge store consists of projections of the enriched data, which can be JSON objects, tables, or image files. When an indexer runs the pipeline to create or update an index, the projections are generated and persisted in the knowledge store. Tip To learn more about using a knowledge store, see Knowledge store in Azure AI Search in the Azure AI Search documentation.',\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/search/knowledge-store-concept-intro'],\n",
       "  'images': [{'image_name': 'image1',\n",
       "    'image_src': 'https://learn.microsoft.com/en-us/training/wwl-data-ai/ai-knowldge-mining/media/knowledge-store.png',\n",
       "    'image_alt': 'Diagram of an indexing process loading projections into a knowledge store.'}],\n",
       "  'videos': None,\n",
       "  'exercise': None},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Create a knowledge mining solution with Azure AI Search',\n",
       "  'topic': 'Exercise - Create a knowledge mining solution',\n",
       "  'text': \"It's time to out what you've learned into practice! In this exercise, you use Azure AI Search to extract and enrich information from documents into a searchable index and a knowledge store. Note To complete this lab, you need an Azure subscription in which you have administrative access. Launch the exercise and follow the instructions.\",\n",
       "  'links': ['https://azure.microsoft.com/free?azure-portal=true'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': 'https://go.microsoft.com/fwlink/?linkid=2320469&azure-portal=true'},\n",
       " {'course': 'Course AI-102T00-A: Develop AI solutions in Azure - Training',\n",
       "  'module': 'Develop AI information extraction solutions in Azure AI-3002 - Training',\n",
       "  'unit': 'Create a knowledge mining solution with Azure AI Search',\n",
       "  'topic': 'Summary',\n",
       "  'text': \"In this module, you've learned how Azure AI Search enables you to build intelligent search and knowledge mining solutions by indexing and enriching data from various sources. You explored the indexing process, the use of AI skills for data enrichment, and how to persist enriched data in a knowledge store for further analysis and integration. With these skills, you're now equipped to design and implement solutions that unlock valuable insights from your data using Azure AI Search. Tip To learn more about Azure AI Search, see the Azure AI Search documentation .\",\n",
       "  'links': ['https://learn.microsoft.com/en-us/azure/search/'],\n",
       "  'images': None,\n",
       "  'videos': None,\n",
       "  'exercise': None}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "459a13e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "#for index, row in df.iterrows():\n",
    "i = 0\n",
    "for item in data:\n",
    "    i += 1\n",
    "    docs.append(\n",
    "        {\n",
    "            \"docid\": str(i),\n",
    "            \"exam\": item['course'],\n",
    "            \"module\": item['module'],\n",
    "            \"unit\": item['unit'],\n",
    "            \"topic\": item['topic'],\n",
    "            \"content\": item['text'],\n",
    "            \"links\": item['links'] if item['links'] is not None else [],\n",
    "            \"images\": item['images'] if item['images'] is not None else [],\n",
    "            \"videos\": item['videos'] if item['videos'] is not None else [],\n",
    "            \"exercise_url\": item['exercise'],\n",
    "            \"content_vector\": embeddings.embed_query(item['text'])\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "423ab0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = search_index_client.upload_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "27394fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_index_client.get_document_count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
